{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_halving_search_cv # required for HalvingGridSearchCV\n",
    "from sklearn.model_selection import train_test_split, HalvingGridSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.588463</td>\n",
       "      <td>1.026888</td>\n",
       "      <td>2.956561</td>\n",
       "      <td>-0.012614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0.144081</td>\n",
       "      <td>0.147693</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-3.835282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0.142465</td>\n",
       "      <td>0.148484</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>-3.813875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0.140811</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-3.792289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.149889</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>-3.770526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.150505</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.201773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0             2.588463                 1.026888                     2.956561   \n",
       "1             2.588463                 1.026888                     2.956561   \n",
       "2             2.588463                 1.026888                     2.956561   \n",
       "3             2.588463                 1.026888                     2.956561   \n",
       "4             2.588463                 1.026888                     2.956561   \n",
       "...                ...                      ...                          ...   \n",
       "124995        0.144081                 0.147693                     0.010285   \n",
       "124996        0.142465                 0.148484                     0.006000   \n",
       "124997        0.140811                 0.149215                     0.001745   \n",
       "124998        0.139120                 0.149889                    -0.002478   \n",
       "124999        0.137394                 0.150505                    -0.006668   \n",
       "\n",
       "          reward  \n",
       "0      -0.012300  \n",
       "1      -0.012300  \n",
       "2      -0.012300  \n",
       "3      -0.012300  \n",
       "4      -0.012614  \n",
       "...          ...  \n",
       "124995 -3.835282  \n",
       "124996 -3.813875  \n",
       "124997 -3.792289  \n",
       "124998 -3.770526  \n",
       "124999 -0.201773  \n",
       "\n",
       "[125000 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/2548153325.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"train_data/raw_data_pd.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df)\n",
    "\n",
    "# split into input and target features, and weights\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0311 - val_loss: 4.9459e-04\n",
      "Epoch 2/100\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 5.7531e-04 - val_loss: 7.1825e-04\n",
      "Epoch 3/100\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 4.4264e-04 - val_loss: 5.8055e-04\n",
      "Epoch 4/100\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 4.8484e-04 - val_loss: 3.8641e-05\n",
      "Epoch 5/100\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 2.5327e-04 - val_loss: 4.7775e-05\n",
      "Epoch 6/100\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.6589e-04 - val_loss: 2.4218e-04\n",
      "Epoch 7/100\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 2.0023e-04 - val_loss: 5.0007e-06\n",
      "Epoch 8/100\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 2.1890e-04 - val_loss: 6.4376e-06\n",
      "Epoch 9/100\n",
      "3125/3125 [==============================] - 2s 509us/step - loss: 1.4528e-04 - val_loss: 1.8688e-05\n",
      "Epoch 10/100\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 1.7536e-04 - val_loss: 1.3059e-05\n",
      "Epoch 11/100\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.1187e-04 - val_loss: 1.0243e-04\n",
      "Epoch 12/100\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.3933e-04 - val_loss: 2.0056e-05\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(learning_rate=0.001), weighted_metrics=[])\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=100, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0202 - mae: 0.0251 - mse: 0.0104 - val_loss: 1.5993e-04 - val_mae: 0.0052 - val_mse: 6.3100e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 7.1293e-04 - mae: 0.0093 - mse: 2.6307e-04 - val_loss: 0.0031 - val_mae: 0.0230 - val_mse: 9.9338e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 5.7935e-04 - mae: 0.0097 - mse: 2.3994e-04 - val_loss: 6.5062e-04 - val_mae: 0.0123 - val_mse: 3.5072e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 4.6699e-04 - mae: 0.0089 - mse: 2.1845e-04 - val_loss: 7.2402e-05 - val_mae: 0.0046 - val_mse: 4.5898e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.8915e-04 - mae: 0.0068 - mse: 1.2366e-04 - val_loss: 2.7315e-05 - val_mae: 0.0033 - val_mse: 1.6124e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 2.4844e-04 - mae: 0.0066 - mse: 1.1776e-04 - val_loss: 1.2982e-04 - val_mae: 0.0078 - val_mse: 9.2997e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 2.2921e-04 - mae: 0.0064 - mse: 1.0785e-04 - val_loss: 1.6559e-04 - val_mae: 0.0049 - val_mse: 4.8297e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.6793e-04 - mae: 0.0057 - mse: 8.9933e-05 - val_loss: 3.0036e-04 - val_mae: 0.0115 - val_mse: 2.4888e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.5048e-04 - mae: 0.0047 - mse: 7.5915e-05 - val_loss: 2.9277e-05 - val_mae: 0.0036 - val_mse: 1.9212e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4594e-04 - mae: 0.0044 - mse: 7.4576e-05 - val_loss: 1.1561e-05 - val_mae: 0.0019 - val_mse: 6.4723e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.2166e-04 - mae: 0.0040 - mse: 6.1894e-05 - val_loss: 2.1074e-05 - val_mae: 0.0029 - val_mse: 1.1959e-05\n",
      "Iteration 1: delta = 0.5, val_mae = 0.002943823579698801, val_mse = 1.1959174116782378e-05\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0179 - mae: 0.0315 - mse: 0.0141 - val_loss: 5.0269e-04 - val_mae: 0.0119 - val_mse: 2.3428e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 9.1817e-04 - mae: 0.0118 - mse: 3.6821e-04 - val_loss: 5.4259e-04 - val_mae: 0.0144 - val_mse: 3.1531e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.1241e-04 - mae: 0.0095 - mse: 2.3283e-04 - val_loss: 2.8834e-04 - val_mae: 0.0075 - val_mse: 1.3596e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 4.6370e-04 - mae: 0.0079 - mse: 1.8891e-04 - val_loss: 5.0387e-05 - val_mae: 0.0041 - val_mse: 3.0028e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 4.4266e-04 - mae: 0.0075 - mse: 1.6384e-04 - val_loss: 1.2292e-04 - val_mae: 0.0065 - val_mse: 8.0425e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 2.8406e-04 - mae: 0.0061 - mse: 1.2867e-04 - val_loss: 0.0011 - val_mae: 0.0171 - val_mse: 7.6907e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 2.7603e-04 - mae: 0.0060 - mse: 1.2185e-04 - val_loss: 1.0649e-04 - val_mae: 0.0081 - val_mse: 8.3981e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.9524e-04 - mae: 0.0055 - mse: 1.0114e-04 - val_loss: 7.5181e-06 - val_mae: 0.0019 - val_mse: 6.3815e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 585us/step - loss: 1.9535e-04 - mae: 0.0051 - mse: 9.1940e-05 - val_loss: 1.6227e-05 - val_mae: 0.0025 - val_mse: 9.9240e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.8053e-04 - mae: 0.0049 - mse: 8.5892e-05 - val_loss: 1.2958e-04 - val_mae: 0.0055 - val_mse: 4.4025e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.3691e-04 - mae: 0.0039 - mse: 6.3916e-05 - val_loss: 0.0014 - val_mae: 0.0249 - val_mse: 0.0011\n",
      "Iteration 2: delta = 0.6, val_mae = 0.024870606139302254, val_mse = 0.0010972677264362574\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0114 - mae: 0.0257 - mse: 0.0094 - val_loss: 5.3391e-05 - val_mae: 0.0044 - val_mse: 3.7553e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 9.6109e-04 - mae: 0.0101 - mse: 3.9020e-04 - val_loss: 0.0014 - val_mae: 0.0215 - val_mse: 9.1424e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 4.7032e-04 - mae: 0.0080 - mse: 1.8341e-04 - val_loss: 2.6630e-04 - val_mae: 0.0081 - val_mse: 1.5600e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 6.6518e-04 - mae: 0.0071 - mse: 2.1967e-04 - val_loss: 1.0308e-04 - val_mae: 0.0078 - val_mse: 9.6577e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 4.7084e-04 - mae: 0.0070 - mse: 1.8236e-04 - val_loss: 2.1098e-05 - val_mae: 0.0024 - val_mse: 1.1410e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 2.2838e-04 - mae: 0.0055 - mse: 9.9337e-05 - val_loss: 5.1811e-04 - val_mae: 0.0174 - val_mse: 4.4689e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.0463e-04 - mae: 0.0057 - mse: 9.5618e-05 - val_loss: 1.9682e-05 - val_mae: 0.0030 - val_mse: 1.3833e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.6610e-04 - mae: 0.0044 - mse: 6.7608e-05 - val_loss: 9.1411e-06 - val_mae: 0.0018 - val_mse: 7.1671e-06\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.3054e-04 - mae: 0.0049 - mse: 1.0475e-04 - val_loss: 0.0012 - val_mae: 0.0150 - val_mse: 3.6073e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6315e-04 - mae: 0.0043 - mse: 9.5132e-05 - val_loss: 1.8588e-05 - val_mae: 0.0019 - val_mse: 6.7877e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.9439e-04 - mae: 0.0043 - mse: 1.0390e-04 - val_loss: 1.7810e-06 - val_mae: 8.9890e-04 - val_mse: 1.7142e-06\n",
      "Iteration 3: delta = 0.7, val_mae = 0.0008989019552245736, val_mse = 1.7142334627351374e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0223 - mae: 0.0271 - mse: 0.0115 - val_loss: 0.0015 - val_mae: 0.0321 - val_mse: 0.0015\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 6.2845e-04 - mae: 0.0095 - mse: 2.5019e-04 - val_loss: 2.4619e-04 - val_mae: 0.0106 - val_mse: 2.0971e-04\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 6.0896e-04 - mae: 0.0094 - mse: 3.1653e-04 - val_loss: 3.8566e-04 - val_mae: 0.0070 - val_mse: 1.2440e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 5.6446e-04 - mae: 0.0095 - mse: 2.9370e-04 - val_loss: 3.7102e-04 - val_mae: 0.0118 - val_mse: 2.7360e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 2.4134e-04 - mae: 0.0059 - mse: 9.6261e-05 - val_loss: 1.5849e-04 - val_mae: 0.0047 - val_mse: 5.4793e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 3.4626e-04 - mae: 0.0072 - mse: 1.6621e-04 - val_loss: 4.5183e-05 - val_mae: 0.0032 - val_mse: 2.0729e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.5767e-04 - mae: 0.0053 - mse: 7.7483e-05 - val_loss: 2.5716e-04 - val_mae: 0.0099 - val_mse: 2.1014e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.8267e-04 - mae: 0.0056 - mse: 9.4568e-05 - val_loss: 6.7420e-05 - val_mae: 0.0032 - val_mse: 2.1570e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 1.9465e-04 - mae: 0.0047 - mse: 8.9738e-05 - val_loss: 1.2042e-04 - val_mae: 0.0095 - val_mse: 1.3092e-04\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.6518e-04 - mae: 0.0050 - mse: 8.5304e-05 - val_loss: 2.0021e-05 - val_mae: 0.0023 - val_mse: 9.3320e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1535e-04 - mae: 0.0037 - mse: 4.7635e-05 - val_loss: 3.0474e-06 - val_mae: 0.0010 - val_mse: 2.0951e-06\n",
      "Iteration 4: delta = 0.7999999999999999, val_mae = 0.0010287829209119081, val_mse = 2.0950956240994856e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0255 - mae: 0.0301 - mse: 0.0121 - val_loss: 3.8480e-04 - val_mae: 0.0080 - val_mse: 1.4563e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 8.5414e-04 - mae: 0.0105 - mse: 3.2599e-04 - val_loss: 2.2098e-05 - val_mae: 0.0032 - val_mse: 2.0943e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 556us/step - loss: 4.6625e-04 - mae: 0.0078 - mse: 1.8403e-04 - val_loss: 3.0350e-05 - val_mae: 0.0029 - val_mse: 1.6471e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 5.0720e-04 - mae: 0.0073 - mse: 2.3415e-04 - val_loss: 2.1361e-05 - val_mae: 0.0024 - val_mse: 1.3373e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 3.0236e-04 - mae: 0.0061 - mse: 1.3501e-04 - val_loss: 1.2982e-05 - val_mae: 0.0018 - val_mse: 5.9104e-06\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.7702e-04 - mae: 0.0051 - mse: 7.9110e-05 - val_loss: 2.9651e-05 - val_mae: 0.0035 - val_mse: 2.0781e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.7276e-04 - mae: 0.0047 - mse: 9.3099e-05 - val_loss: 0.0023 - val_mae: 0.0280 - val_mse: 0.0018\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 2.4888e-04 - mae: 0.0043 - mse: 1.0232e-04 - val_loss: 2.2918e-05 - val_mae: 0.0042 - val_mse: 2.6106e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1057e-04 - mae: 0.0037 - mse: 5.9276e-05 - val_loss: 5.0996e-06 - val_mae: 0.0016 - val_mse: 4.2748e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 1.4172e-04 - mae: 0.0046 - mse: 6.8312e-05 - val_loss: 4.0365e-06 - val_mae: 0.0013 - val_mse: 3.3686e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 1.1190e-04 - mae: 0.0033 - mse: 5.2435e-05 - val_loss: 2.0810e-06 - val_mae: 0.0011 - val_mse: 2.5243e-06\n",
      "Iteration 5: delta = 0.8999999999999999, val_mae = 0.0011450344463810325, val_mse = 2.5242668471037177e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0221 - mae: 0.0245 - mse: 0.0082 - val_loss: 0.0014 - val_mae: 0.0218 - val_mse: 7.6380e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 7.3161e-04 - mae: 0.0098 - mse: 2.7063e-04 - val_loss: 1.0519e-04 - val_mae: 0.0048 - val_mse: 4.6909e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.0558e-04 - mae: 0.0089 - mse: 2.3874e-04 - val_loss: 1.4503e-04 - val_mae: 0.0058 - val_mse: 6.3119e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 3.1684e-04 - mae: 0.0072 - mse: 1.4621e-04 - val_loss: 6.0222e-04 - val_mae: 0.0152 - val_mse: 3.2410e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 521us/step - loss: 2.8636e-04 - mae: 0.0066 - mse: 1.2217e-04 - val_loss: 1.0486e-04 - val_mae: 0.0048 - val_mse: 4.2946e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 522us/step - loss: 1.8973e-04 - mae: 0.0056 - mse: 8.7885e-05 - val_loss: 1.7900e-04 - val_mae: 0.0047 - val_mse: 4.8652e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.5495e-04 - mae: 0.0048 - mse: 9.8091e-05 - val_loss: 4.8226e-06 - val_mae: 0.0014 - val_mse: 3.9607e-06\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 1.8153e-04 - mae: 0.0037 - mse: 7.3064e-05 - val_loss: 0.0053 - val_mae: 0.0303 - val_mse: 0.0016\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 520us/step - loss: 1.3014e-04 - mae: 0.0039 - mse: 6.9618e-05 - val_loss: 1.8095e-06 - val_mae: 8.7120e-04 - val_mse: 1.7165e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 523us/step - loss: 9.9326e-05 - mae: 0.0032 - mse: 4.2085e-05 - val_loss: 2.3668e-06 - val_mae: 9.2389e-04 - val_mse: 1.6628e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 508us/step - loss: 1.0992e-04 - mae: 0.0039 - mse: 5.5732e-05 - val_loss: 2.7451e-06 - val_mae: 9.7746e-04 - val_mse: 1.7462e-06\n",
      "Iteration 6: delta = 0.9999999999999999, val_mae = 0.0009774580830708146, val_mse = 1.7462235746279475e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0248 - mae: 0.0229 - mse: 0.0078 - val_loss: 4.8171e-05 - val_mae: 0.0045 - val_mse: 4.0733e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.2655e-04 - mae: 0.0121 - mse: 4.1391e-04 - val_loss: 1.1910e-04 - val_mae: 0.0052 - val_mse: 5.1821e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 526us/step - loss: 5.7925e-04 - mae: 0.0096 - mse: 2.3423e-04 - val_loss: 6.7896e-04 - val_mae: 0.0105 - val_mse: 2.2890e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 524us/step - loss: 5.0893e-04 - mae: 0.0074 - mse: 1.9574e-04 - val_loss: 8.1922e-04 - val_mae: 0.0162 - val_mse: 3.5495e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 617us/step - loss: 3.1946e-04 - mae: 0.0070 - mse: 1.7556e-04 - val_loss: 3.5842e-04 - val_mae: 0.0160 - val_mse: 3.2989e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 588us/step - loss: 2.2603e-04 - mae: 0.0061 - mse: 1.0574e-04 - val_loss: 7.4329e-04 - val_mae: 0.0159 - val_mse: 5.7037e-04\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 2.8874e-04 - mae: 0.0066 - mse: 1.2610e-04 - val_loss: 2.8004e-04 - val_mae: 0.0120 - val_mse: 2.3289e-04\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 2.0672e-04 - mae: 0.0047 - mse: 9.4651e-05 - val_loss: 3.2528e-05 - val_mae: 0.0043 - val_mse: 2.2604e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 2.0818e-04 - mae: 0.0049 - mse: 9.5721e-05 - val_loss: 2.3110e-05 - val_mae: 0.0022 - val_mse: 9.1263e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.7532e-04 - mae: 0.0043 - mse: 8.3759e-05 - val_loss: 3.2591e-05 - val_mae: 0.0039 - val_mse: 2.3659e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0985e-04 - mae: 0.0045 - mse: 5.3047e-05 - val_loss: 1.7614e-04 - val_mae: 0.0078 - val_mse: 1.0241e-04\n",
      "Iteration 7: delta = 1.0999999999999999, val_mae = 0.007791235577315092, val_mse = 0.00010241157724522054\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0222 - mae: 0.0238 - mse: 0.0100 - val_loss: 2.2203e-04 - val_mae: 0.0057 - val_mse: 6.9195e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 8.8828e-04 - mae: 0.0098 - mse: 3.4518e-04 - val_loss: 1.8628e-05 - val_mae: 0.0029 - val_mse: 1.5384e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 6.4063e-04 - mae: 0.0091 - mse: 2.8052e-04 - val_loss: 4.4558e-04 - val_mae: 0.0091 - val_mse: 1.7246e-04\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.9207e-04 - mae: 0.0080 - mse: 1.7642e-04 - val_loss: 1.1273e-04 - val_mae: 0.0048 - val_mse: 4.7311e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 3.3197e-04 - mae: 0.0076 - mse: 1.6882e-04 - val_loss: 4.8506e-05 - val_mae: 0.0029 - val_mse: 1.7219e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 2.3450e-04 - mae: 0.0053 - mse: 9.7111e-05 - val_loss: 3.2150e-04 - val_mae: 0.0063 - val_mse: 9.9308e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 3.5725e-04 - mae: 0.0055 - mse: 1.5663e-04 - val_loss: 2.9813e-05 - val_mae: 0.0028 - val_mse: 1.3478e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.8142e-04 - mae: 0.0044 - mse: 8.9416e-05 - val_loss: 1.4668e-04 - val_mae: 0.0070 - val_mse: 1.0808e-04\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 1.0711e-04 - mae: 0.0042 - mse: 5.3082e-05 - val_loss: 6.3507e-06 - val_mae: 0.0024 - val_mse: 7.9813e-06\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 551us/step - loss: 1.4143e-04 - mae: 0.0039 - mse: 6.4006e-05 - val_loss: 0.0011 - val_mae: 0.0212 - val_mse: 9.5096e-04\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.4094e-04 - mae: 0.0034 - mse: 5.3753e-05 - val_loss: 4.8846e-06 - val_mae: 0.0011 - val_mse: 2.5810e-06\n",
      "Iteration 8: delta = 1.2, val_mae = 0.001096259569749236, val_mse = 2.5810188617469976e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0304 - mae: 0.0230 - mse: 0.0107 - val_loss: 1.5114e-04 - val_mae: 0.0054 - val_mse: 5.8559e-05\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 8.7749e-04 - mae: 0.0106 - mse: 3.3684e-04 - val_loss: 2.5765e-05 - val_mae: 0.0035 - val_mse: 2.1725e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 535us/step - loss: 5.9876e-04 - mae: 0.0084 - mse: 2.8471e-04 - val_loss: 5.2961e-05 - val_mae: 0.0054 - val_mse: 5.3889e-05\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 4.7338e-04 - mae: 0.0073 - mse: 1.8480e-04 - val_loss: 6.4867e-05 - val_mae: 0.0051 - val_mse: 4.2587e-05\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.5211e-04 - mae: 0.0064 - mse: 1.2908e-04 - val_loss: 0.0027 - val_mae: 0.0139 - val_mse: 5.6304e-04\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 2.6622e-04 - mae: 0.0062 - mse: 1.1705e-04 - val_loss: 1.0077e-05 - val_mae: 0.0022 - val_mse: 1.0210e-05\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 536us/step - loss: 1.5176e-04 - mae: 0.0048 - mse: 7.4001e-05 - val_loss: 3.1539e-05 - val_mae: 0.0036 - val_mse: 2.3778e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 532us/step - loss: 1.5780e-04 - mae: 0.0048 - mse: 9.1301e-05 - val_loss: 6.0171e-05 - val_mae: 0.0022 - val_mse: 1.4143e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 531us/step - loss: 2.1998e-04 - mae: 0.0052 - mse: 1.4376e-04 - val_loss: 7.8193e-05 - val_mae: 0.0055 - val_mse: 3.8699e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 9.8217e-05 - mae: 0.0035 - mse: 4.5323e-05 - val_loss: 1.8920e-05 - val_mae: 0.0020 - val_mse: 8.4045e-06\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 528us/step - loss: 1.3668e-04 - mae: 0.0038 - mse: 5.4840e-05 - val_loss: 7.8829e-06 - val_mae: 0.0013 - val_mse: 4.0456e-06\n",
      "Iteration 9: delta = 1.3, val_mae = 0.0013006088556721807, val_mse = 4.045555670018075e-06\n",
      "Epoch 1/11\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0286 - mae: 0.0258 - mse: 0.0099 - val_loss: 8.9270e-04 - val_mae: 0.0148 - val_mse: 4.5197e-04\n",
      "Epoch 2/11\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 9.5435e-04 - mae: 0.0120 - mse: 4.5623e-04 - val_loss: 1.2477e-04 - val_mae: 0.0046 - val_mse: 4.3036e-05\n",
      "Epoch 3/11\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 9.4698e-04 - mae: 0.0104 - mse: 3.7462e-04 - val_loss: 0.0092 - val_mae: 0.0475 - val_mse: 0.0033\n",
      "Epoch 4/11\n",
      "3125/3125 [==============================] - 2s 586us/step - loss: 5.2611e-04 - mae: 0.0085 - mse: 2.0991e-04 - val_loss: 7.3887e-04 - val_mae: 0.0086 - val_mse: 1.9458e-04\n",
      "Epoch 5/11\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 4.0725e-04 - mae: 0.0074 - mse: 1.8848e-04 - val_loss: 1.3648e-04 - val_mae: 0.0050 - val_mse: 5.5901e-05\n",
      "Epoch 6/11\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 2.1604e-04 - mae: 0.0063 - mse: 1.0755e-04 - val_loss: 1.4588e-05 - val_mae: 0.0020 - val_mse: 8.5403e-06\n",
      "Epoch 7/11\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 2.4824e-04 - mae: 0.0061 - mse: 1.0683e-04 - val_loss: 4.5263e-05 - val_mae: 0.0029 - val_mse: 1.7558e-05\n",
      "Epoch 8/11\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.6628e-04 - mae: 0.0052 - mse: 8.6930e-05 - val_loss: 2.3331e-04 - val_mae: 0.0042 - val_mse: 6.1065e-05\n",
      "Epoch 9/11\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.6646e-04 - mae: 0.0051 - mse: 8.4189e-05 - val_loss: 2.4806e-05 - val_mae: 0.0026 - val_mse: 1.0330e-05\n",
      "Epoch 10/11\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 1.3747e-04 - mae: 0.0045 - mse: 8.1643e-05 - val_loss: 6.2283e-05 - val_mae: 0.0039 - val_mse: 2.4124e-05\n",
      "Epoch 11/11\n",
      "3125/3125 [==============================] - 2s 659us/step - loss: 1.3672e-04 - mae: 0.0042 - mse: 6.9716e-05 - val_loss: 1.6083e-05 - val_mae: 0.0020 - val_mse: 8.3948e-06\n",
      "Iteration 10: delta = 1.4000000000000001, val_mae = 0.0019502261420711875, val_mse = 8.39479162095813e-06\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'], weighted_metrics=[])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE, MSE and Huber loss\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "    return val_mae, val_mse, val_loss\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 0.5\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# initialise arrays for plotting\n",
    "arr_val_mae = []\n",
    "arr_val_mse = []\n",
    "arr_val_loss = []\n",
    "\n",
    "# iterative search for optimal delta\n",
    "for i in range(10):\n",
    "    val_mae, val_mse, val_loss = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # store data\n",
    "    arr_val_mae.append(val_mae)\n",
    "    arr_val_mse.append(val_mse)\n",
    "    arr_val_loss.append(val_loss)\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best delta: 0.7\n",
      "Best validation MAE: 0.0008989019552245736\n",
      "Best validation MSE: 1.7142334627351374e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results\n",
    "deltas = np.arange(0.5, 1.5, 0.1)\n",
    "plt.plot(deltas, arr_val_mae, marker='o', linestyle='-', color='C0', label='MAE')\n",
    "plt.plot(deltas, arr_val_mse, marker='o', linestyle='-', color='C1', label='MSE')\n",
    "plt.plot(deltas, arr_val_loss, marker='o', linestyle='-', color='C2', label='Huber Loss')\n",
    "plt.legend()\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0124WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 0.0124 - val_loss: 1.4695e-04\n",
      "Epoch 2/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 7.2352e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 7.0489e-04 - val_loss: 3.3615e-04\n",
      "Epoch 3/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 4.9224e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 4.8774e-04 - val_loss: 1.1937e-04\n",
      "Epoch 4/21\n",
      "3032/3125 [============================>.] - ETA: 0s - loss: 3.0539e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 503us/step - loss: 2.9723e-04 - val_loss: 4.5692e-05\n",
      "Epoch 5/21\n",
      "3092/3125 [============================>.] - ETA: 0s - loss: 2.7563e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 2.7682e-04 - val_loss: 2.9324e-04\n",
      "Epoch 6/21\n",
      "3085/3125 [============================>.] - ETA: 0s - loss: 2.5836e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 2.5597e-04 - val_loss: 8.6236e-05\n",
      "Epoch 7/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.7472e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.9329e-04 - val_loss: 5.5759e-05\n",
      "Epoch 8/21\n",
      "3059/3125 [============================>.] - ETA: 0s - loss: 1.1668e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 517us/step - loss: 1.1497e-04 - val_loss: 2.0861e-04\n",
      "Epoch 9/21\n",
      "3111/3125 [============================>.] - ETA: 0s - loss: 1.6754e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 1.6720e-04 - val_loss: 1.1083e-04\n",
      "Epoch 10/21\n",
      "3078/3125 [============================>.] - ETA: 0s - loss: 1.4426e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 515us/step - loss: 1.4221e-04 - val_loss: 4.7471e-06\n",
      "Epoch 11/21\n",
      "3018/3125 [===========================>..] - ETA: 0s - loss: 1.1278e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 506us/step - loss: 1.1056e-04 - val_loss: 1.0144e-05\n",
      "Epoch 12/21\n",
      "3084/3125 [============================>.] - ETA: 0s - loss: 1.7072e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 514us/step - loss: 1.6850e-04 - val_loss: 1.2806e-06\n",
      "Epoch 13/21\n",
      "3104/3125 [============================>.] - ETA: 0s - loss: 9.1941e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 512us/step - loss: 9.1351e-05 - val_loss: 1.4919e-05\n",
      "Epoch 14/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 1.0733e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 507us/step - loss: 1.0450e-04 - val_loss: 5.0571e-05\n",
      "Epoch 15/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 1.2246e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 1.2123e-04 - val_loss: 1.0353e-05\n",
      "Epoch 16/21\n",
      "3033/3125 [============================>.] - ETA: 0s - loss: 7.1371e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 504us/step - loss: 6.9639e-05 - val_loss: 1.1107e-05\n",
      "Epoch 17/21\n",
      "3074/3125 [============================>.] - ETA: 0s - loss: 9.0365e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 516us/step - loss: 8.9453e-05 - val_loss: 1.1710e-05\n",
      "Epoch 18/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 8.5348e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 511us/step - loss: 8.5123e-05 - val_loss: 2.5125e-04\n",
      "Epoch 19/21\n",
      "3024/3125 [============================>.] - ETA: 0s - loss: 1.2449e-04WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 505us/step - loss: 1.2051e-04 - val_loss: 1.3043e-06\n",
      "Epoch 20/21\n",
      "3089/3125 [============================>.] - ETA: 0s - loss: 7.9355e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 513us/step - loss: 7.8451e-05 - val_loss: 5.0948e-07\n",
      "Epoch 21/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 8.6113e-05WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 510us/step - loss: 8.3932e-05 - val_loss: 4.8413e-06\n",
      "Training Huber Loss: 8.393226016778499e-05\n",
      "Validation Huber Loss: 4.84133943245979e-06\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "                    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# create folder for models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3116/3125 [============================>.] - ETA: 0s - loss: 0.1236WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.1233 - val_loss: 0.0321\n",
      "Epoch 2/21\n",
      "3060/3125 [============================>.] - ETA: 0s - loss: 0.0195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0195 - val_loss: 0.0155\n",
      "Epoch 3/21\n",
      "3107/3125 [============================>.] - ETA: 0s - loss: 0.0136WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0136 - val_loss: 0.0111\n",
      "Epoch 4/21\n",
      "3100/3125 [============================>.] - ETA: 0s - loss: 0.0106WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0106 - val_loss: 0.0088\n",
      "Epoch 5/21\n",
      "3047/3125 [============================>.] - ETA: 0s - loss: 0.0085WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0085 - val_loss: 0.0072\n",
      "Epoch 6/21\n",
      "3121/3125 [============================>.] - ETA: 0s - loss: 0.0073WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 563us/step - loss: 0.0073 - val_loss: 0.0059\n",
      "Epoch 7/21\n",
      "3098/3125 [============================>.] - ETA: 0s - loss: 0.0082WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 0.0085 - val_loss: 0.0135\n",
      "Epoch 8/21\n",
      "3016/3125 [===========================>..] - ETA: 0s - loss: 0.0057WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 0.0057 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "3067/3125 [============================>.] - ETA: 0s - loss: 0.0069WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 533us/step - loss: 0.0068 - val_loss: 0.0050\n",
      "Epoch 10/21\n",
      "3019/3125 [===========================>..] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 0.0055 - val_loss: 0.0059\n",
      "Epoch 11/21\n",
      "3043/3125 [============================>.] - ETA: 0s - loss: 0.0055WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 555us/step - loss: 0.0055 - val_loss: 0.0046\n",
      "Epoch 12/21\n",
      "3050/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 0.0054 - val_loss: 0.0044\n",
      "Epoch 13/21\n",
      "3076/3125 [============================>.] - ETA: 0s - loss: 0.0060WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0060 - val_loss: 0.0044\n",
      "Epoch 14/21\n",
      "3020/3125 [===========================>..] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.0048 - val_loss: 0.0042\n",
      "Epoch 15/21\n",
      "3048/3125 [============================>.] - ETA: 0s - loss: 0.0054WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 0.0054 - val_loss: 0.0042\n",
      "Epoch 16/21\n",
      "3025/3125 [============================>.] - ETA: 0s - loss: 0.0050WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 562us/step - loss: 0.0049 - val_loss: 0.0040\n",
      "Epoch 17/21\n",
      "3079/3125 [============================>.] - ETA: 0s - loss: 0.0058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 550us/step - loss: 0.0058 - val_loss: 0.0040\n",
      "Epoch 18/21\n",
      "3119/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 594us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "3066/3125 [============================>.] - ETA: 0s - loss: 0.0049WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 534us/step - loss: 0.0049 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "3030/3125 [============================>.] - ETA: 0s - loss: 0.0048WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.0049 - val_loss: 0.0114\n",
      "Epoch 21/21\n",
      "3049/3125 [============================>.] - ETA: 0s - loss: 0.0045WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "3125/3125 [==============================] - 2s 519us/step - loss: 0.0045 - val_loss: 0.0037\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Huber Loss: 0.00447514234110713\n",
      "Validation Huber Loss: 0.003721917513757944\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_95306/2299525531.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=20, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3913 - val_loss: 0.0504\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0354 - val_loss: 0.0248\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0214 - val_loss: 0.0172\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0180 - val_loss: 0.0160\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0132 - val_loss: 0.0119\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0118 - val_loss: 0.0102\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0102 - val_loss: 0.0092\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0085 - val_loss: 0.0077\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0071 - val_loss: 0.0065\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0067 - val_loss: 0.0061\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0060 - val_loss: 0.0060\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  47.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4063 - val_loss: 0.0449\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0310 - val_loss: 0.0262\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0198 - val_loss: 0.0165\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0155 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0125 - val_loss: 0.0111\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0107 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0075 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0052 - val_loss: 0.0051\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0049 - val_loss: 0.0049\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3926 - val_loss: 0.0517\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0350 - val_loss: 0.0245\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0208 - val_loss: 0.0169\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0165 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0098 - val_loss: 0.0106\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0086\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0080 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0070 - val_loss: 0.0071\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0063 - val_loss: 0.0056\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0057 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0051 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3740 - val_loss: 0.0547\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0360 - val_loss: 0.0254\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0214 - val_loss: 0.0179\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0173 - val_loss: 0.0184\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0138 - val_loss: 0.0121\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0116 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0107 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0090 - val_loss: 0.0085\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0085 - val_loss: 0.0078\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0077 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0071 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0061 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0056 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4054 - val_loss: 0.0512\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0364 - val_loss: 0.0256\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0220 - val_loss: 0.0176\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0169 - val_loss: 0.0141\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0137 - val_loss: 0.0115\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0111 - val_loss: 0.0101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0099 - val_loss: 0.0097\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0087 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0085 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 705us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0064 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0056 - val_loss: 0.0057\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4548 - val_loss: 1.5654\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.4229 - val_loss: 1.5156\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2551 - val_loss: 1.6405\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.4295 - val_loss: 1.2983\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 1.1793 - val_loss: 1.1226\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 1.1613 - val_loss: 2.0160\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2354 - val_loss: 1.1302\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 1.1498 - val_loss: 0.7400\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1035 - val_loss: 0.9781\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0542 - val_loss: 1.7383\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1506 - val_loss: 1.5261\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.2901 - val_loss: 1.2529\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0950 - val_loss: 0.9223\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.0057 - val_loss: 1.2830\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 1.1760 - val_loss: 1.9218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1963 - val_loss: 1.1139\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0766 - val_loss: 1.0442\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 1.0828 - val_loss: 0.9278\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 1.0241 - val_loss: 1.2778\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.0451 - val_loss: 1.1896\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.1658 - val_loss: 0.8618\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4894 - val_loss: 1.5536\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 681us/step - loss: 1.3949 - val_loss: 0.8424\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.2348 - val_loss: 2.4273\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 686us/step - loss: 1.3126 - val_loss: 1.0281\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.1545 - val_loss: 1.8219\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.2431 - val_loss: 1.7817\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 1.1064 - val_loss: 0.9372\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 1.1494 - val_loss: 1.1165\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 1.0696 - val_loss: 1.1350\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 1.2778 - val_loss: 1.2750\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.9945 - val_loss: 1.0512\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9584 - val_loss: 0.8012\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0732 - val_loss: 1.2680\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0877 - val_loss: 0.7701\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.0771 - val_loss: 1.2036\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1390 - val_loss: 1.3090\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 1.0615 - val_loss: 1.4114\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 1.1238 - val_loss: 0.6854\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.1122 - val_loss: 1.5386\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0594 - val_loss: 1.5226\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 1.0355 - val_loss: 0.7678\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3110 - val_loss: 1.8916\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 1.5598 - val_loss: 0.9401\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2016 - val_loss: 3.6472\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.3655 - val_loss: 1.1979\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1794 - val_loss: 0.9721\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.0479 - val_loss: 0.7877\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.1509 - val_loss: 1.0790\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1672 - val_loss: 1.0041\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.1202 - val_loss: 1.1706\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.0972 - val_loss: 1.0665\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1506 - val_loss: 0.9168\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 1.0471 - val_loss: 0.7631\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.0863 - val_loss: 1.4450\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.0688 - val_loss: 0.6892\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.0769 - val_loss: 1.9591\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 1.0388 - val_loss: 0.6801\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0011 - val_loss: 0.8488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 1.0507 - val_loss: 1.0638\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 1.2632 - val_loss: 1.5597\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 1.0690 - val_loss: 1.5742\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 1.0258 - val_loss: 1.2597\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.4352 - val_loss: 2.3524\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.4732 - val_loss: 1.8195\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 1.3513 - val_loss: 1.6046\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2815 - val_loss: 1.0628\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 1.2549 - val_loss: 0.8947\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.3595 - val_loss: 1.0425\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 1.2575 - val_loss: 1.1783\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 1.1457 - val_loss: 1.1053\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 1.2386 - val_loss: 0.8328\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 1.1121 - val_loss: 0.7683\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0800 - val_loss: 0.8122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1334 - val_loss: 1.0681\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 1.1360 - val_loss: 1.7227\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.1523 - val_loss: 0.8457\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 1.0846 - val_loss: 1.0112\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 1.1795 - val_loss: 1.2654\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 1.1770 - val_loss: 2.9915\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 1.1655 - val_loss: 1.0430\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 1.1196 - val_loss: 0.6694\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 1.1168 - val_loss: 0.9777\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.9897 - val_loss: 1.5852\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  48.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2536 - val_loss: 1.4058\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.4792 - val_loss: 1.9845\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 1.3039 - val_loss: 1.2030\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 1.3195 - val_loss: 2.4667\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 1.2003 - val_loss: 0.9571\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 1.1969 - val_loss: 1.6163\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 1.0961 - val_loss: 1.3730\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 1.0934 - val_loss: 1.9173\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 1.1045 - val_loss: 1.4569\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.9799 - val_loss: 0.7403\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 1.3255 - val_loss: 1.5843\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 1.2985 - val_loss: 1.4799\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 1.0522 - val_loss: 1.1813\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 1.0945 - val_loss: 1.1196\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 1.2515 - val_loss: 1.6401\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 1.1444 - val_loss: 1.0283\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 1.2275 - val_loss: 1.8738\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 1.0204 - val_loss: 0.6723\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 1.1011 - val_loss: 0.8072\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0705 - val_loss: 1.9125\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 1.0199 - val_loss: 1.0806\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  47.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3781 - val_loss: 0.4339\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.1522 - val_loss: 0.0624\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0513 - val_loss: 0.0401\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0392 - val_loss: 0.0306\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0310 - val_loss: 0.0303\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0244 - val_loss: 0.0202\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0201 - val_loss: 0.0183\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0199 - val_loss: 0.0159\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0174 - val_loss: 0.0228\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0156 - val_loss: 0.0133\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0140 - val_loss: 0.0125\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0127 - val_loss: 0.0115\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0114 - val_loss: 0.0107\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.0107 - val_loss: 0.0103\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0106 - val_loss: 0.0099\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0114 - val_loss: 0.0098\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0106 - val_loss: 0.0093\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0103 - val_loss: 0.0094\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 512us/step - loss: 0.0098 - val_loss: 0.0091\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3577 - val_loss: 0.4205\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.1412 - val_loss: 0.0505\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 448us/step - loss: 0.0395 - val_loss: 0.0309\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0283 - val_loss: 0.0222\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0219 - val_loss: 0.0179\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0205 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0170 - val_loss: 0.0140\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0155 - val_loss: 0.0136\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0147 - val_loss: 0.0117\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0135 - val_loss: 0.0110\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0128 - val_loss: 0.0108\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0105\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0098 - val_loss: 0.0093\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0095 - val_loss: 0.0089\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0088 - val_loss: 0.0086\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0085 - val_loss: 0.0083\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0082 - val_loss: 0.0081\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 453us/step - loss: 0.0080 - val_loss: 0.0079\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0078 - val_loss: 0.0077\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0076 - val_loss: 0.0076\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3143 - val_loss: 0.3964\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1338 - val_loss: 0.0543\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0444 - val_loss: 0.0361\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0357 - val_loss: 0.0506\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0293 - val_loss: 0.0239\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0274 - val_loss: 0.0221\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0238 - val_loss: 0.0209\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0212 - val_loss: 0.0182\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0176 - val_loss: 0.0222\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0164 - val_loss: 0.0135\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0154 - val_loss: 0.0128\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0158 - val_loss: 0.0131\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0169 - val_loss: 0.0113\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0136 - val_loss: 0.0106\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0147 - val_loss: 0.0102\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0105 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0101 - val_loss: 0.0091\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0089 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0092 - val_loss: 0.0092\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0084 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  41.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.3639 - val_loss: 0.4114\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.1400 - val_loss: 0.0602\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0439 - val_loss: 0.0363\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0306 - val_loss: 0.0254\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0254 - val_loss: 0.0205\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0222 - val_loss: 0.0173\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0196 - val_loss: 0.0166\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0163 - val_loss: 0.0142\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0142 - val_loss: 0.0130\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0128 - val_loss: 0.0119\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0119 - val_loss: 0.0114\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0106 - val_loss: 0.0103\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0106 - val_loss: 0.0101\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0108 - val_loss: 0.0101\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0108 - val_loss: 0.0093\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0107 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0096 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0099 - val_loss: 0.0086\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0102 - val_loss: 0.0085\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  43.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3247 - val_loss: 0.4125\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1398 - val_loss: 0.0532\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0414 - val_loss: 0.0324\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0292 - val_loss: 0.0250\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0263 - val_loss: 0.0200\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0218 - val_loss: 0.0180\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0196 - val_loss: 0.0212\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0181 - val_loss: 0.0150\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0166 - val_loss: 0.0907\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0165 - val_loss: 0.0143\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0138 - val_loss: 0.0122\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0123 - val_loss: 0.0116\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0114 - val_loss: 0.0109\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0108 - val_loss: 0.0097\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0121 - val_loss: 0.0095\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0101 - val_loss: 0.0090\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0112 - val_loss: 0.0088\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0111 - val_loss: 0.0084\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0088 - val_loss: 0.0082\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  44.1s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 811us/step - loss: 2.0495 - val_loss: 1.8333\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.9853 - val_loss: 1.0758\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 2.0177 - val_loss: 1.7394\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 571us/step - loss: 1.8155 - val_loss: 0.6018\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5840 - val_loss: 1.1553\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.7037 - val_loss: 1.7619\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6147 - val_loss: 1.4786\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.6448 - val_loss: 1.1432\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.7160 - val_loss: 6.7320\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 559us/step - loss: 1.5923 - val_loss: 1.7710\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6038 - val_loss: 0.8006\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.5625 - val_loss: 3.3897\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 584us/step - loss: 1.6752 - val_loss: 3.7497\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.5930 - val_loss: 1.3925\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.7788 - val_loss: 1.2418\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5695 - val_loss: 1.1778\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.4755 - val_loss: 1.0965\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6045 - val_loss: 0.8042\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6140 - val_loss: 2.1645\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6665 - val_loss: 3.2180\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 566us/step - loss: 1.5921 - val_loss: 1.6398\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 795us/step - loss: 2.0826 - val_loss: 1.6291\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.8595 - val_loss: 2.8289\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 551us/step - loss: 1.7470 - val_loss: 1.6122\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 526us/step - loss: 1.7286 - val_loss: 1.8451\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6521 - val_loss: 0.9254\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.7053 - val_loss: 2.2272\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.5833 - val_loss: 1.3344\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7034 - val_loss: 1.0786\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 539us/step - loss: 1.6402 - val_loss: 2.0677\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6700 - val_loss: 0.8380\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 576us/step - loss: 1.5911 - val_loss: 0.9440\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6941 - val_loss: 1.8238\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6054 - val_loss: 1.3997\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6051 - val_loss: 1.0353\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.5952 - val_loss: 0.9604\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.6235 - val_loss: 1.1089\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 573us/step - loss: 1.8128 - val_loss: 0.7699\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7668 - val_loss: 0.8111\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6631 - val_loss: 2.6912\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 4s 562us/step - loss: 1.7038 - val_loss: 0.7816\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.7381 - val_loss: 1.8475\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 824us/step - loss: 2.0848 - val_loss: 1.8964\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 537us/step - loss: 1.9604 - val_loss: 0.9799\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 538us/step - loss: 1.7706 - val_loss: 2.7968\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 4s 581us/step - loss: 1.8178 - val_loss: 0.8405\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.8514 - val_loss: 3.7617\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 542us/step - loss: 1.6464 - val_loss: 1.2769\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 4s 572us/step - loss: 1.7299 - val_loss: 2.0901\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6110 - val_loss: 3.6727\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5695 - val_loss: 0.9895\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6302 - val_loss: 2.2467\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 528us/step - loss: 1.7181 - val_loss: 2.0060\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.5945 - val_loss: 1.1329\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.7250 - val_loss: 0.9861\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 543us/step - loss: 1.6462 - val_loss: 1.3101\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.8206 - val_loss: 1.6681\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.6675 - val_loss: 0.9190\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.5987 - val_loss: 0.9040\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6117 - val_loss: 0.9613\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.7704 - val_loss: 0.7324\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.6189 - val_loss: 3.5304\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.5071 - val_loss: 3.6898\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 791us/step - loss: 2.2296 - val_loss: 2.6894\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8376 - val_loss: 1.2147\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7435 - val_loss: 0.9304\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.8264 - val_loss: 1.1540\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 558us/step - loss: 1.8282 - val_loss: 1.8848\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7473 - val_loss: 2.3212\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 535us/step - loss: 1.8021 - val_loss: 2.3163\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 4s 563us/step - loss: 1.6223 - val_loss: 2.0090\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 549us/step - loss: 1.6435 - val_loss: 0.8118\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 3s 530us/step - loss: 1.7447 - val_loss: 4.3669\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 4s 574us/step - loss: 1.6616 - val_loss: 1.0755\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 541us/step - loss: 1.7779 - val_loss: 1.2148\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 540us/step - loss: 1.6262 - val_loss: 3.5810\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.6404 - val_loss: 1.0313\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 534us/step - loss: 1.7011 - val_loss: 1.3704\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.6487 - val_loss: 3.3312\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 4s 569us/step - loss: 1.7005 - val_loss: 0.7953\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.6248 - val_loss: 1.2721\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 4s 560us/step - loss: 1.7068 - val_loss: 1.7680\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 557us/step - loss: 1.7333 - val_loss: 0.5884\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6675 - val_loss: 0.9908\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.2min\n",
      "Epoch 1/21\n",
      "6250/6250 [==============================] - 5s 774us/step - loss: 2.1779 - val_loss: 1.4063\n",
      "Epoch 2/21\n",
      "6250/6250 [==============================] - 3s 525us/step - loss: 1.8682 - val_loss: 3.0732\n",
      "Epoch 3/21\n",
      "6250/6250 [==============================] - 3s 532us/step - loss: 1.8061 - val_loss: 0.7397\n",
      "Epoch 4/21\n",
      "6250/6250 [==============================] - 3s 536us/step - loss: 1.6944 - val_loss: 1.5711\n",
      "Epoch 5/21\n",
      "6250/6250 [==============================] - 3s 545us/step - loss: 1.7009 - val_loss: 3.3226\n",
      "Epoch 6/21\n",
      "6250/6250 [==============================] - 4s 575us/step - loss: 1.6455 - val_loss: 1.0860\n",
      "Epoch 7/21\n",
      "6250/6250 [==============================] - 3s 560us/step - loss: 1.8057 - val_loss: 1.3950\n",
      "Epoch 8/21\n",
      "6250/6250 [==============================] - 3s 527us/step - loss: 1.6595 - val_loss: 3.0940\n",
      "Epoch 9/21\n",
      "6250/6250 [==============================] - 3s 554us/step - loss: 1.8185 - val_loss: 1.0772\n",
      "Epoch 10/21\n",
      "6250/6250 [==============================] - 4s 567us/step - loss: 1.7835 - val_loss: 2.3305\n",
      "Epoch 11/21\n",
      "6250/6250 [==============================] - 3s 522us/step - loss: 1.6996 - val_loss: 2.7528\n",
      "Epoch 12/21\n",
      "6250/6250 [==============================] - 3s 556us/step - loss: 1.8290 - val_loss: 1.8732\n",
      "Epoch 13/21\n",
      "6250/6250 [==============================] - 3s 548us/step - loss: 1.7584 - val_loss: 2.3031\n",
      "Epoch 14/21\n",
      "6250/6250 [==============================] - 3s 531us/step - loss: 1.6866 - val_loss: 1.2577\n",
      "Epoch 15/21\n",
      "6250/6250 [==============================] - 3s 553us/step - loss: 1.7467 - val_loss: 2.4673\n",
      "Epoch 16/21\n",
      "6250/6250 [==============================] - 3s 552us/step - loss: 1.6664 - val_loss: 1.1721\n",
      "Epoch 17/21\n",
      "6250/6250 [==============================] - 3s 533us/step - loss: 1.8591 - val_loss: 3.9664\n",
      "Epoch 18/21\n",
      "6250/6250 [==============================] - 4s 570us/step - loss: 1.9158 - val_loss: 3.0424\n",
      "Epoch 19/21\n",
      "6250/6250 [==============================] - 3s 546us/step - loss: 1.6148 - val_loss: 0.9112\n",
      "Epoch 20/21\n",
      "6250/6250 [==============================] - 3s 529us/step - loss: 1.5801 - val_loss: 0.9372\n",
      "Epoch 21/21\n",
      "6250/6250 [==============================] - 3s 555us/step - loss: 1.6065 - val_loss: 0.7728\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 2.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.3138 - val_loss: 0.4532\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2757 - val_loss: 0.5570\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2025 - val_loss: 1.0355\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1667 - val_loss: 1.0184\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.2145 - val_loss: 0.8794\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 538us/step - loss: 1.0625 - val_loss: 0.8471\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 525us/step - loss: 1.1149 - val_loss: 2.7763\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.1695 - val_loss: 1.3112\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.1349 - val_loss: 0.7711\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2292 - val_loss: 0.6008\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 596us/step - loss: 1.1854 - val_loss: 0.4148\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1794 - val_loss: 1.3229\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1348 - val_loss: 0.4279\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 564us/step - loss: 1.1109 - val_loss: 0.9632\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1120 - val_loss: 0.6256\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 1.1666 - val_loss: 0.7041\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0740 - val_loss: 2.1306\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 539us/step - loss: 1.1004 - val_loss: 1.3319\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1979 - val_loss: 1.4518\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 559us/step - loss: 1.0777 - val_loss: 1.4715\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.0911 - val_loss: 0.9016\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.4827 - val_loss: 1.4923\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.4227 - val_loss: 1.4094\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.3079 - val_loss: 1.0970\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.3872 - val_loss: 0.9075\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 595us/step - loss: 1.2271 - val_loss: 1.0691\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.2792 - val_loss: 0.4303\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.1838 - val_loss: 1.6258\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2704 - val_loss: 2.6877\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1844 - val_loss: 0.9091\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.2034 - val_loss: 1.2839\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.3089 - val_loss: 1.0650\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 606us/step - loss: 1.2428 - val_loss: 1.8647\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.1691 - val_loss: 1.6116\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 548us/step - loss: 1.2043 - val_loss: 0.8117\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.0830 - val_loss: 0.8571\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.1902 - val_loss: 1.5128\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.1331 - val_loss: 0.8823\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 568us/step - loss: 1.1470 - val_loss: 1.1137\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 552us/step - loss: 1.0958 - val_loss: 1.7578\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1036 - val_loss: 0.7895\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0959 - val_loss: 2.5830\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.5119 - val_loss: 1.0155\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 579us/step - loss: 1.2930 - val_loss: 0.8148\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 581us/step - loss: 1.2643 - val_loss: 0.4344\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.3187 - val_loss: 2.0839\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.1392 - val_loss: 1.6559\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2644 - val_loss: 0.8539\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2365 - val_loss: 0.7371\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1408 - val_loss: 0.6687\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.0993 - val_loss: 0.8836\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 537us/step - loss: 1.0337 - val_loss: 0.7442\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.2246 - val_loss: 1.0844\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 0.9990 - val_loss: 1.1175\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2760 - val_loss: 2.3433\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.1260 - val_loss: 0.6654\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.0991 - val_loss: 0.8295\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 0.9976 - val_loss: 1.2872\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 0.9738 - val_loss: 0.5575\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 0.9822 - val_loss: 0.5591\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1437 - val_loss: 2.8904\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.1838 - val_loss: 0.6201\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.0390 - val_loss: 1.1287\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.3537 - val_loss: 3.2304\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 557us/step - loss: 1.2241 - val_loss: 0.7114\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.2891 - val_loss: 0.8875\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.2299 - val_loss: 1.2056\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 549us/step - loss: 1.2072 - val_loss: 1.4694\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 565us/step - loss: 1.2006 - val_loss: 0.9376\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2708 - val_loss: 0.5127\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 573us/step - loss: 1.2289 - val_loss: 0.4724\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1163 - val_loss: 1.9452\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 608us/step - loss: 1.1768 - val_loss: 2.2652\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2898 - val_loss: 0.9334\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 547us/step - loss: 1.1127 - val_loss: 0.5052\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 543us/step - loss: 1.0420 - val_loss: 1.0299\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 574us/step - loss: 0.9830 - val_loss: 1.3125\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.1674 - val_loss: 0.4857\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.1020 - val_loss: 0.7583\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.0609 - val_loss: 0.6556\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 541us/step - loss: 1.1073 - val_loss: 0.6437\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1356 - val_loss: 1.4740\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 571us/step - loss: 1.0726 - val_loss: 0.5107\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 583us/step - loss: 0.9805 - val_loss: 0.7234\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Epoch 1/21\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 1.5645 - val_loss: 0.9579\n",
      "Epoch 2/21\n",
      "3125/3125 [==============================] - 2s 545us/step - loss: 1.3933 - val_loss: 2.0165\n",
      "Epoch 3/21\n",
      "3125/3125 [==============================] - 2s 540us/step - loss: 1.2286 - val_loss: 0.7878\n",
      "Epoch 4/21\n",
      "3125/3125 [==============================] - 2s 554us/step - loss: 1.0514 - val_loss: 1.3322\n",
      "Epoch 5/21\n",
      "3125/3125 [==============================] - 2s 570us/step - loss: 1.2913 - val_loss: 3.2380\n",
      "Epoch 6/21\n",
      "3125/3125 [==============================] - 2s 576us/step - loss: 1.1876 - val_loss: 0.8611\n",
      "Epoch 7/21\n",
      "3125/3125 [==============================] - 2s 560us/step - loss: 1.1269 - val_loss: 1.5577\n",
      "Epoch 8/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 0.9962 - val_loss: 0.6705\n",
      "Epoch 9/21\n",
      "3125/3125 [==============================] - 2s 542us/step - loss: 1.2040 - val_loss: 1.1424\n",
      "Epoch 10/21\n",
      "3125/3125 [==============================] - 2s 577us/step - loss: 1.2394 - val_loss: 1.0566\n",
      "Epoch 11/21\n",
      "3125/3125 [==============================] - 2s 605us/step - loss: 1.1684 - val_loss: 1.4895\n",
      "Epoch 12/21\n",
      "3125/3125 [==============================] - 2s 603us/step - loss: 1.1026 - val_loss: 0.7058\n",
      "Epoch 13/21\n",
      "3125/3125 [==============================] - 2s 546us/step - loss: 1.1220 - val_loss: 1.2121\n",
      "Epoch 14/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1655 - val_loss: 1.4523\n",
      "Epoch 15/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.2646 - val_loss: 0.8966\n",
      "Epoch 16/21\n",
      "3125/3125 [==============================] - 2s 567us/step - loss: 1.0991 - val_loss: 0.4141\n",
      "Epoch 17/21\n",
      "3125/3125 [==============================] - 2s 569us/step - loss: 1.0723 - val_loss: 1.9051\n",
      "Epoch 18/21\n",
      "3125/3125 [==============================] - 2s 572us/step - loss: 1.0062 - val_loss: 1.5180\n",
      "Epoch 19/21\n",
      "3125/3125 [==============================] - 2s 544us/step - loss: 1.1790 - val_loss: 1.8682\n",
      "Epoch 20/21\n",
      "3125/3125 [==============================] - 2s 558us/step - loss: 1.0427 - val_loss: 0.5334\n",
      "Epoch 21/21\n",
      "3125/3125 [==============================] - 2s 590us/step - loss: 1.1291 - val_loss: 1.4961\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.3min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.5009 - val_loss: 0.1091\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.1017 - val_loss: 0.0809\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0805 - val_loss: 0.0724\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0688 - val_loss: 0.0596\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0604 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 704us/step - loss: 0.0544 - val_loss: 0.0579\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0499 - val_loss: 0.0574\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 864us/step - loss: 0.0440 - val_loss: 0.0389\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 698us/step - loss: 0.0364 - val_loss: 0.0321\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.0315 - val_loss: 0.0298\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0292 - val_loss: 0.0270\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0276 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0261 - val_loss: 0.0240\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0253 - val_loss: 0.0297\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0238 - val_loss: 0.0218\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0230 - val_loss: 0.0213\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 696us/step - loss: 0.0223 - val_loss: 0.0222\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 675us/step - loss: 0.0217 - val_loss: 0.0201\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 720us/step - loss: 0.0209 - val_loss: 0.0205\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0203 - val_loss: 0.0189\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 683us/step - loss: 0.0198 - val_loss: 0.0212\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  51.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4908 - val_loss: 0.1065\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0989 - val_loss: 0.0824\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 682us/step - loss: 0.0800 - val_loss: 0.0764\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0679 - val_loss: 0.0597\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 629us/step - loss: 0.0601 - val_loss: 0.0615\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0543 - val_loss: 0.0469\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0482 - val_loss: 0.0461\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0409 - val_loss: 0.0342\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0335 - val_loss: 0.0304\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.0304 - val_loss: 0.0283\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0283 - val_loss: 0.0256\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0264 - val_loss: 0.0246\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0239 - val_loss: 0.0221\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0230 - val_loss: 0.0210\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0218 - val_loss: 0.0217\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0208 - val_loss: 0.0202\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 666us/step - loss: 0.0202 - val_loss: 0.0197\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0194 - val_loss: 0.0184\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.0190 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0181 - val_loss: 0.0171\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4765 - val_loss: 0.1089\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.1041 - val_loss: 0.0823\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0826 - val_loss: 0.0745\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0701 - val_loss: 0.0706\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0614 - val_loss: 0.0549\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0554 - val_loss: 0.0477\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0499 - val_loss: 0.0434\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0441 - val_loss: 0.0420\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0370 - val_loss: 0.0333\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0263\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0274 - val_loss: 0.0249\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0260 - val_loss: 0.0242\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0250 - val_loss: 0.0231\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0240 - val_loss: 0.0228\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0229 - val_loss: 0.0211\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0222 - val_loss: 0.0232\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0215 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0209 - val_loss: 0.0196\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0200 - val_loss: 0.0209\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 740us/step - loss: 0.0196 - val_loss: 0.0180\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4379 - val_loss: 0.1000\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0946 - val_loss: 0.0831\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0760 - val_loss: 0.0678\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0650 - val_loss: 0.0618\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0583 - val_loss: 0.0513\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0532 - val_loss: 0.0471\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0491 - val_loss: 0.0448\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0456 - val_loss: 0.0413\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0438 - val_loss: 0.0424\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0415 - val_loss: 0.0402\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0383 - val_loss: 0.0332\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0348 - val_loss: 0.0381\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0303 - val_loss: 0.0290\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0263 - val_loss: 0.0246\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0243 - val_loss: 0.0220\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0232 - val_loss: 0.0251\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0221 - val_loss: 0.0214\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 700us/step - loss: 0.0219 - val_loss: 0.0196\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.0207 - val_loss: 0.0191\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0202 - val_loss: 0.0192\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0196 - val_loss: 0.0184\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  49.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4648 - val_loss: 0.1101\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.1003 - val_loss: 0.0799\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0791 - val_loss: 0.0689\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0678 - val_loss: 0.0584\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0600 - val_loss: 0.0537\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0543 - val_loss: 0.0600\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0501 - val_loss: 0.0457\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0464 - val_loss: 0.0417\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0429 - val_loss: 0.0384\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0397 - val_loss: 0.0369\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0363 - val_loss: 0.0344\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0319 - val_loss: 0.0279\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0276 - val_loss: 0.0249\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0254 - val_loss: 0.0237\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0240 - val_loss: 0.0219\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0228 - val_loss: 0.0230\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0219 - val_loss: 0.0201\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0214 - val_loss: 0.0200\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0209 - val_loss: 0.0193\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0203 - val_loss: 0.0191\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0198 - val_loss: 0.0182\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  48.7s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8257 - val_loss: 1.0841\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.7961 - val_loss: 1.8390\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.9293 - val_loss: 0.7291\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7981 - val_loss: 0.4040\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.7961 - val_loss: 0.6338\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7606 - val_loss: 0.5805\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8357 - val_loss: 0.6535\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7379 - val_loss: 1.3158\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.7925 - val_loss: 0.8457\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.9728 - val_loss: 0.4094\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8004 - val_loss: 0.7565\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.7480 - val_loss: 1.6881\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.6774 - val_loss: 0.7345\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.7003 - val_loss: 0.9248\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.8209 - val_loss: 0.5067\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7357 - val_loss: 0.8436\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7064 - val_loss: 0.7397\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.6986 - val_loss: 0.4369\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.6661 - val_loss: 0.7733\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.7657 - val_loss: 0.5325\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8372 - val_loss: 0.6942\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.7540 - val_loss: 0.5935\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.9600 - val_loss: 0.5619\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.8767 - val_loss: 1.4013\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.7201 - val_loss: 0.8441\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7841 - val_loss: 1.2540\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.9837 - val_loss: 0.5844\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.7907 - val_loss: 0.8751\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.8221 - val_loss: 0.4917\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.6586 - val_loss: 0.6788\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.8244 - val_loss: 0.7687\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8070 - val_loss: 1.2177\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7881 - val_loss: 0.3000\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.7595 - val_loss: 1.0335\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.7706 - val_loss: 1.2513\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7115 - val_loss: 1.6043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.8340 - val_loss: 0.4869\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.7587 - val_loss: 0.4795\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7453 - val_loss: 0.5054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.8020 - val_loss: 1.2062\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.6782 - val_loss: 1.0712\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7217 - val_loss: 0.6947\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8259 - val_loss: 0.7914\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.8503 - val_loss: 1.4953\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 1.0301 - val_loss: 0.4811\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 695us/step - loss: 0.8100 - val_loss: 0.6207\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.8781 - val_loss: 0.6775\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7969 - val_loss: 0.7816\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.7632 - val_loss: 0.5114\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7226 - val_loss: 0.9786\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7845 - val_loss: 0.4548\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.6876 - val_loss: 1.2931\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8842 - val_loss: 0.7475\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7147 - val_loss: 0.8473\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.7138 - val_loss: 0.4855\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.8563 - val_loss: 0.6942\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7323 - val_loss: 0.5321\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.7344 - val_loss: 0.4130\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.6635 - val_loss: 0.4413\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.7459 - val_loss: 0.5090\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.7696 - val_loss: 0.6121\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.6943 - val_loss: 0.3167\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7159 - val_loss: 0.4275\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  48.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8108 - val_loss: 0.6632\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 1.0144 - val_loss: 1.3635\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.9622 - val_loss: 0.7523\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.7466 - val_loss: 0.7003\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8471 - val_loss: 2.2345\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.7158 - val_loss: 0.6101\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.7460 - val_loss: 0.9688\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.8945 - val_loss: 2.0750\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.8393 - val_loss: 0.7837\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.7145 - val_loss: 1.3448\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.8506 - val_loss: 0.8265\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.7636 - val_loss: 0.4143\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.7345 - val_loss: 0.7574\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.9312 - val_loss: 0.8654\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.7928 - val_loss: 1.1743\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.8166 - val_loss: 0.6067\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.7407 - val_loss: 0.6288\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.8150 - val_loss: 1.3362\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.7802 - val_loss: 0.6273\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.6838 - val_loss: 0.8952\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.6871 - val_loss: 0.7546\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  49.4s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.8026 - val_loss: 1.1283\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.9399 - val_loss: 1.0160\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.8103 - val_loss: 0.8321\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.8769 - val_loss: 1.0033\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 658us/step - loss: 0.6961 - val_loss: 0.8674\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.6724 - val_loss: 0.9293\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.8434 - val_loss: 0.4928\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.7881 - val_loss: 0.8932\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.6938 - val_loss: 0.3938\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.8286 - val_loss: 0.4843\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.6383 - val_loss: 0.5622\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.6835 - val_loss: 0.4465\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.8587 - val_loss: 0.9712\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7464 - val_loss: 0.6541\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.6609 - val_loss: 0.6444\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.7408 - val_loss: 0.3988\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.7057 - val_loss: 0.6988\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.7602 - val_loss: 1.0903\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.7159 - val_loss: 1.4128\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.6291 - val_loss: 0.8044\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.6761 - val_loss: 0.3876\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  50.0s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3594 - val_loss: 0.2418\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.1824 - val_loss: 0.1319\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.1013 - val_loss: 0.0742\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0582 - val_loss: 0.0431\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0349 - val_loss: 0.0266\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0152 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0113 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0090 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0076 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0067 - val_loss: 0.0058\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0062 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0059 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0044 - val_loss: 0.0041\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  42.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3683 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1846 - val_loss: 0.1333\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.1021 - val_loss: 0.0790\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0586 - val_loss: 0.0434\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0350 - val_loss: 0.0276\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0222 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0111 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0088 - val_loss: 0.0074\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0066 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0060 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0054 - val_loss: 0.0055\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0051 - val_loss: 0.0044\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0049 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0042 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3678 - val_loss: 0.2445\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.1842 - val_loss: 0.1327\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.1018 - val_loss: 0.0746\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0584 - val_loss: 0.0435\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0350 - val_loss: 0.0267\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0222 - val_loss: 0.0172\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0152 - val_loss: 0.0124\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0090 - val_loss: 0.0079\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0051 - val_loss: 0.0047\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0048 - val_loss: 0.0043\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0046 - val_loss: 0.0040\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0045 - val_loss: 0.0045\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0043 - val_loss: 0.0040\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  43.8s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3613 - val_loss: 0.2416\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.1823 - val_loss: 0.1308\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.1005 - val_loss: 0.0748\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0575 - val_loss: 0.0427\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0344 - val_loss: 0.0261\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0218 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0148 - val_loss: 0.0214\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0109 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0087 - val_loss: 0.0085\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0073 - val_loss: 0.0089\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0065 - val_loss: 0.0056\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0055 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0050 - val_loss: 0.0043\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0046 - val_loss: 0.0042\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0041 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0041 - val_loss: 0.0037\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3676 - val_loss: 0.2454\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.1854 - val_loss: 0.1341\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.1029 - val_loss: 0.0762\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0588 - val_loss: 0.0436\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0351 - val_loss: 0.0265\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0221 - val_loss: 0.0176\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0150 - val_loss: 0.0119\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0110 - val_loss: 0.0089\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0073 - val_loss: 0.0063\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0065 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 508us/step - loss: 0.0055 - val_loss: 0.0052\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0052 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0050 - val_loss: 0.0045\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0047 - val_loss: 0.0042\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0043 - val_loss: 0.0037\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0041 - val_loss: 0.0055\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  44.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2769 - val_loss: 0.2397\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.1453 - val_loss: 0.1188\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.1091 - val_loss: 0.0981\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0972 - val_loss: 0.0922\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0899 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0847 - val_loss: 0.0765\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0800 - val_loss: 0.1352\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0759 - val_loss: 0.0872\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0728 - val_loss: 0.0686\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0702 - val_loss: 0.0866\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0674 - val_loss: 0.0599\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0640 - val_loss: 0.0720\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0635 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0602 - val_loss: 0.0559\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0584 - val_loss: 0.0525\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0573 - val_loss: 0.0563\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0564 - val_loss: 0.0494\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0557 - val_loss: 0.0490\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0548 - val_loss: 0.0563\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0526 - val_loss: 0.0657\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0521 - val_loss: 0.0447\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2681 - val_loss: 0.2390\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.1428 - val_loss: 0.1090\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.1058 - val_loss: 0.0928\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0947 - val_loss: 0.0967\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0871 - val_loss: 0.0765\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0816 - val_loss: 0.0699\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 517us/step - loss: 0.0775 - val_loss: 0.0949\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0730 - val_loss: 0.0666\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0698 - val_loss: 0.0621\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0667 - val_loss: 0.0601\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0642 - val_loss: 0.0535\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0614 - val_loss: 0.0562\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0605 - val_loss: 0.0663\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 490us/step - loss: 0.0575 - val_loss: 0.0528\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0562 - val_loss: 0.0489\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0556 - val_loss: 0.1579\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0539 - val_loss: 0.0491\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0528 - val_loss: 0.0480\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0525 - val_loss: 0.0819\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0511 - val_loss: 0.0474\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0501 - val_loss: 0.0466\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.0s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3061 - val_loss: 0.2437\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.1441 - val_loss: 0.1102\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.1083 - val_loss: 0.0982\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0971 - val_loss: 0.0875\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 515us/step - loss: 0.0890 - val_loss: 0.1368\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0838 - val_loss: 0.0713\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0791 - val_loss: 0.0713\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0747 - val_loss: 0.0696\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0725 - val_loss: 0.0610\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0689 - val_loss: 0.0759\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0668 - val_loss: 0.0687\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0641 - val_loss: 0.0774\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0621 - val_loss: 0.0567\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0598 - val_loss: 0.0794\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0588 - val_loss: 0.0523\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 497us/step - loss: 0.0574 - val_loss: 0.0530\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 492us/step - loss: 0.0561 - val_loss: 0.0527\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0549 - val_loss: 0.0510\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0535 - val_loss: 0.0495\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0532 - val_loss: 0.0458\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0523 - val_loss: 0.0474\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2884 - val_loss: 0.2423\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.1446 - val_loss: 0.1139\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.1081 - val_loss: 0.0931\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0969 - val_loss: 0.0828\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 504us/step - loss: 0.0887 - val_loss: 0.0761\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0833 - val_loss: 0.0905\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0790 - val_loss: 0.0874\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0765 - val_loss: 0.0652\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0713 - val_loss: 0.0870\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0694 - val_loss: 0.0638\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 501us/step - loss: 0.0658 - val_loss: 0.0615\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0650 - val_loss: 0.0570\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0624 - val_loss: 0.1057\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 499us/step - loss: 0.0611 - val_loss: 0.0652\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0585 - val_loss: 0.0502\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0583 - val_loss: 0.0559\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0575 - val_loss: 0.2498\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0557 - val_loss: 0.0494\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0549 - val_loss: 0.0465\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0541 - val_loss: 0.0468\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0525 - val_loss: 0.0620\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  43.2s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3054 - val_loss: 0.2460\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.1494 - val_loss: 0.1231\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.1100 - val_loss: 0.1390\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0962 - val_loss: 0.1000\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0885 - val_loss: 0.0773\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0813 - val_loss: 0.0716\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 505us/step - loss: 0.0769 - val_loss: 0.0676\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0726 - val_loss: 0.0648\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0693 - val_loss: 0.0599\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0667 - val_loss: 0.1222\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0645 - val_loss: 0.0580\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0622 - val_loss: 0.0556\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0600 - val_loss: 0.0534\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0586 - val_loss: 0.0526\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0568 - val_loss: 0.0568\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0560 - val_loss: 0.0509\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0540 - val_loss: 0.0488\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0534 - val_loss: 0.0505\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 506us/step - loss: 0.0532 - val_loss: 0.0480\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0515 - val_loss: 0.0466\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 510us/step - loss: 0.0510 - val_loss: 0.0433\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  44.3s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3760 - val_loss: 0.0559\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0393 - val_loss: 0.0275\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0236 - val_loss: 0.0183\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 661us/step - loss: 0.0168 - val_loss: 0.0143\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0136 - val_loss: 0.0118\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0117 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 727us/step - loss: 0.0103 - val_loss: 0.0095\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0095 - val_loss: 0.0084\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 652us/step - loss: 0.0085 - val_loss: 0.0096\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0079 - val_loss: 0.0073\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 669us/step - loss: 0.0069 - val_loss: 0.0062\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0062 - val_loss: 0.0078\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0065 - val_loss: 0.0058\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0058 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 660us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 728us/step - loss: 0.0056 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0055 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.7s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3673 - val_loss: 0.0431\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0298 - val_loss: 0.0211\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0189 - val_loss: 0.0153\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0146 - val_loss: 0.0125\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0127 - val_loss: 0.0112\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0108 - val_loss: 0.0106\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0102 - val_loss: 0.0087\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0085 - val_loss: 0.0094\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0070\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0069 - val_loss: 0.0063\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 692us/step - loss: 0.0060 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0055 - val_loss: 0.0054\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0052 - val_loss: 0.0050\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.5s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4291 - val_loss: 0.0434\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0306 - val_loss: 0.0227\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0196 - val_loss: 0.0160\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 628us/step - loss: 0.0151 - val_loss: 0.0132\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 659us/step - loss: 0.0124 - val_loss: 0.0109\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0101 - val_loss: 0.0094\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0088 - val_loss: 0.0080\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0067 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0068 - val_loss: 0.0060\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0060 - val_loss: 0.0059\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0058 - val_loss: 0.0054\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0055 - val_loss: 0.0064\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0054 - val_loss: 0.0056\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0052 - val_loss: 0.0048\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0050 - val_loss: 0.0067\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.3s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3826 - val_loss: 0.0523\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0364 - val_loss: 0.0265\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0220 - val_loss: 0.0188\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0161 - val_loss: 0.0136\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0135 - val_loss: 0.0117\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0108 - val_loss: 0.0099\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0097 - val_loss: 0.0086\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0086 - val_loss: 0.0082\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0076 - val_loss: 0.0072\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.0058 - val_loss: 0.0056\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 755us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 668us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 711us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  50.9s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3885 - val_loss: 0.0525\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0369 - val_loss: 0.0272\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.0231 - val_loss: 0.0185\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 724us/step - loss: 0.0169 - val_loss: 0.0140\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 838us/step - loss: 0.0132 - val_loss: 0.0122\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.0114 - val_loss: 0.0103\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 2s 990us/step - loss: 0.0100 - val_loss: 0.0090\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 712us/step - loss: 0.0085 - val_loss: 0.0112\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0077 - val_loss: 0.0071\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0071 - val_loss: 0.0069\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0062 - val_loss: 0.0058\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0061 - val_loss: 0.0057\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0057 - val_loss: 0.0061\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 636us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0052 - val_loss: 0.0054\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  52.6s\n",
      "Epoch 1/21\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3564 - val_loss: 0.2425\n",
      "Epoch 2/21\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.1828 - val_loss: 0.1314\n",
      "Epoch 3/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.1011 - val_loss: 0.0740\n",
      "Epoch 4/21\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0580 - val_loss: 0.0430\n",
      "Epoch 5/21\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0348 - val_loss: 0.0263\n",
      "Epoch 6/21\n",
      "1563/1563 [==============================] - 1s 502us/step - loss: 0.0221 - val_loss: 0.0171\n",
      "Epoch 7/21\n",
      "1563/1563 [==============================] - 1s 503us/step - loss: 0.0151 - val_loss: 0.0120\n",
      "Epoch 8/21\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0112 - val_loss: 0.0091\n",
      "Epoch 9/21\n",
      "1563/1563 [==============================] - 1s 507us/step - loss: 0.0089 - val_loss: 0.0075\n",
      "Epoch 10/21\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0075 - val_loss: 0.0064\n",
      "Epoch 11/21\n",
      "1563/1563 [==============================] - 1s 500us/step - loss: 0.0067 - val_loss: 0.0057\n",
      "Epoch 12/21\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0061 - val_loss: 0.0053\n",
      "Epoch 13/21\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 14/21\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0053 - val_loss: 0.0047\n",
      "Epoch 15/21\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0051 - val_loss: 0.0046\n",
      "Epoch 16/21\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0049 - val_loss: 0.0044\n",
      "Epoch 17/21\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 18/21\n",
      "1563/1563 [==============================] - 1s 509us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 19/21\n",
      "1563/1563 [==============================] - 1s 511us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 20/21\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0043 - val_loss: 0.0038\n",
      "Epoch 21/21\n",
      "1563/1563 [==============================] - 1s 514us/step - loss: 0.0042 - val_loss: 0.0037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32b1abe20>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epochs, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.897756</td>\n",
       "      <td>0.870543</td>\n",
       "      <td>0.156242</td>\n",
       "      <td>0.011116</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004013</td>\n",
       "      <td>-0.003887</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.004065</td>\n",
       "      <td>-0.004182</td>\n",
       "      <td>-0.004026</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>47.712921</td>\n",
       "      <td>0.397587</td>\n",
       "      <td>0.144377</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.920400</td>\n",
       "      <td>-0.372206</td>\n",
       "      <td>-0.663099</td>\n",
       "      <td>-0.555351</td>\n",
       "      <td>-0.749872</td>\n",
       "      <td>-0.652185</td>\n",
       "      <td>0.184047</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42.110752</td>\n",
       "      <td>1.256021</td>\n",
       "      <td>0.151572</td>\n",
       "      <td>0.010048</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.006722</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>-0.007479</td>\n",
       "      <td>-0.006512</td>\n",
       "      <td>-0.006029</td>\n",
       "      <td>-0.006538</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>135.101206</td>\n",
       "      <td>0.380900</td>\n",
       "      <td>0.378550</td>\n",
       "      <td>0.007319</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-1.221039</td>\n",
       "      <td>-2.241018</td>\n",
       "      <td>-2.258673</td>\n",
       "      <td>-1.308517</td>\n",
       "      <td>-1.526313</td>\n",
       "      <td>-1.711112</td>\n",
       "      <td>0.451005</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.599411</td>\n",
       "      <td>0.461664</td>\n",
       "      <td>0.228074</td>\n",
       "      <td>0.008765</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.551183</td>\n",
       "      <td>-0.671337</td>\n",
       "      <td>-0.275650</td>\n",
       "      <td>-0.343786</td>\n",
       "      <td>-0.761923</td>\n",
       "      <td>-0.520776</td>\n",
       "      <td>0.186095</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49.627714</td>\n",
       "      <td>0.804522</td>\n",
       "      <td>0.150416</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.011203</td>\n",
       "      <td>-0.010994</td>\n",
       "      <td>-0.012591</td>\n",
       "      <td>-0.012352</td>\n",
       "      <td>-0.013580</td>\n",
       "      <td>-0.012144</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>49.005421</td>\n",
       "      <td>0.492690</td>\n",
       "      <td>0.149297</td>\n",
       "      <td>0.007677</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.624877</td>\n",
       "      <td>-0.266007</td>\n",
       "      <td>-0.635726</td>\n",
       "      <td>-0.305873</td>\n",
       "      <td>-1.075089</td>\n",
       "      <td>-0.581515</td>\n",
       "      <td>0.291190</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>43.598086</td>\n",
       "      <td>0.795214</td>\n",
       "      <td>0.150025</td>\n",
       "      <td>0.006603</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.002285</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>-0.002348</td>\n",
       "      <td>-0.002100</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>43.600693</td>\n",
       "      <td>0.447184</td>\n",
       "      <td>0.152037</td>\n",
       "      <td>0.005516</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.041846</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>-0.034058</td>\n",
       "      <td>-0.031372</td>\n",
       "      <td>-0.037725</td>\n",
       "      <td>0.008277</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>50.836121</td>\n",
       "      <td>0.821611</td>\n",
       "      <td>0.152331</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>-0.004070</td>\n",
       "      <td>-0.003794</td>\n",
       "      <td>-0.004121</td>\n",
       "      <td>-0.004033</td>\n",
       "      <td>0.000127</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      48.897756      0.870543         0.156242        0.011116   \n",
       "1      47.712921      0.397587         0.144377        0.003170   \n",
       "2      42.110752      1.256021         0.151572        0.010048   \n",
       "3     135.101206      0.380900         0.378550        0.007319   \n",
       "4      76.599411      0.461664         0.228074        0.008765   \n",
       "5      49.627714      0.804522         0.150416        0.005282   \n",
       "6      49.005421      0.492690         0.149297        0.007677   \n",
       "7      43.598086      0.795214         0.150025        0.006603   \n",
       "8      43.600693      0.447184         0.152037        0.005516   \n",
       "9      50.836121      0.821611         0.152331        0.004190   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004013   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.920400   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.006722   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -1.221039   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.551183   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.011203   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.624877   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.002285   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.041846   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.004155   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.003887          -0.003985          -0.004065          -0.004182   \n",
       "1          -0.372206          -0.663099          -0.555351          -0.749872   \n",
       "2          -0.005950          -0.007479          -0.006512          -0.006029   \n",
       "3          -2.241018          -2.258673          -1.308517          -1.526313   \n",
       "4          -0.671337          -0.275650          -0.343786          -0.761923   \n",
       "5          -0.010994          -0.012591          -0.012352          -0.013580   \n",
       "6          -0.266007          -0.635726          -0.305873          -1.075089   \n",
       "7          -0.002225          -0.002348          -0.002100          -0.002193   \n",
       "8          -0.029398          -0.051953          -0.034058          -0.031372   \n",
       "9          -0.004024          -0.004070          -0.003794          -0.004121   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.004026        0.000097                2  \n",
       "1        -0.652185        0.184047                9  \n",
       "2        -0.006538        0.000552                4  \n",
       "3        -1.711112        0.451005               10  \n",
       "4        -0.520776        0.186095                7  \n",
       "5        -0.012144        0.000950                5  \n",
       "6        -0.581515        0.291190                8  \n",
       "7        -0.002230        0.000084                1  \n",
       "8        -0.037725        0.008277                6  \n",
       "9        -0.004033        0.000127                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/3143193033.py:24: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 3\n",
      "n_required_iterations: 3\n",
      "n_possible_iterations: 3\n",
      "min_resources_: 11111\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 12\n",
      "n_resources: 11111\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 1ms/step - loss: 1.3465 - val_loss: 0.4055\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.1402 - val_loss: 0.0562\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0448 - val_loss: 0.0366\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0348 - val_loss: 0.0322\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0338 - val_loss: 0.0500\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0288 - val_loss: 0.0334\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 494us/step - loss: 0.0286 - val_loss: 0.0230\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 466us/step - loss: 0.0245 - val_loss: 0.0188\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0235 - val_loss: 0.0199\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.0237 - val_loss: 0.0168\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0213 - val_loss: 0.0153\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0208 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0396 - 90ms/epoch - 324us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0186 - 90ms/epoch - 323us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0177 - 86ms/epoch - 311us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0195 - 93ms/epoch - 333us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0194 - 89ms/epoch - 319us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0162 - 86ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0179 - 85ms/epoch - 307us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0256 - 86ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 313us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0187 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0142 - 65ms/epoch - 929us/step\n",
      "278/278 - 0s - loss: 0.0142 - 82ms/epoch - 296us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3651 - val_loss: 0.4194\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 441us/step - loss: 0.1461 - val_loss: 0.0542\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 440us/step - loss: 0.0406 - val_loss: 0.0334\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0324 - val_loss: 0.0320\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0252 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0239 - val_loss: 0.0236\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0190 - val_loss: 0.0152\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 435us/step - loss: 0.0163 - val_loss: 0.0162\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0156 - val_loss: 0.0131\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0163 - val_loss: 0.0134\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0142 - val_loss: 0.0132\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0174 - 88ms/epoch - 315us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0170 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 85ms/epoch - 307us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0137 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0216 - 85ms/epoch - 304us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 86ms/epoch - 308us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0148 - 85ms/epoch - 307us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0125 - 85ms/epoch - 306us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0124 - 85ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0145 - 85ms/epoch - 307us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0172 - 85ms/epoch - 305us/step\n",
      "70/70 - 0s - loss: 0.0136 - 66ms/epoch - 945us/step\n",
      "278/278 - 0s - loss: 0.0136 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3610 - val_loss: 0.3997\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 445us/step - loss: 0.1283 - val_loss: 0.0460\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 436us/step - loss: 0.0364 - val_loss: 0.0298\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0257 - val_loss: 0.0228\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0211 - val_loss: 0.0193\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 439us/step - loss: 0.0203 - val_loss: 0.0165\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0180 - val_loss: 0.0154\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 433us/step - loss: 0.0177 - val_loss: 0.0181\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0182 - val_loss: 0.0136\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0165 - val_loss: 0.0124\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0143 - val_loss: 0.0117\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0160 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0149 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0193 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0144 - 86ms/epoch - 310us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0157 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0123 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 88ms/epoch - 317us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0132 - 91ms/epoch - 326us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0170 - 93ms/epoch - 333us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0137 - 93ms/epoch - 334us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 93ms/epoch - 334us/step\n",
      "70/70 - 0s - loss: 0.0105 - 68ms/epoch - 965us/step\n",
      "278/278 - 0s - loss: 0.0105 - 93ms/epoch - 336us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3553 - val_loss: 0.4157\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.1376 - val_loss: 0.0526\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0418 - val_loss: 0.0327\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 427us/step - loss: 0.0318 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0272 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 431us/step - loss: 0.0272 - val_loss: 0.3417\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0219 - val_loss: 0.0328\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0180 - val_loss: 0.0157\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 421us/step - loss: 0.0159 - val_loss: 0.0150\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 438us/step - loss: 0.0159 - val_loss: 0.0133\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 422us/step - loss: 0.0133 - val_loss: 0.0118\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0124 - 88ms/epoch - 317us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0117 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0117 - 88ms/epoch - 315us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0115 - 87ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0119 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 311us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0121 - 87ms/epoch - 313us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0114 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0141 - 87ms/epoch - 315us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0111 - 88ms/epoch - 316us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0110 - 87ms/epoch - 312us/step\n",
      "70/70 - 0s - loss: 0.0104 - 65ms/epoch - 923us/step\n",
      "278/278 - 0s - loss: 0.0104 - 81ms/epoch - 292us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  17.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.3507 - val_loss: 0.4093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.1345 - val_loss: 0.0495\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 426us/step - loss: 0.0388 - val_loss: 0.0318\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 423us/step - loss: 0.0311 - val_loss: 0.0300\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 437us/step - loss: 0.0287 - val_loss: 0.0213\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 424us/step - loss: 0.0239 - val_loss: 0.0198\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0188 - val_loss: 0.0164\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 430us/step - loss: 0.0170 - val_loss: 0.0150\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 428us/step - loss: 0.0149 - val_loss: 0.0141\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 425us/step - loss: 0.0166 - val_loss: 0.0128\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 429us/step - loss: 0.0133 - val_loss: 0.0121\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0234 - 100ms/epoch - 358us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0244 - 87ms/epoch - 313us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0153 - 86ms/epoch - 310us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0148 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0207 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0186 - 87ms/epoch - 312us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0124 - 99ms/epoch - 357us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0194 - 86ms/epoch - 311us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0161 - 86ms/epoch - 311us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0146 - 86ms/epoch - 311us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0146 - 87ms/epoch - 315us/step\n",
      "70/70 - 0s - loss: 0.0113 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0113 - 83ms/epoch - 297us/step\n",
      "[CV] END ...............kernel_regularizer=l1, optimizer=sgd; total time=  16.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3960 - val_loss: 0.0494\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0342 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0205 - val_loss: 0.0212\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0150 - val_loss: 0.0130\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0125 - val_loss: 0.0107\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0090 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0080 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0070 - val_loss: 0.0064\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0063 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0063 - 125ms/epoch - 448us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0059 - 122ms/epoch - 440us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0063 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0058 - 123ms/epoch - 442us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 119ms/epoch - 428us/step\n",
      "70/70 - 0s - loss: 0.0057 - 68ms/epoch - 976us/step\n",
      "278/278 - 0s - loss: 0.0057 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3854 - val_loss: 0.0480\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0310 - val_loss: 0.0213\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0177 - val_loss: 0.0149\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0147 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0113 - val_loss: 0.0112\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0100 - val_loss: 0.0089\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0066 - val_loss: 0.0062\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0060 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0090 - 115ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0058 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0057 - 113ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0062 - 114ms/epoch - 409us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0060 - 113ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0062 - 113ms/epoch - 406us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0056 - 113ms/epoch - 406us/step\n",
      "70/70 - 0s - loss: 0.0055 - 67ms/epoch - 961us/step\n",
      "278/278 - 0s - loss: 0.0055 - 81ms/epoch - 291us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  18.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3846 - val_loss: 0.0478\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0327 - val_loss: 0.0284\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0189 - val_loss: 0.0150\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0139 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0113 - val_loss: 0.0101\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0096 - val_loss: 0.0086\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0065 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0060 - val_loss: 0.0056\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0077 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0056 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0058 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0056 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0055 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0055 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0056 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0059 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.0054 - 72ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3621 - val_loss: 0.0486\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0342 - val_loss: 0.0245\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0209 - val_loss: 0.0177\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0156 - val_loss: 0.0132\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0126 - val_loss: 0.0114\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.0107 - val_loss: 0.0096\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0091 - val_loss: 0.0083\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0084 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 535us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0071 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0096 - 114ms/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0063 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0088 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0060 - 116ms/epoch - 416us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 924us/step\n",
      "278/278 - 0s - loss: 0.0060 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3908 - val_loss: 0.0493\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 540us/step - loss: 0.0328 - val_loss: 0.0224\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0191 - val_loss: 0.0156\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0140 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0115 - val_loss: 0.0100\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0096 - val_loss: 0.0090\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.0087 - val_loss: 0.0078\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0070 - val_loss: 0.0065\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0059 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0078 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0056 - 123ms/epoch - 443us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0060 - 122ms/epoch - 437us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0056 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0055 - 120ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0055 - 126ms/epoch - 452us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0056 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0075 - 128ms/epoch - 459us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0054 - 122ms/epoch - 437us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0054 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0054 - 65ms/epoch - 927us/step\n",
      "278/278 - 0s - loss: 0.0054 - 83ms/epoch - 299us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1026 - val_loss: 0.0409\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0401 - val_loss: 0.0304\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0293 - val_loss: 0.0273\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0295 - val_loss: 0.0323\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0276 - val_loss: 0.0268\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0277 - val_loss: 0.0266\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0286 - val_loss: 0.0279\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0282 - val_loss: 0.0261\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0277 - val_loss: 0.0262\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0275 - val_loss: 0.0254\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0287 - val_loss: 0.0260\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0483 - 117ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0281 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0317 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0271 - 113ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0266 - 127ms/epoch - 458us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0262 - 125ms/epoch - 449us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0324 - 114ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0299 - 120ms/epoch - 430us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0264 - 115ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0402 - 115ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0270 - 114ms/epoch - 409us/step\n",
      "70/70 - 0s - loss: 0.0267 - 66ms/epoch - 938us/step\n",
      "278/278 - 0s - loss: 0.0267 - 83ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0995 - val_loss: 0.0342\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0355 - val_loss: 0.0484\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0305 - val_loss: 0.0325\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0276 - val_loss: 0.0263\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0306 - val_loss: 0.0280\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0307 - val_loss: 0.0276\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0269 - val_loss: 0.0261\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0282 - val_loss: 0.0266\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0266 - val_loss: 0.0258\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0295 - val_loss: 0.0274\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 651us/step - loss: 0.0265 - val_loss: 0.0285\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0566 - 117ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0273 - 113ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0274 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0267 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0266 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0429 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0275 - 122ms/epoch - 439us/step\n",
      "70/70 - 0s - loss: 0.0270 - 70ms/epoch - 996us/step\n",
      "278/278 - 0s - loss: 0.0270 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0978 - val_loss: 0.0353\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0337 - val_loss: 0.0345\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0290 - val_loss: 0.0268\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0281 - val_loss: 0.0262\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0280 - val_loss: 0.0265\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0286 - val_loss: 0.0303\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0277 - val_loss: 0.0271\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0282 - val_loss: 0.0262\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0278 - val_loss: 0.0259\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0275 - val_loss: 0.0279\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0287 - val_loss: 0.0268\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0333 - 123ms/epoch - 444us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0267 - 121ms/epoch - 436us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0324 - 125ms/epoch - 449us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0307 - 122ms/epoch - 438us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0265 - 119ms/epoch - 430us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0352 - 119ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0266 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0293 - 118ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0277 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0430 - 117ms/epoch - 422us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0274 - 110ms/epoch - 395us/step\n",
      "70/70 - 0s - loss: 0.0264 - 68ms/epoch - 977us/step\n",
      "278/278 - 0s - loss: 0.0264 - 88ms/epoch - 316us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1056 - val_loss: 0.0613\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0358 - val_loss: 0.0303\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0272\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0270\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0280 - val_loss: 0.0276\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0284 - val_loss: 0.0263\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0269 - val_loss: 0.0280\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0294 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0288 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 121ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0275 - 125ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0279 - 117ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0290 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0275 - 131ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0301 - 127ms/epoch - 456us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0273 - 116ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0352 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0337 - 142ms/epoch - 512us/step\n",
      "70/70 - 0s - loss: 0.0287 - 66ms/epoch - 936us/step\n",
      "278/278 - 0s - loss: 0.0287 - 85ms/epoch - 305us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1011 - val_loss: 0.0319\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0308 - val_loss: 0.0288\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0324 - val_loss: 0.0287\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0279 - val_loss: 0.0273\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0291 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0289 - val_loss: 0.0281\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0271 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0300 - val_loss: 0.0275\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0291 - val_loss: 0.0263\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0285 - val_loss: 0.0322\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0277 - val_loss: 0.0263\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0323 - 124ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0305 - 115ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0283 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0423 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0309 - 117ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0267 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0272 - 109ms/epoch - 393us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0274 - 128ms/epoch - 460us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0273 - 104ms/epoch - 374us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0276 - 103ms/epoch - 370us/step\n",
      "70/70 - 0s - loss: 0.0269 - 69ms/epoch - 979us/step\n",
      "278/278 - 0s - loss: 0.0269 - 84ms/epoch - 303us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3300 - val_loss: 0.2675\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 531us/step - loss: 0.2860 - val_loss: 0.2474\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.2707 - val_loss: 0.3840\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.2758 - val_loss: 0.2520\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2681 - val_loss: 0.2504\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.3030 - val_loss: 0.2759\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.2590 - val_loss: 0.2493\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2685 - val_loss: 0.4090\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2641 - val_loss: 0.2842\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2743 - val_loss: 0.2543\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 538us/step - loss: 0.2898 - val_loss: 0.2599\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3654 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2581 - 115ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3261 - 112ms/epoch - 404us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2542 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2930 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2542 - 115ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3043 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2996 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3365 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2554 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3258 - 113ms/epoch - 407us/step\n",
      "70/70 - 0s - loss: 0.2511 - 70ms/epoch - 994us/step\n",
      "278/278 - 0s - loss: 0.2510 - 86ms/epoch - 311us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3370 - val_loss: 0.2700\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.2954 - val_loss: 0.2630\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2865 - val_loss: 0.2740\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.2765 - val_loss: 0.2524\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2759 - val_loss: 0.2738\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.2765 - val_loss: 0.2599\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.2716 - val_loss: 0.2615\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.2705 - val_loss: 0.2531\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.2752 - val_loss: 0.2520\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.2780 - val_loss: 0.2560\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2632 - val_loss: 0.2596\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4385 - 119ms/epoch - 429us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4000 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2614 - 118ms/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2551 - 117ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3828 - 133ms/epoch - 478us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2856 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2560 - 115ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3851 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2599 - 117ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.4005 - 119ms/epoch - 429us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2574 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.2491 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2490 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3473 - val_loss: 0.2886\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2792 - val_loss: 0.2545\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2848 - val_loss: 0.2629\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.2880 - val_loss: 0.3089\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.2769 - val_loss: 0.2625\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2739 - val_loss: 0.2643\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.2778 - val_loss: 0.2504\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.2621 - val_loss: 0.2485\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.2746 - val_loss: 0.2556\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2697 - val_loss: 0.2559\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.2861 - val_loss: 0.2588\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4729 - 115ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2995 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3738 - 113ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2598 - 114ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2608 - 114ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2660 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2864 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.4161 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2603 - 113ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3387 - 115ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2565 - 126ms/epoch - 452us/step\n",
      "70/70 - 0s - loss: 0.2542 - 64ms/epoch - 914us/step\n",
      "278/278 - 0s - loss: 0.2541 - 85ms/epoch - 306us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3284 - val_loss: 0.2611\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2845 - val_loss: 0.2483\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.2944 - val_loss: 0.2620\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.2854 - val_loss: 0.2783\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.2849 - val_loss: 0.2550\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.2712 - val_loss: 0.2555\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.2731 - val_loss: 0.2508\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.2727 - val_loss: 0.2477\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.2684 - val_loss: 0.2540\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.2748 - val_loss: 0.7859\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.2805 - val_loss: 0.2492\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4792 - 114ms/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3824 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2546 - 114ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2534 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2529 - 126ms/epoch - 453us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3175 - 114ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4005 - 113ms/epoch - 405us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2540 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2625 - 114ms/epoch - 409us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2538 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.2455 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.2456 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3266 - val_loss: 0.2568\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.2785 - val_loss: 0.2578\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.2913 - val_loss: 0.2531\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.2769 - val_loss: 0.2475\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.2669 - val_loss: 0.2601\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.2981 - val_loss: 0.2510\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.2715 - val_loss: 0.2700\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.2651 - val_loss: 0.2603\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.2743 - val_loss: 0.2537\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.2679 - val_loss: 0.6151\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.2706 - val_loss: 0.2791\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3185 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3209 - 114ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3510 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.2949 - 131ms/epoch - 471us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2534 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2539 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3196 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3097 - 114ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.4370 - 114ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2578 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2545 - 117ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.2485 - 65ms/epoch - 930us/step\n",
      "278/278 - 0s - loss: 0.2486 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3513 - val_loss: 0.2317\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 450us/step - loss: 0.1737 - val_loss: 0.1265\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 534us/step - loss: 0.0955 - val_loss: 0.0702\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0534 - val_loss: 0.0398\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0307 - val_loss: 0.0233\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0184 - val_loss: 0.0143\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 458us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 90ms/epoch - 325us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 323us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 321us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 316us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 315us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 320us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 317us/step\n",
      "70/70 - 0s - loss: 0.0027 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3425 - val_loss: 0.2290\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.1716 - val_loss: 0.1249\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0941 - val_loss: 0.0691\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0526 - val_loss: 0.0390\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0301 - val_loss: 0.0228\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0179 - val_loss: 0.0140\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 442us/step - loss: 0.0112 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 446us/step - loss: 0.0075 - val_loss: 0.0063\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0054 - val_loss: 0.0047\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 444us/step - loss: 0.0042 - val_loss: 0.0038\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 454us/step - loss: 0.0035 - val_loss: 0.0032\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0032 - 89ms/epoch - 318us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0030 - 88ms/epoch - 316us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 312us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 88ms/epoch - 316us/step\n",
      "70/70 - 0s - loss: 0.0025 - 67ms/epoch - 953us/step\n",
      "278/278 - 0s - loss: 0.0025 - 85ms/epoch - 307us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 0.3490 - val_loss: 0.2337\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.1752 - val_loss: 0.1276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0962 - val_loss: 0.0706\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0538 - val_loss: 0.0400\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 498us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 495us/step - loss: 0.0078 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 451us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 447us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 104ms/epoch - 375us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 94ms/epoch - 337us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 93ms/epoch - 336us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 317us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 89ms/epoch - 318us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 90ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 314us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 321us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 326us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 90ms/epoch - 325us/step\n",
      "70/70 - 0s - loss: 0.0026 - 66ms/epoch - 947us/step\n",
      "278/278 - 0s - loss: 0.0026 - 84ms/epoch - 301us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3657 - val_loss: 0.2352\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.1762 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0966 - val_loss: 0.0709\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0309 - val_loss: 0.0234\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 481us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0056 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0034 - 93ms/epoch - 336us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0032 - 91ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 102ms/epoch - 365us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 88ms/epoch - 318us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 92ms/epoch - 332us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 94ms/epoch - 339us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 94ms/epoch - 338us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 95ms/epoch - 341us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 89ms/epoch - 319us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 101ms/epoch - 364us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 941us/step\n",
      "278/278 - 0s - loss: 0.0027 - 86ms/epoch - 311us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  18.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 0.3586 - val_loss: 0.2408\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.1804 - val_loss: 0.1313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 467us/step - loss: 0.0990 - val_loss: 0.0727\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 493us/step - loss: 0.0553 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 449us/step - loss: 0.0317 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0189 - val_loss: 0.0148\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0080 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 460us/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 462us/step - loss: 0.0045 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 452us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0033 - 90ms/epoch - 324us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0033 - 88ms/epoch - 318us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0032 - 87ms/epoch - 313us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0031 - 87ms/epoch - 314us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0030 - 87ms/epoch - 313us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0030 - 90ms/epoch - 324us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0029 - 89ms/epoch - 321us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0029 - 87ms/epoch - 313us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0028 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0028 - 90ms/epoch - 323us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0027 - 87ms/epoch - 314us/step\n",
      "70/70 - 0s - loss: 0.0027 - 66ms/epoch - 940us/step\n",
      "278/278 - 0s - loss: 0.0027 - 85ms/epoch - 304us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  17.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1058 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0054 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0038 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0026 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0017 - val_loss: 0.0058\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0030 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 9.5735e-04 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0070 - 114ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0012 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0010 - 113ms/epoch - 405us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0012 - 116ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0012 - 64ms/epoch - 910us/step\n",
      "278/278 - 0s - loss: 0.0012 - 82ms/epoch - 294us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1016 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0060 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0029 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0017 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0084 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0015 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0014 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0052 - 115ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0019 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0021 - 121ms/epoch - 436us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0057 - 112ms/epoch - 404us/step\n",
      "70/70 - 0s - loss: 0.0023 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0022 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0125\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0065 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0046 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0032 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0026 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0028 - val_loss: 0.0057\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0022 - val_loss: 0.0237\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0021 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0076 - 118ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0012 - 114ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0021 - 113ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0042 - 133ms/epoch - 477us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0011 - 116ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0048 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0045 - 114ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0015 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0014 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0014 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0827 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0050 - val_loss: 0.0030\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0040 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0025 - val_loss: 0.0031\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0019 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0013 - val_loss: 9.8234e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0032 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0085 - 114ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0014 - 113ms/epoch - 406us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0020 - 113ms/epoch - 408us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0011 - 112ms/epoch - 404us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0011 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0022 - 113ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0045 - 113ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0064 - 114ms/epoch - 408us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0013 - 114ms/epoch - 409us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0011 - 113ms/epoch - 408us/step\n",
      "70/70 - 0s - loss: 0.0011 - 67ms/epoch - 960us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 293us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1294 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0064 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0040 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0039 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0028 - val_loss: 0.0022\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0028 - val_loss: 0.0020\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0102 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0026 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0013 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0040 - 122ms/epoch - 440us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0057 - 115ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0023 - 114ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0012 - 117ms/epoch - 422us/step\n",
      "70/70 - 0s - loss: 0.0011 - 65ms/epoch - 932us/step\n",
      "278/278 - 0s - loss: 0.0011 - 82ms/epoch - 296us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0322 - val_loss: 0.0051\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0106 - val_loss: 0.0178\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0089 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0065 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0037 - val_loss: 0.0060\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0066 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0027 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0037 - val_loss: 0.0466\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0599 - 125ms/epoch - 449us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0036 - 114ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0026 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0025 - 115ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0021 - 114ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0038 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0040 - 116ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0020 - 120ms/epoch - 431us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0015 - 129ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0038 - 117ms/epoch - 420us/step\n",
      "70/70 - 0s - loss: 0.0013 - 65ms/epoch - 926us/step\n",
      "278/278 - 0s - loss: 0.0013 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0084\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0095 - val_loss: 0.0098\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0069 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0064 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0043 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0174\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0048 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0038 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0052 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0033 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0028 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0060 - 115ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0016 - 114ms/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0053 - 117ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0014 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0018 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0017 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0080 - 135ms/epoch - 485us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0033 - 123ms/epoch - 443us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0087 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0034 - 64ms/epoch - 921us/step\n",
      "278/278 - 0s - loss: 0.0034 - 94ms/epoch - 337us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0338 - val_loss: 0.0053\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0155 - val_loss: 0.0129\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0087 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0062 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0057 - val_loss: 0.0067\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0048 - val_loss: 0.0041\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0033 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0023 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 127ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0018 - 124ms/epoch - 445us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0017 - 125ms/epoch - 451us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 120ms/epoch - 432us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0084 - 119ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0018 - 120ms/epoch - 430us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0030 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0099 - 120ms/epoch - 432us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0017 - 121ms/epoch - 434us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0082 - 120ms/epoch - 430us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0022 - 122ms/epoch - 438us/step\n",
      "70/70 - 0s - loss: 0.0016 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0015 - 90ms/epoch - 322us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0385 - val_loss: 0.0204\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0093 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0064 - val_loss: 0.0029\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0068 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0045 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0041 - val_loss: 0.0067\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0069 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0022 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0051 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0194 - 118ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0019 - 125ms/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0018 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0014 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0102 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0043 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0014 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0016 - 116ms/epoch - 419us/step\n",
      "70/70 - 0s - loss: 0.0071 - 65ms/epoch - 931us/step\n",
      "278/278 - 0s - loss: 0.0073 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0353 - val_loss: 0.1669\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0122 - val_loss: 0.0094\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0068 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0051 - val_loss: 0.0022\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0034 - val_loss: 0.0021\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0368\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0024 - val_loss: 0.0036\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0030 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0337 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0086 - 138ms/epoch - 496us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0023 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0018 - 116ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0016 - 115ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0015 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0015 - 121ms/epoch - 435us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0036 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0013 - 117ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0019 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0086 - 115ms/epoch - 412us/step\n",
      "70/70 - 0s - loss: 0.0019 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0019 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  19.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0817 - val_loss: 0.0719\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0353 - val_loss: 0.0108\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0420 - val_loss: 0.0113\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0245 - val_loss: 0.0061\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0225 - val_loss: 0.0195\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0251 - val_loss: 0.0983\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0231 - val_loss: 0.0048\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0328 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0175 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0196 - val_loss: 0.0082\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0482 - val_loss: 0.0049\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0521 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0598 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0818 - 115ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0135 - 113ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.2159 - 113ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0829 - 113ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0225 - 115ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0093 - 114ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0182 - 115ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2546 - 115ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0247 - 116ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 64ms/epoch - 912us/step\n",
      "278/278 - 0s - loss: 0.0053 - 84ms/epoch - 302us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0878 - val_loss: 0.0192\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0237 - val_loss: 0.0060\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0388 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0459 - val_loss: 0.0518\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0228 - val_loss: 0.0159\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0335 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0216 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0268 - val_loss: 0.1032\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0221 - val_loss: 0.0091\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0214 - val_loss: 0.0045\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0220 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3516 - 114ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0303 - 113ms/epoch - 407us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0235 - 113ms/epoch - 407us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0064 - 113ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0081 - 113ms/epoch - 408us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.1599 - 113ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0151 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0095 - 113ms/epoch - 407us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0903 - 112ms/epoch - 403us/step\n",
      "70/70 - 0s - loss: 0.0060 - 65ms/epoch - 925us/step\n",
      "278/278 - 0s - loss: 0.0059 - 83ms/epoch - 300us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  19.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0877 - val_loss: 0.0093\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0334 - val_loss: 0.0140\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0528 - val_loss: 0.0086\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0287 - val_loss: 0.0060\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0349 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0271 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0251 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0172 - val_loss: 0.0033\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0357 - val_loss: 0.0203\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0447 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0229 - 125ms/epoch - 451us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0526 - 115ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0649 - 114ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0931 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0098 - 114ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0061 - 120ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0743 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0225 - 119ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.1106 - 114ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.1751 - 117ms/epoch - 419us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0106 - 115ms/epoch - 415us/step\n",
      "70/70 - 0s - loss: 0.0053 - 65ms/epoch - 933us/step\n",
      "278/278 - 0s - loss: 0.0052 - 82ms/epoch - 295us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0882 - val_loss: 0.0172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0399 - val_loss: 0.0151\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 635us/step - loss: 0.0260 - val_loss: 0.0048\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0364 - val_loss: 0.0051\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0262 - val_loss: 0.3343\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0240 - val_loss: 0.0113\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0266 - val_loss: 0.0071\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0250 - val_loss: 0.0041\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0323 - val_loss: 0.0075\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0234 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0325 - val_loss: 0.0572\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0198 - 123ms/epoch - 443us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.2494 - 115ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0991 - 115ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0154 - 115ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0154 - 116ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1370 - 115ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0135 - 114ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0518 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0127 - 126ms/epoch - 452us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0781 - 119ms/epoch - 427us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0349 - 137ms/epoch - 492us/step\n",
      "70/70 - 0s - loss: 0.0134 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0127 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0691 - val_loss: 0.0070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0339 - val_loss: 0.0592\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0340 - val_loss: 0.1339\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0350 - val_loss: 0.0085\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0499 - val_loss: 0.0041\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0263 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 718us/step - loss: 0.0294 - val_loss: 0.0107\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0311 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0200 - val_loss: 0.0118\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0191 - val_loss: 0.0047\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0217 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.1301 - 118ms/epoch - 426us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0081 - 133ms/epoch - 479us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0370 - 119ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.1167 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0240 - 114ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.1504 - 114ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0156 - 111ms/epoch - 398us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0087 - 117ms/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0406 - 116ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0828 - 117ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0310 - 121ms/epoch - 436us/step\n",
      "70/70 - 0s - loss: 0.0158 - 64ms/epoch - 918us/step\n",
      "278/278 - 0s - loss: 0.0153 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 1ms/step - loss: 1.2734 - val_loss: 0.1981\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0732 - val_loss: 0.0428\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 455us/step - loss: 0.0380 - val_loss: 0.0382\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 464us/step - loss: 0.0342 - val_loss: 0.0256\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0299 - val_loss: 0.0222\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0250 - val_loss: 0.0268\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 443us/step - loss: 0.0225 - val_loss: 0.0200\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 457us/step - loss: 0.0195 - val_loss: 0.0161\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0166 - val_loss: 0.0148\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0153 - val_loss: 0.0137\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 459us/step - loss: 0.0143 - val_loss: 0.0136\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0237 - 90ms/epoch - 322us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0154 - 84ms/epoch - 304us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0135 - 85ms/epoch - 305us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0197 - 84ms/epoch - 302us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0129 - 83ms/epoch - 299us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0122 - 83ms/epoch - 300us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0130 - 82ms/epoch - 294us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0124 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0119 - 84ms/epoch - 301us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0124 - 92ms/epoch - 331us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0118 - 98ms/epoch - 352us/step\n",
      "70/70 - 0s - loss: 0.0114 - 69ms/epoch - 984us/step\n",
      "278/278 - 0s - loss: 0.0114 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.3252 - val_loss: 0.2172\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 483us/step - loss: 0.0797 - val_loss: 0.0475\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0443 - val_loss: 0.0407\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0380 - val_loss: 0.0327\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0325 - val_loss: 0.0257\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0257 - val_loss: 0.0217\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 461us/step - loss: 0.0220 - val_loss: 0.0191\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0203 - val_loss: 0.0173\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 484us/step - loss: 0.0226 - val_loss: 0.0162\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 463us/step - loss: 0.0184 - val_loss: 0.0160\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 473us/step - loss: 0.0170 - val_loss: 0.0142\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0205 - 84ms/epoch - 303us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0188 - 83ms/epoch - 299us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0218 - 83ms/epoch - 298us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0147 - 83ms/epoch - 298us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0172 - 83ms/epoch - 300us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0223 - 96ms/epoch - 346us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0152 - 82ms/epoch - 296us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0152 - 84ms/epoch - 302us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0146 - 84ms/epoch - 302us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0255 - 83ms/epoch - 299us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 84ms/epoch - 304us/step\n",
      "70/70 - 0s - loss: 0.0126 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0126 - 86ms/epoch - 310us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 2s 2ms/step - loss: 1.2958 - val_loss: 0.2223\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 472us/step - loss: 0.0857 - val_loss: 0.0480\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0444 - val_loss: 0.0355\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 489us/step - loss: 0.0390 - val_loss: 0.0581\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 468us/step - loss: 0.0325 - val_loss: 0.0306\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 487us/step - loss: 0.0248 - val_loss: 0.0212\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 471us/step - loss: 0.0224 - val_loss: 0.0177\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 456us/step - loss: 0.0203 - val_loss: 0.0159\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 475us/step - loss: 0.0180 - val_loss: 0.0146\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 476us/step - loss: 0.0161 - val_loss: 0.0155\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 474us/step - loss: 0.0143 - val_loss: 0.0123\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0143 - 109ms/epoch - 392us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0156 - 95ms/epoch - 342us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0171 - 84ms/epoch - 303us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0132 - 82ms/epoch - 295us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0137 - 84ms/epoch - 301us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0145 - 89ms/epoch - 321us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0128 - 88ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0139 - 86ms/epoch - 309us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0126 - 85ms/epoch - 304us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0129 - 82ms/epoch - 296us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0178 - 81ms/epoch - 293us/step\n",
      "70/70 - 0s - loss: 0.0111 - 67ms/epoch - 964us/step\n",
      "278/278 - 0s - loss: 0.0111 - 82ms/epoch - 296us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  17.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2390 - val_loss: 0.2058\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0847 - val_loss: 0.0552\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0486 - val_loss: 0.0368\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 486us/step - loss: 0.0365 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 482us/step - loss: 0.0295 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 488us/step - loss: 0.0265 - val_loss: 0.0210\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0207 - val_loss: 0.0187\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 649us/step - loss: 0.0192 - val_loss: 0.0156\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0174 - val_loss: 0.0149\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 496us/step - loss: 0.0166 - val_loss: 0.0135\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 477us/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0189 - 94ms/epoch - 338us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0324 - 92ms/epoch - 332us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0128 - 86ms/epoch - 309us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0205 - 86ms/epoch - 308us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0169 - 87ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0150 - 86ms/epoch - 310us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0260 - 86ms/epoch - 309us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0233 - 86ms/epoch - 308us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0175 - 87ms/epoch - 314us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0160 - 86ms/epoch - 309us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0138 - 86ms/epoch - 308us/step\n",
      "70/70 - 0s - loss: 0.0115 - 73ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0115 - 84ms/epoch - 301us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 1.2859 - val_loss: 0.2070\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 469us/step - loss: 0.0795 - val_loss: 0.0464\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0418 - val_loss: 0.2256\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 480us/step - loss: 0.0332 - val_loss: 0.0442\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0276 - val_loss: 0.0220\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 470us/step - loss: 0.0240 - val_loss: 0.0195\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 478us/step - loss: 0.0208 - val_loss: 0.0173\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0183 - val_loss: 0.0160\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 479us/step - loss: 0.0185 - val_loss: 0.0143\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 465us/step - loss: 0.0158 - val_loss: 0.0140\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 485us/step - loss: 0.0139 - val_loss: 0.0122\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0349 - 93ms/epoch - 333us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0385 - 85ms/epoch - 305us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 84ms/epoch - 301us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0230 - 84ms/epoch - 301us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0309 - 82ms/epoch - 293us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0208 - 91ms/epoch - 326us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0144 - 84ms/epoch - 302us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0137 - 85ms/epoch - 305us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0131 - 84ms/epoch - 300us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0285 - 81ms/epoch - 291us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0184 - 81ms/epoch - 290us/step\n",
      "70/70 - 0s - loss: 0.0118 - 69ms/epoch - 991us/step\n",
      "278/278 - 0s - loss: 0.0118 - 92ms/epoch - 330us/step\n",
      "[CV] END ............kernel_regularizer=l1_l2, optimizer=sgd; total time=  18.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3932 - val_loss: 0.0532\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 663us/step - loss: 0.0365 - val_loss: 0.0276\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0224 - val_loss: 0.0267\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0173 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0140 - val_loss: 0.0120\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0099 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0093 - val_loss: 0.0081\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0085 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0077 - val_loss: 0.0068\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0073 - 142ms/epoch - 509us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0073 - 116ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0109 - 119ms/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0076 - 120ms/epoch - 433us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 114ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0066 - 115ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 115ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0076 - 122ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0067 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0074 - 116ms/epoch - 418us/step\n",
      "70/70 - 0s - loss: 0.0070 - 67ms/epoch - 958us/step\n",
      "278/278 - 0s - loss: 0.0070 - 83ms/epoch - 298us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.4212 - val_loss: 0.0576\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0376 - val_loss: 0.0280\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0225 - val_loss: 0.0180\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0165 - val_loss: 0.0142\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0133 - val_loss: 0.0147\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0114 - val_loss: 0.0099\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0095 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0089 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0070 - val_loss: 0.0077\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0137 - 117ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0066 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0065 - 115ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0064 - 115ms/epoch - 415us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0082 - 122ms/epoch - 439us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0070 - 116ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0065 - 121ms/epoch - 436us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0066 - 123ms/epoch - 441us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0098 - 115ms/epoch - 414us/step\n",
      "70/70 - 0s - loss: 0.0062 - 67ms/epoch - 963us/step\n",
      "278/278 - 0s - loss: 0.0062 - 80ms/epoch - 287us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3702 - val_loss: 0.0449\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.0314 - val_loss: 0.0240\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0195 - val_loss: 0.0166\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0146 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0123 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0103 - val_loss: 0.0093\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0092 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0088 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0075 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0071 - val_loss: 0.0073\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0172 - 119ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0065 - 118ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0062 - 117ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0061 - 119ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0062 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0065 - 116ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0063 - 118ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0061 - 116ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0064 - 116ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0061 - 131ms/epoch - 470us/step\n",
      "70/70 - 0s - loss: 0.0059 - 71ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0059 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3909 - val_loss: 0.0514\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0353 - val_loss: 0.0250\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0216 - val_loss: 0.0170\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0167 - val_loss: 0.0136\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0134 - val_loss: 0.0116\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0115 - val_loss: 0.0102\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0100 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0083 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0114 - 121ms/epoch - 435us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0067 - 120ms/epoch - 432us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0067 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0068 - 119ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0079 - 134ms/epoch - 481us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0066 - 119ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0101 - 117ms/epoch - 422us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0065 - 117ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0066 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0069 - 119ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0064 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0064 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0064 - 84ms/epoch - 301us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3897 - val_loss: 0.0476\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0340 - val_loss: 0.0279\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0218 - val_loss: 0.0220\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0165 - val_loss: 0.0138\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0141 - val_loss: 0.0117\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0116 - val_loss: 0.0104\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0106 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0094 - val_loss: 0.0086\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0100 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0069 - 117ms/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0070 - 120ms/epoch - 433us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0069 - 116ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0071 - 131ms/epoch - 470us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0078 - 116ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0087 - 118ms/epoch - 425us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0068 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0072 - 125ms/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0073 - 117ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0075 - 118ms/epoch - 426us/step\n",
      "70/70 - 0s - loss: 0.0065 - 70ms/epoch - 999us/step\n",
      "278/278 - 0s - loss: 0.0065 - 85ms/epoch - 307us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1149 - val_loss: 0.0355\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0374 - val_loss: 0.0884\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0321 - val_loss: 0.0288\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0319 - val_loss: 0.0290\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0292 - val_loss: 0.0269\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0302 - val_loss: 0.0279\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0290 - val_loss: 0.0266\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0269\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0284 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0282 - val_loss: 0.0281\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0295 - val_loss: 0.0280\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0366 - 120ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0283 - 117ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0270 - 118ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0348 - 118ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0282 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0301 - 118ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0271 - 131ms/epoch - 471us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0449 - 118ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0288 - 116ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0315 - 115ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0287 - 117ms/epoch - 421us/step\n",
      "70/70 - 0s - loss: 0.0287 - 70ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0287 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1176 - val_loss: 0.0386\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0349 - val_loss: 0.0302\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0315 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0308 - val_loss: 0.0279\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0288 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0281 - val_loss: 0.0265\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0289 - val_loss: 0.0262\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0295 - val_loss: 0.0265\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0277 - val_loss: 0.0267\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0280 - val_loss: 0.0282\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0281 - val_loss: 0.0267\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0473 - 119ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0286 - 118ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0267 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 424us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0405 - 118ms/epoch - 425us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0270 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0270 - 116ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0289 - 118ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0307 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0851 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0284 - 90ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0284 - 86ms/epoch - 309us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0966 - val_loss: 0.0323\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0338 - val_loss: 0.0286\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0311 - val_loss: 0.0276\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0289 - val_loss: 0.0269\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0288 - val_loss: 0.0261\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0295 - val_loss: 0.0263\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0283 - val_loss: 0.0283\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0284 - val_loss: 0.0257\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0275 - val_loss: 0.0278\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0286 - val_loss: 0.0261\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0269 - val_loss: 0.0370\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0373 - 120ms/epoch - 433us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0298 - 119ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0272 - 117ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0593 - 116ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0278 - 118ms/epoch - 423us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0268 - 117ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0265 - 118ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0274 - 117ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0292 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0280 - 118ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0269 - 118ms/epoch - 424us/step\n",
      "70/70 - 0s - loss: 0.0279 - 69ms/epoch - 987us/step\n",
      "278/278 - 0s - loss: 0.0279 - 86ms/epoch - 308us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1051 - val_loss: 0.0344\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0361 - val_loss: 0.0313\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0323 - val_loss: 0.0284\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0298 - val_loss: 0.0277\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0290 - val_loss: 0.0271\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 688us/step - loss: 0.0287 - val_loss: 0.0744\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0292 - val_loss: 0.0274\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0280 - val_loss: 0.0271\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0288 - val_loss: 0.0264\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0274 - val_loss: 0.0269\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0307 - val_loss: 0.0261\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0297 - 120ms/epoch - 431us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0287 - 119ms/epoch - 427us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0296 - 118ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0367 - 117ms/epoch - 423us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0272 - 118ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 118ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0461 - 117ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0368 - 119ms/epoch - 427us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0297 - 118ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0274 - 119ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0271 - 120ms/epoch - 431us/step\n",
      "70/70 - 0s - loss: 0.0268 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0268 - 107ms/epoch - 386us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1078 - val_loss: 0.1346\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0369 - val_loss: 0.0307\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0344 - val_loss: 0.0296\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0305 - val_loss: 0.0307\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0293 - val_loss: 0.0279\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0310 - val_loss: 0.0272\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0278 - val_loss: 0.0495\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0319 - val_loss: 0.0282\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0289 - val_loss: 0.0266\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.0295 - val_loss: 0.0266\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0279 - val_loss: 0.0269\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.0339 - 124ms/epoch - 445us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.0363 - 123ms/epoch - 441us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.0283 - 121ms/epoch - 434us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.0275 - 118ms/epoch - 426us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.0315 - 118ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.0282 - 117ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.0330 - 117ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.0380 - 116ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.0290 - 117ms/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.0290 - 122ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.0345 - 129ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.0308 - 76ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.0307 - 93ms/epoch - 334us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3782 - val_loss: 0.3166\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.3327 - val_loss: 0.4504\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.3195 - val_loss: 0.3393\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 640us/step - loss: 0.3142 - val_loss: 0.2991\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.3164 - val_loss: 0.2992\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 645us/step - loss: 0.3190 - val_loss: 0.3262\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3080 - val_loss: 0.4860\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.3168 - val_loss: 0.2888\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.3281 - val_loss: 0.3004\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 612us/step - loss: 0.3165 - val_loss: 0.2895\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 626us/step - loss: 0.3248 - val_loss: 0.3144\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3540 - 118ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3588 - 112ms/epoch - 403us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3080 - 116ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3944 - 124ms/epoch - 444us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3076 - 130ms/epoch - 469us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.4744 - 116ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4531 - 120ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2998 - 116ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3002 - 119ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3556 - 128ms/epoch - 461us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2999 - 118ms/epoch - 423us/step\n",
      "70/70 - 0s - loss: 0.2939 - 68ms/epoch - 967us/step\n",
      "278/278 - 0s - loss: 0.2939 - 92ms/epoch - 330us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3947 - val_loss: 0.3179\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 638us/step - loss: 0.3194 - val_loss: 0.2996\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.3185 - val_loss: 0.2978\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.3373 - val_loss: 0.3016\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 642us/step - loss: 0.3158 - val_loss: 0.2999\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.3178 - val_loss: 0.2952\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3218 - val_loss: 0.2971\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.3168 - val_loss: 0.2865\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3151 - val_loss: 0.3037\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3192 - val_loss: 0.2902\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 623us/step - loss: 0.3218 - val_loss: 0.2987\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.4631 - 137ms/epoch - 492us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3052 - 125ms/epoch - 451us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.2990 - 119ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4327 - 157ms/epoch - 564us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3723 - 123ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2992 - 120ms/epoch - 432us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4223 - 191ms/epoch - 688us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3772 - 131ms/epoch - 471us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.2995 - 126ms/epoch - 453us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 458us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3899 - 127ms/epoch - 458us/step\n",
      "70/70 - 0s - loss: 0.2943 - 79ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2942 - 95ms/epoch - 341us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3624 - val_loss: 0.3177\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.3419 - val_loss: 0.3196\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3412 - val_loss: 0.3667\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 667us/step - loss: 0.3314 - val_loss: 0.3096\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.3364 - val_loss: 0.3324\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.3355 - val_loss: 0.3025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.3217 - val_loss: 0.3061\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.3149 - val_loss: 0.3035\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 631us/step - loss: 0.3260 - val_loss: 0.3380\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3121 - val_loss: 0.2881\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3141 - val_loss: 0.2969\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 122ms/epoch - 438us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.4016 - 119ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3005 - 116ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4045 - 115ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4195 - 116ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.3345 - 116ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.3063 - 125ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2954 - 127ms/epoch - 458us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3579 - 125ms/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3304 - 120ms/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.3007 - 121ms/epoch - 434us/step\n",
      "70/70 - 0s - loss: 0.2859 - 82ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2858 - 86ms/epoch - 310us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  21.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3558 - val_loss: 0.3438\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3256 - val_loss: 0.5836\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3140 - val_loss: 0.2953\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.3133 - val_loss: 0.2914\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 637us/step - loss: 0.3147 - val_loss: 1.5693\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3286 - val_loss: 0.3493\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 633us/step - loss: 0.3031 - val_loss: 0.2912\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.3061 - val_loss: 0.2818\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 625us/step - loss: 0.2972 - val_loss: 0.4654\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.3142 - val_loss: 0.2764\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 632us/step - loss: 0.3095 - val_loss: 0.2926\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3065 - 130ms/epoch - 468us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3538 - 136ms/epoch - 489us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3296 - 126ms/epoch - 452us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.3106 - 124ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.4847 - 126ms/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2962 - 130ms/epoch - 466us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.2911 - 122ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.2918 - 123ms/epoch - 442us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3779 - 141ms/epoch - 507us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3753 - 136ms/epoch - 489us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.2926 - 133ms/epoch - 478us/step\n",
      "70/70 - 0s - loss: 0.2845 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2847 - 89ms/epoch - 320us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3848 - val_loss: 0.3250\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.3340 - val_loss: 0.2941\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3317 - val_loss: 0.3055\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.3139 - val_loss: 0.3066\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3352 - val_loss: 0.3064\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.3177 - val_loss: 0.2936\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.3240 - val_loss: 0.3031\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.3333 - val_loss: 0.2881\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.3098 - val_loss: 0.3041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.3106 - val_loss: 0.2827\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.3217 - val_loss: 0.3031\n",
      "Epoch 1/11\n",
      "278/278 - 0s - loss: 0.3835 - 134ms/epoch - 484us/step\n",
      "Epoch 2/11\n",
      "278/278 - 0s - loss: 0.3741 - 119ms/epoch - 430us/step\n",
      "Epoch 3/11\n",
      "278/278 - 0s - loss: 0.3056 - 118ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "278/278 - 0s - loss: 0.4580 - 124ms/epoch - 447us/step\n",
      "Epoch 5/11\n",
      "278/278 - 0s - loss: 0.3001 - 134ms/epoch - 483us/step\n",
      "Epoch 6/11\n",
      "278/278 - 0s - loss: 0.2970 - 127ms/epoch - 455us/step\n",
      "Epoch 7/11\n",
      "278/278 - 0s - loss: 0.4821 - 126ms/epoch - 454us/step\n",
      "Epoch 8/11\n",
      "278/278 - 0s - loss: 0.3075 - 124ms/epoch - 446us/step\n",
      "Epoch 9/11\n",
      "278/278 - 0s - loss: 0.3250 - 119ms/epoch - 428us/step\n",
      "Epoch 10/11\n",
      "278/278 - 0s - loss: 0.3180 - 126ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "278/278 - 0s - loss: 0.4951 - 130ms/epoch - 466us/step\n",
      "70/70 - 0s - loss: 0.2916 - 74ms/epoch - 1ms/step\n",
      "278/278 - 0s - loss: 0.2917 - 90ms/epoch - 325us/step\n",
      "[CV] END kernel_regularizer=l1_l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>; total time=  20.5s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 4\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3532 - val_loss: 0.0434\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0252 - val_loss: 0.0167\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0149 - val_loss: 0.0125\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 621us/step - loss: 0.0116 - val_loss: 0.0106\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0101 - val_loss: 0.0089\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 618us/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 622us/step - loss: 0.0076 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0070 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0064 - val_loss: 0.0060\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0059 - val_loss: 0.0055\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 379ms/epoch - 455us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0053 - 376ms/epoch - 450us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0050 - 373ms/epoch - 448us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0053 - 356ms/epoch - 427us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0050 - 384ms/epoch - 461us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0048 - 392ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0050 - 495ms/epoch - 593us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 398ms/epoch - 477us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 385ms/epoch - 462us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 387ms/epoch - 464us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0046 - 395ms/epoch - 473us/step\n",
      "209/209 - 0s - loss: 0.0045 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0045 - 247ms/epoch - 297us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3719 - val_loss: 0.0465\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0306 - val_loss: 0.0232\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0185 - val_loss: 0.0155\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0133 - val_loss: 0.0114\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0106 - val_loss: 0.0090\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0085 - val_loss: 0.0080\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0081 - val_loss: 0.0074\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 648us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 647us/step - loss: 0.0057 - val_loss: 0.0055\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0053 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 406us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0054 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0049 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0052 - 361ms/epoch - 433us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0051 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0047 - 340ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0048 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0049 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0047 - 105ms/epoch - 504us/step\n",
      "834/834 - 0s - loss: 0.0047 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3726 - val_loss: 0.0481\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0336 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0191 - val_loss: 0.0159\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0147 - val_loss: 0.0128\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0121 - val_loss: 0.0108\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0104 - val_loss: 0.0095\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0091 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0083 - val_loss: 0.0077\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0073 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0064 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0066 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 358ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 362ms/epoch - 434us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0065 - 366ms/epoch - 439us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0056 - 353ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0064 - 356ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0060 - 355ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0054 - 356ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0055 - 350ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0069 - 354ms/epoch - 425us/step\n",
      "209/209 - 0s - loss: 0.0053 - 103ms/epoch - 491us/step\n",
      "834/834 - 0s - loss: 0.0053 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3506 - val_loss: 0.0491\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0345 - val_loss: 0.0253\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0210 - val_loss: 0.0172\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0156 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0124 - val_loss: 0.0113\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 619us/step - loss: 0.0104 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 627us/step - loss: 0.0093 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0082 - val_loss: 0.0082\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0075 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0067 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0062 - 349ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0070 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0061 - 351ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0054 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0056 - 107ms/epoch - 512us/step\n",
      "834/834 - 0s - loss: 0.0056 - 225ms/epoch - 269us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3701 - val_loss: 0.0489\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 662us/step - loss: 0.0344 - val_loss: 0.0242\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0207 - val_loss: 0.0168\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0155 - val_loss: 0.0134\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0124 - val_loss: 0.0111\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0111 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0093 - val_loss: 0.0087\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0084 - val_loss: 0.0079\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0081 - val_loss: 0.0071\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0069 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0068 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0068 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0061 - 345ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0072 - 344ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0062 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0060 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0065 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0059 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0055 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0057 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0062 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0054 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0054 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l1, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0315 - val_loss: 0.0136\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0130 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0078 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0068 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0051 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0081 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0027 - val_loss: 0.0075\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0031 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0049 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0023 - val_loss: 0.0029\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0056 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 420ms/epoch - 504us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0128 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0023 - 347ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0031 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 353ms/epoch - 423us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0086 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0021 - 345ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0032 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0018 - 101ms/epoch - 485us/step\n",
      "834/834 - 0s - loss: 0.0018 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 0.0056\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0094 - val_loss: 0.0031\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0085 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0049 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0044 - val_loss: 0.0063\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0054 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0029 - val_loss: 0.0343\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0036 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0077 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0266 - 341ms/epoch - 408us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0036 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0040 - 351ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0030 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0062 - 342ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0043 - 340ms/epoch - 408us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0024 - 344ms/epoch - 412us/step\n",
      "209/209 - 0s - loss: 0.0013 - 105ms/epoch - 502us/step\n",
      "834/834 - 0s - loss: 0.0013 - 227ms/epoch - 272us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0328 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0087 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0079 - val_loss: 0.0023\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0048 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0042 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0041 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0020 - val_loss: 0.0141\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0032 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0032 - val_loss: 0.0542\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0023 - val_loss: 0.0085\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0047 - 342ms/epoch - 410us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0055 - 342ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0044 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0031 - 344ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0062 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0034 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0034 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0037 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0042 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 0.0013 - 102ms/epoch - 490us/step\n",
      "834/834 - 0s - loss: 0.0013 - 229ms/epoch - 274us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0305 - val_loss: 0.0046\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0101 - val_loss: 0.0074\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0037 - val_loss: 0.0030\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0046 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0033 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0064 - val_loss: 0.0017\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0024 - 346ms/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0126 - 342ms/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0038 - 339ms/epoch - 407us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0018 - 345ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0088 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0018 - 341ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0051 - 339ms/epoch - 407us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0058 - 343ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0040 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0045 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 234ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0318 - val_loss: 0.0064\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0089 - val_loss: 0.0142\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 634us/step - loss: 0.0073 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0059 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0068 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0042 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0031 - val_loss: 0.0019\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 586us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0061 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 350ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0018 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0043 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0037 - 349ms/epoch - 419us/step\n",
      "209/209 - 0s - loss: 9.8222e-04 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 9.8258e-04 - 232ms/epoch - 279us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3783 - val_loss: 0.2375\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 545us/step - loss: 0.1780 - val_loss: 0.1296\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 539us/step - loss: 0.0977 - val_loss: 0.0717\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 532us/step - loss: 0.0545 - val_loss: 0.0405\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.0312 - val_loss: 0.0236\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0186 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0116 - val_loss: 0.0093\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0056 - val_loss: 0.0048\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0043 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 290ms/epoch - 348us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 272ms/epoch - 326us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 265ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 268ms/epoch - 321us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 283ms/epoch - 339us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 267ms/epoch - 320us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 265ms/epoch - 318us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 291ms/epoch - 349us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0021 - 355ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 314us/step\n",
      "209/209 - 0s - loss: 0.0021 - 108ms/epoch - 519us/step\n",
      "834/834 - 0s - loss: 0.0021 - 232ms/epoch - 278us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3651 - val_loss: 0.2418\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.1812 - val_loss: 0.1319\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0994 - val_loss: 0.0729\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 523us/step - loss: 0.0555 - val_loss: 0.0412\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 529us/step - loss: 0.0318 - val_loss: 0.0240\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0189 - val_loss: 0.0147\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0118 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0079 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 491us/step - loss: 0.0044 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 513us/step - loss: 0.0036 - val_loss: 0.0033\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 287ms/epoch - 344us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 286ms/epoch - 343us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 276ms/epoch - 330us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 272ms/epoch - 326us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 269ms/epoch - 323us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0024 - 269ms/epoch - 323us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 270ms/epoch - 323us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 268ms/epoch - 322us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 310ms/epoch - 372us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 314ms/epoch - 377us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 279ms/epoch - 334us/step\n",
      "209/209 - 0s - loss: 0.0020 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0020 - 235ms/epoch - 282us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3778 - val_loss: 0.2500\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.1872 - val_loss: 0.1362\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.1027 - val_loss: 0.0754\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0573 - val_loss: 0.0426\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0329 - val_loss: 0.0249\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0196 - val_loss: 0.0152\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0123 - val_loss: 0.0099\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 530us/step - loss: 0.0082 - val_loss: 0.0069\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0059 - val_loss: 0.0051\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 522us/step - loss: 0.0046 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 526us/step - loss: 0.0038 - val_loss: 0.0035\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0034 - 263ms/epoch - 316us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 260ms/epoch - 312us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 262ms/epoch - 314us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 261ms/epoch - 313us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0027 - 261ms/epoch - 312us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 275ms/epoch - 330us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0025 - 260ms/epoch - 311us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 261ms/epoch - 312us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 273ms/epoch - 328us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0023 - 280ms/epoch - 335us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 273ms/epoch - 327us/step\n",
      "209/209 - 0s - loss: 0.0021 - 107ms/epoch - 513us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3615 - val_loss: 0.2347\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.1759 - val_loss: 0.1282\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0967 - val_loss: 0.0710\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0540 - val_loss: 0.0402\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 537us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0117 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0057 - val_loss: 0.0050\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 533us/step - loss: 0.0037 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 273ms/epoch - 327us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 259ms/epoch - 311us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 264ms/epoch - 317us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0028 - 258ms/epoch - 309us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 267ms/epoch - 320us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 261ms/epoch - 313us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 263ms/epoch - 316us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 273ms/epoch - 328us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 258ms/epoch - 310us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 271ms/epoch - 325us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 261ms/epoch - 313us/step\n",
      "209/209 - 0s - loss: 0.0021 - 111ms/epoch - 531us/step\n",
      "834/834 - 0s - loss: 0.0021 - 233ms/epoch - 279us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  22.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.3648 - val_loss: 0.2338\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 527us/step - loss: 0.1753 - val_loss: 0.1277\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 524us/step - loss: 0.0963 - val_loss: 0.0707\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0539 - val_loss: 0.0401\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0310 - val_loss: 0.0235\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 521us/step - loss: 0.0185 - val_loss: 0.0144\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 528us/step - loss: 0.0116 - val_loss: 0.0094\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 525us/step - loss: 0.0078 - val_loss: 0.0065\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 518us/step - loss: 0.0057 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 520us/step - loss: 0.0044 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0036 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0033 - 257ms/epoch - 308us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0031 - 256ms/epoch - 307us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0029 - 253ms/epoch - 304us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0027 - 260ms/epoch - 312us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 270ms/epoch - 324us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 256ms/epoch - 307us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 260ms/epoch - 312us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 254ms/epoch - 304us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 255ms/epoch - 306us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0022 - 254ms/epoch - 304us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 262ms/epoch - 315us/step\n",
      "209/209 - 0s - loss: 0.0020 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0020 - 224ms/epoch - 268us/step\n",
      "[CV] END ...............kernel_regularizer=l2, optimizer=sgd; total time=  21.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1108 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0066 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 694us/step - loss: 0.0039 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 665us/step - loss: 0.0029 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 655us/step - loss: 0.0036 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 656us/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 684us/step - loss: 0.0024 - val_loss: 0.0030\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 650us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 654us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0046 - 345ms/epoch - 413us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0034 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0024 - 342ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0030 - 339ms/epoch - 406us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0033 - 338ms/epoch - 406us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0024 - 339ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 338ms/epoch - 405us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 336ms/epoch - 402us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0048 - 352ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0014 - 102ms/epoch - 488us/step\n",
      "834/834 - 0s - loss: 0.0014 - 228ms/epoch - 273us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  25.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1175 - val_loss: 0.0102\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 653us/step - loss: 0.0074 - val_loss: 0.0050\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0039 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 610us/step - loss: 0.0034 - val_loss: 0.0023\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0031 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0026 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0022 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0022 - val_loss: 0.0074\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 613us/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0055 - 357ms/epoch - 428us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0016 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0068 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0021 - 350ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0026 - 349ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0031 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0013 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0016 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0054 - 348ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0013 - 108ms/epoch - 517us/step\n",
      "834/834 - 0s - loss: 0.0013 - 226ms/epoch - 271us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1157 - val_loss: 0.0088\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0036 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0018\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0027 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0023 - val_loss: 0.0057\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0023 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0014 - val_loss: 9.3695e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 341ms/epoch - 409us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0017 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0048 - 340ms/epoch - 408us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 341ms/epoch - 409us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 340ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0018 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0019 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0011 - 103ms/epoch - 495us/step\n",
      "834/834 - 0s - loss: 0.0011 - 223ms/epoch - 267us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1163 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0065 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0042 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0034 - val_loss: 0.0028\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0025 - val_loss: 0.0026\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0021 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0018 - val_loss: 0.0051\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0053 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0032 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0026 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0026 - 341ms/epoch - 408us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0034 - 339ms/epoch - 407us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0032 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0021 - 340ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0026 - 343ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0018 - 362ms/epoch - 434us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0052 - 340ms/epoch - 407us/step\n",
      "209/209 - 0s - loss: 0.0014 - 107ms/epoch - 510us/step\n",
      "834/834 - 0s - loss: 0.0014 - 225ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1215 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0071 - val_loss: 0.0083\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0031 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0026 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 657us/step - loss: 0.0021 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0042 - 356ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0035 - 339ms/epoch - 407us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0020 - 339ms/epoch - 406us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 342ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0048 - 338ms/epoch - 405us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0013 - 341ms/epoch - 409us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0031 - 339ms/epoch - 407us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0018 - 339ms/epoch - 407us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0019 - 342ms/epoch - 410us/step\n",
      "209/209 - 0s - loss: 0.0012 - 102ms/epoch - 487us/step\n",
      "834/834 - 0s - loss: 0.0012 - 231ms/epoch - 277us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  23.7s\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 2\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0301 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0105 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0097 - val_loss: 0.0094\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0059 - val_loss: 0.0041\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0049 - val_loss: 0.0029\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0035 - val_loss: 0.0029\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0033 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0035 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0010\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0028 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0019 - val_loss: 8.6016e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 445us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0020 - 206ms/epoch - 329us/step\n",
      "2500/2500 - 1s - loss: 0.0020 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0373 - val_loss: 0.0112\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0137 - val_loss: 0.0131\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 587us/step - loss: 0.0075 - val_loss: 0.0059\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0027\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0055 - val_loss: 0.0028\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0075 - val_loss: 0.0028\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0035 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0030 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0028 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0034 - val_loss: 0.0214\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0046 - val_loss: 0.0015\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0040 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0011 - 1s/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 7.4554e-04 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 7.4529e-04 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0044\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0116 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 590us/step - loss: 0.0074 - val_loss: 0.0034\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 620us/step - loss: 0.0083 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0059 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0036 - val_loss: 0.0023\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0064 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 729us/step - loss: 0.0019 - val_loss: 0.0020\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0026 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0042 - 1s/epoch - 409us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.0551e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 9.0586e-04 - 674ms/epoch - 270us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.0s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0311\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0086 - val_loss: 0.0199\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0063 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0040 - val_loss: 0.0019\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0042 - val_loss: 0.0015\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0036 - val_loss: 0.0024\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0021 - val_loss: 0.0045\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0024 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0031 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0026 - val_loss: 9.5754e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0048 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0041 - 1s/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 426us/step\n",
      "625/625 - 0s - loss: 0.0011 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 657ms/epoch - 263us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0274 - val_loss: 0.0052\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0089 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0081 - val_loss: 0.0041\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0038 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0042 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0041 - val_loss: 0.0022\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0033 - val_loss: 0.0102\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0026 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0030 - val_loss: 0.0010\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 9.5333e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0057 - 1s/epoch - 416us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0068 - 1s/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 433us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 7.8962e-04 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 7.8973e-04 - 659ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>; total time=  35.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1327 - val_loss: 0.0109\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 583us/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0030 - val_loss: 0.0044\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0021 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0021 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 977ms/epoch - 391us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 410us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 413us/step\n",
      "625/625 - 0s - loss: 0.0010 - 216ms/epoch - 346us/step\n",
      "2500/2500 - 1s - loss: 0.0010 - 655ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1080 - val_loss: 0.0094\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0063 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0040 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0030 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0022 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 592us/step - loss: 0.0017 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0020 - val_loss: 9.9945e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 412us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 9.6913e-04 - 212ms/epoch - 339us/step\n",
      "2500/2500 - 1s - loss: 9.6887e-04 - 661ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1281 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0067 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0042 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0034 - val_loss: 0.0025\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0031 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 600us/step - loss: 0.0025 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 608us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0024 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0017 - val_loss: 0.0087\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 341us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 656ms/epoch - 262us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  35.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1250 - val_loss: 0.0110\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0069 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0031 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0035 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0035 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 611us/step - loss: 0.0024 - val_loss: 0.0029\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0025 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0020 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0024 - val_loss: 0.0012\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 411us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0016 - 1s/epoch - 414us/step\n",
      "625/625 - 0s - loss: 9.1906e-04 - 217ms/epoch - 347us/step\n",
      "2500/2500 - 1s - loss: 9.1922e-04 - 660ms/epoch - 264us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.8s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1172 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0062 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0038 - val_loss: 0.0030\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0032 - val_loss: 0.0046\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 606us/step - loss: 0.0026 - val_loss: 0.0014\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 609us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 604us/step - loss: 0.0018 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 607us/step - loss: 0.0016 - val_loss: 0.0012\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0016 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0022 - 1s/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 449us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0014 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0019 - 1s/epoch - 421us/step\n",
      "625/625 - 0s - loss: 0.0011 - 233ms/epoch - 373us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 702ms/epoch - 281us/step\n",
      "[CV] END kernel_regularizer=l2, optimizer=<keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>; total time=  34.9s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1129 - val_loss: 0.0086\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 596us/step - loss: 0.0060 - val_loss: 0.0034\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 603us/step - loss: 0.0041 - val_loss: 0.0024\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 597us/step - loss: 0.0035 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0029 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0024 - val_loss: 0.0025\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 601us/step - loss: 0.0023 - val_loss: 0.0017\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 594us/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 615us/step - loss: 0.0022 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0028 - 1s/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 452us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0020 - 1s/epoch - 444us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0027 - 1s/epoch - 450us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 468us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0026 - 1s/epoch - 448us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0017 - 1s/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0018 - 1s/epoch - 446us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;,\n",
       "                    param_grid={&#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                                &#x27;optimizer&#x27;: [&#x27;sgd&#x27;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60&gt;,\n",
       "                                              &lt;keras.optimizers.legacy.adam.Adam object at 0x31e2a4070&gt;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x31e2a4940>,\n",
       "                    param_grid={'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                                'optimizer': ['sgd',\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4a60>,\n",
       "                                              <keras.optimizers.legacy.adam.Adam object at 0x31e2a4070>]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', optimizer='adam', kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'optimizer': ['sgd', Adam(learning_rate=0.001), Adam(learning_rate=0.01), Adam(learning_rate=0.1)],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>16.997419</td>\n",
       "      <td>0.293825</td>\n",
       "      <td>0.079295</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>l1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.014213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.014210</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.010464</td>\n",
       "      <td>-0.010404</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.001605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.082942</td>\n",
       "      <td>0.169986</td>\n",
       "      <td>0.081252</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.005692</td>\n",
       "      <td>-0.005540</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005991</td>\n",
       "      <td>-0.005397</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.000223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.774823</td>\n",
       "      <td>0.208586</td>\n",
       "      <td>0.081163</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.026682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027134</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>15</td>\n",
       "      <td>-0.026685</td>\n",
       "      <td>-0.026955</td>\n",
       "      <td>-0.026410</td>\n",
       "      <td>-0.028737</td>\n",
       "      <td>-0.026886</td>\n",
       "      <td>-0.027135</td>\n",
       "      <td>0.000823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.576868</td>\n",
       "      <td>0.366474</td>\n",
       "      <td>0.079455</td>\n",
       "      <td>0.003233</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.251134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.249694</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>17</td>\n",
       "      <td>-0.251033</td>\n",
       "      <td>-0.248961</td>\n",
       "      <td>-0.254116</td>\n",
       "      <td>-0.245601</td>\n",
       "      <td>-0.248622</td>\n",
       "      <td>-0.249666</td>\n",
       "      <td>0.002820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.627119</td>\n",
       "      <td>0.196109</td>\n",
       "      <td>0.079965</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002666</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002632</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.002665</td>\n",
       "      <td>-0.002537</td>\n",
       "      <td>-0.002637</td>\n",
       "      <td>-0.002655</td>\n",
       "      <td>-0.002661</td>\n",
       "      <td>-0.002631</td>\n",
       "      <td>0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.685975</td>\n",
       "      <td>0.251114</td>\n",
       "      <td>0.078898</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001247</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001442</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.001248</td>\n",
       "      <td>-0.002222</td>\n",
       "      <td>-0.001386</td>\n",
       "      <td>-0.001109</td>\n",
       "      <td>-0.001138</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>0.000412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>19.925349</td>\n",
       "      <td>0.165671</td>\n",
       "      <td>0.079988</td>\n",
       "      <td>0.003639</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003046</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.003359</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>-0.007301</td>\n",
       "      <td>-0.001889</td>\n",
       "      <td>-0.003083</td>\n",
       "      <td>0.002226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.040764</td>\n",
       "      <td>0.241711</td>\n",
       "      <td>0.079797</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.005327</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009157</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>-0.005871</td>\n",
       "      <td>-0.005183</td>\n",
       "      <td>-0.012737</td>\n",
       "      <td>-0.015338</td>\n",
       "      <td>-0.008880</td>\n",
       "      <td>0.004297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>17.926032</td>\n",
       "      <td>0.387758</td>\n",
       "      <td>0.083417</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': '...</td>\n",
       "      <td>-0.011382</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011686</td>\n",
       "      <td>0.000507</td>\n",
       "      <td>13</td>\n",
       "      <td>-0.011381</td>\n",
       "      <td>-0.012582</td>\n",
       "      <td>-0.011095</td>\n",
       "      <td>-0.011546</td>\n",
       "      <td>-0.011830</td>\n",
       "      <td>-0.011687</td>\n",
       "      <td>0.000507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.120462</td>\n",
       "      <td>0.067826</td>\n",
       "      <td>0.083020</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.007023</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006413</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.007033</td>\n",
       "      <td>-0.006217</td>\n",
       "      <td>-0.005919</td>\n",
       "      <td>-0.006353</td>\n",
       "      <td>-0.006546</td>\n",
       "      <td>-0.006414</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.078545</td>\n",
       "      <td>0.151325</td>\n",
       "      <td>0.092128</td>\n",
       "      <td>0.008104</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.028657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028504</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>16</td>\n",
       "      <td>-0.028676</td>\n",
       "      <td>-0.028378</td>\n",
       "      <td>-0.027875</td>\n",
       "      <td>-0.026800</td>\n",
       "      <td>-0.030723</td>\n",
       "      <td>-0.028491</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11111</td>\n",
       "      <td>20.761524</td>\n",
       "      <td>0.236672</td>\n",
       "      <td>0.089498</td>\n",
       "      <td>0.005330</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1_l2', 'optimizer': &lt;...</td>\n",
       "      <td>-0.293942</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.290032</td>\n",
       "      <td>0.004074</td>\n",
       "      <td>18</td>\n",
       "      <td>-0.293926</td>\n",
       "      <td>-0.294151</td>\n",
       "      <td>-0.285779</td>\n",
       "      <td>-0.284657</td>\n",
       "      <td>-0.291711</td>\n",
       "      <td>-0.290045</td>\n",
       "      <td>0.004048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.141023</td>\n",
       "      <td>0.367304</td>\n",
       "      <td>0.119679</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>l1</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l1', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005074</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.004456</td>\n",
       "      <td>-0.004653</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>-0.005073</td>\n",
       "      <td>0.000441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>23.382346</td>\n",
       "      <td>0.121729</td>\n",
       "      <td>0.117038</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>-0.001265</td>\n",
       "      <td>-0.001696</td>\n",
       "      <td>-0.000983</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>21.985600</td>\n",
       "      <td>0.174433</td>\n",
       "      <td>0.121637</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>l2</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': 'sgd'}</td>\n",
       "      <td>-0.002051</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>-0.002024</td>\n",
       "      <td>-0.002124</td>\n",
       "      <td>-0.002068</td>\n",
       "      <td>-0.002035</td>\n",
       "      <td>-0.002061</td>\n",
       "      <td>0.000035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>33333</td>\n",
       "      <td>24.130400</td>\n",
       "      <td>0.423410</td>\n",
       "      <td>0.118057</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.001427</td>\n",
       "      <td>-0.001292</td>\n",
       "      <td>-0.001089</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>-0.001202</td>\n",
       "      <td>-0.001290</td>\n",
       "      <td>0.000134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.752902</td>\n",
       "      <td>0.121369</td>\n",
       "      <td>0.224412</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001968</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001110</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.001972</td>\n",
       "      <td>-0.000745</td>\n",
       "      <td>-0.000906</td>\n",
       "      <td>-0.001142</td>\n",
       "      <td>-0.000790</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>0.000452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>34.617896</td>\n",
       "      <td>0.145296</td>\n",
       "      <td>0.233857</td>\n",
       "      <td>0.006886</td>\n",
       "      <td>l2</td>\n",
       "      <td>&lt;keras.optimizers.legacy.adam.Adam object at 0...</td>\n",
       "      <td>{'kernel_regularizer': 'l2', 'optimizer': &lt;ker...</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.001307</td>\n",
       "      <td>-0.000919</td>\n",
       "      <td>-0.001052</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>0.000135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0      0        11111      16.997419      0.293825         0.079295   \n",
       "1      0        11111      19.082942      0.169986         0.081252   \n",
       "2      0        11111      19.774823      0.208586         0.081163   \n",
       "3      0        11111      19.576868      0.366474         0.079455   \n",
       "4      0        11111      17.627119      0.196109         0.079965   \n",
       "5      0        11111      19.685975      0.251114         0.078898   \n",
       "6      0        11111      19.925349      0.165671         0.079988   \n",
       "7      0        11111      20.040764      0.241711         0.079797   \n",
       "8      0        11111      17.926032      0.387758         0.083417   \n",
       "9      0        11111      20.120462      0.067826         0.083020   \n",
       "10     0        11111      20.078545      0.151325         0.092128   \n",
       "11     0        11111      20.761524      0.236672         0.089498   \n",
       "12     1        33333      24.141023      0.367304         0.119679   \n",
       "13     1        33333      23.382346      0.121729         0.117038   \n",
       "14     1        33333      21.985600      0.174433         0.121637   \n",
       "15     1        33333      24.130400      0.423410         0.118057   \n",
       "16     2        99999      34.752902      0.121369         0.224412   \n",
       "17     2        99999      34.617896      0.145296         0.233857   \n",
       "\n",
       "    std_score_time param_kernel_regularizer  \\\n",
       "0         0.001660                       l1   \n",
       "1         0.002900                       l1   \n",
       "2         0.002130                       l1   \n",
       "3         0.003233                       l1   \n",
       "4         0.000889                       l2   \n",
       "5         0.000797                       l2   \n",
       "6         0.003639                       l2   \n",
       "7         0.002502                       l2   \n",
       "8         0.002099                    l1_l2   \n",
       "9         0.001753                    l1_l2   \n",
       "10        0.008104                    l1_l2   \n",
       "11        0.005330                    l1_l2   \n",
       "12        0.002844                       l1   \n",
       "13        0.001417                       l2   \n",
       "14        0.002785                       l2   \n",
       "15        0.003221                       l2   \n",
       "16        0.002090                       l2   \n",
       "17        0.006886                       l2   \n",
       "\n",
       "                                      param_optimizer  \\\n",
       "0                                                 sgd   \n",
       "1   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "2   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "3   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "4                                                 sgd   \n",
       "5   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "6   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "7   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "8                                                 sgd   \n",
       "9   <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "10  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "11  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "12  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "13  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "14                                                sgd   \n",
       "15  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "16  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "17  <keras.optimizers.legacy.adam.Adam object at 0...   \n",
       "\n",
       "                                               params  split0_test_score  ...  \\\n",
       "0    {'kernel_regularizer': 'l1', 'optimizer': 'sgd'}          -0.014213  ...   \n",
       "1   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.005693  ...   \n",
       "2   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.026682  ...   \n",
       "3   {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.251134  ...   \n",
       "4    {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002666  ...   \n",
       "5   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001247  ...   \n",
       "6   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001318  ...   \n",
       "7   {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.005327  ...   \n",
       "8   {'kernel_regularizer': 'l1_l2', 'optimizer': '...          -0.011382  ...   \n",
       "9   {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.007023  ...   \n",
       "10  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.028657  ...   \n",
       "11  {'kernel_regularizer': 'l1_l2', 'optimizer': <...          -0.293942  ...   \n",
       "12  {'kernel_regularizer': 'l1', 'optimizer': <ker...          -0.004456  ...   \n",
       "13  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001846  ...   \n",
       "14   {'kernel_regularizer': 'l2', 'optimizer': 'sgd'}          -0.002051  ...   \n",
       "15  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001426  ...   \n",
       "16  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001968  ...   \n",
       "17  {'kernel_regularizer': 'l2', 'optimizer': <ker...          -0.001019  ...   \n",
       "\n",
       "    mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         -0.012012        0.001611               14           -0.014210   \n",
       "1         -0.005604        0.000223               10           -0.005692   \n",
       "2         -0.027134        0.000823               15           -0.026685   \n",
       "3         -0.249694        0.002871               17           -0.251033   \n",
       "4         -0.002632        0.000044                7           -0.002665   \n",
       "5         -0.001442        0.000451                5           -0.001248   \n",
       "6         -0.003046        0.002154                8           -0.001318   \n",
       "7         -0.009157        0.004497               12           -0.005273   \n",
       "8         -0.011686        0.000507               13           -0.011381   \n",
       "9         -0.006413        0.000367               11           -0.007033   \n",
       "10        -0.028504        0.001293               16           -0.028676   \n",
       "11        -0.290032        0.004074               18           -0.293926   \n",
       "12        -0.005074        0.000441                9           -0.004456   \n",
       "13        -0.001427        0.000309                4           -0.001846   \n",
       "14        -0.002061        0.000035                6           -0.002052   \n",
       "15        -0.001290        0.000134                3           -0.001427   \n",
       "16        -0.001110        0.000450                2           -0.001972   \n",
       "17        -0.001054        0.000135                1           -0.001019   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0            -0.013634           -0.010464           -0.010404   \n",
       "1            -0.005540           -0.005397           -0.005991   \n",
       "2            -0.026955           -0.026410           -0.028737   \n",
       "3            -0.248961           -0.254116           -0.245601   \n",
       "4            -0.002537           -0.002637           -0.002655   \n",
       "5            -0.002222           -0.001386           -0.001109   \n",
       "6            -0.003359           -0.001547           -0.007301   \n",
       "7            -0.005871           -0.005183           -0.012737   \n",
       "8            -0.012582           -0.011095           -0.011546   \n",
       "9            -0.006217           -0.005919           -0.006353   \n",
       "10           -0.028378           -0.027875           -0.026800   \n",
       "11           -0.294151           -0.285779           -0.284657   \n",
       "12           -0.004653           -0.005298           -0.005607   \n",
       "13           -0.001345           -0.001265           -0.001696   \n",
       "14           -0.002024           -0.002124           -0.002068   \n",
       "15           -0.001292           -0.001089           -0.001441   \n",
       "16           -0.000745           -0.000906           -0.001142   \n",
       "17           -0.000969           -0.001307           -0.000919   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0            -0.011347         -0.012012         0.001605  \n",
       "1            -0.005397         -0.005604         0.000223  \n",
       "2            -0.026886         -0.027135         0.000823  \n",
       "3            -0.248622         -0.249666         0.002820  \n",
       "4            -0.002661         -0.002631         0.000048  \n",
       "5            -0.001138         -0.001421         0.000412  \n",
       "6            -0.001889         -0.003083         0.002226  \n",
       "7            -0.015338         -0.008880         0.004297  \n",
       "8            -0.011830         -0.011687         0.000507  \n",
       "9            -0.006546         -0.006414         0.000371  \n",
       "10           -0.030723         -0.028491         0.001286  \n",
       "11           -0.291711         -0.290045         0.004048  \n",
       "12           -0.005352         -0.005073         0.000441  \n",
       "13           -0.000983         -0.001427         0.000309  \n",
       "14           -0.002035         -0.002061         0.000035  \n",
       "15           -0.001202         -0.001290         0.000134  \n",
       "16           -0.000790         -0.001111         0.000452  \n",
       "17           -0.001052         -0.001053         0.000135  \n",
       "\n",
       "[18 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'kernel_regularizer': 'l2', 'optimizer': <keras.optimizers.legacy.adam.Adam object at 0x31e2a44c0>}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/377624069.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 33333\n",
      "max_resources_: 100000\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 3\n",
      "n_resources: 33333\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1467 - val_loss: 0.0128\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0077 - val_loss: 0.0043\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 599us/step - loss: 0.0045 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 588us/step - loss: 0.0032 - val_loss: 0.0068\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0028 - val_loss: 0.0017\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0019 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0022 - val_loss: 0.0012\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0016 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0018 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0014 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0050 - 352ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 349ms/epoch - 418us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0057 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0027 - 348ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0041 - 365ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 358ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 362ms/epoch - 435us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0027 - 365ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0027 - 385ms/epoch - 462us/step\n",
      "209/209 - 0s - loss: 0.0019 - 112ms/epoch - 536us/step\n",
      "834/834 - 0s - loss: 0.0019 - 239ms/epoch - 287us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1279 - val_loss: 0.0116\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0064 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0039 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0029 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0025 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 624us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0025 - val_loss: 0.0028\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0022 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0016 - val_loss: 0.0111\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0057 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0034 - 348ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0022 - 349ms/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0035 - 347ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0016 - 364ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0033 - 346ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0012 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0038 - 344ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0017 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0012 - 107ms/epoch - 514us/step\n",
      "834/834 - 0s - loss: 0.0012 - 227ms/epoch - 272us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1154 - val_loss: 0.0111\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0069 - val_loss: 0.0042\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0030 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0023 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0023 - val_loss: 0.0070\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0020 - val_loss: 0.0013\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0017 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0026 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0041 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0059 - 346ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0024 - 345ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0016 - 344ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0048 - 345ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0023 - 344ms/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0014 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0039 - 346ms/epoch - 415us/step\n",
      "209/209 - 0s - loss: 0.0014 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1286 - val_loss: 0.0118\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0070 - val_loss: 0.0037\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0044 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0036 - val_loss: 0.0021\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0026 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0021 - val_loss: 0.0083\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0025 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0019 - val_loss: 0.0019\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0019 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0056 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 351ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0033 - 350ms/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0025 - 350ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0030 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0025 - 351ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0015 - 349ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0022 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0031 - 354ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0024 - 366ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0018 - 351ms/epoch - 421us/step\n",
      "209/209 - 0s - loss: 0.0016 - 105ms/epoch - 503us/step\n",
      "834/834 - 0s - loss: 0.0016 - 230ms/epoch - 275us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1365 - val_loss: 0.0104\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0073 - val_loss: 0.0057\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0035 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0027 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0028 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0025 - val_loss: 0.0052\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0024 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 0.0016\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 576us/step - loss: 0.0020 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0058 - 350ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0030 - 359ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0021 - 372ms/epoch - 446us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0025 - 380ms/epoch - 456us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 386ms/epoch - 463us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0036 - 350ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0019 - 351ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0053 - 344ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0021 - 346ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0015 - 104ms/epoch - 497us/step\n",
      "834/834 - 0s - loss: 0.0015 - 230ms/epoch - 276us/step\n",
      "[CV] END ................................learning_rate=0.001; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0291 - val_loss: 0.0171\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0088 - val_loss: 0.0028\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0060 - val_loss: 0.0453\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 595us/step - loss: 0.0071 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0034 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 536us/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0035 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0032 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 543us/step - loss: 0.0024 - val_loss: 0.0018\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 546us/step - loss: 0.0033 - val_loss: 0.0065\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 363ms/epoch - 436us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0048 - 344ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0039 - 342ms/epoch - 410us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0033 - 357ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 356ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0017 - 365ms/epoch - 438us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0039 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0042 - 350ms/epoch - 419us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0030 - 354ms/epoch - 425us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0075 - 377ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0043 - 364ms/epoch - 437us/step\n",
      "209/209 - 0s - loss: 0.0014 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0014 - 229ms/epoch - 274us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0063\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 643us/step - loss: 0.0096 - val_loss: 0.0039\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 672us/step - loss: 0.0065 - val_loss: 0.0026\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 602us/step - loss: 0.0069 - val_loss: 0.0104\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 617us/step - loss: 0.0046 - val_loss: 0.0021\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 630us/step - loss: 0.0038 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0035 - val_loss: 0.0012\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 605us/step - loss: 0.0023 - val_loss: 0.0015\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 598us/step - loss: 0.0046 - val_loss: 0.0013\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0038 - 394ms/epoch - 473us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0036 - 363ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0058 - 369ms/epoch - 443us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0045 - 373ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0052 - 352ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0029 - 345ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0024 - 377ms/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0132 - 371ms/epoch - 445us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0025 - 401ms/epoch - 481us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0039 - 377ms/epoch - 451us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0031 - 390ms/epoch - 468us/step\n",
      "209/209 - 0s - loss: 0.0019 - 114ms/epoch - 544us/step\n",
      "834/834 - 0s - loss: 0.0019 - 241ms/epoch - 289us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0508\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 549us/step - loss: 0.0094 - val_loss: 0.0137\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0075 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0063 - val_loss: 0.0029\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0047 - val_loss: 0.0027\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0050 - val_loss: 0.0020\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0023 - val_loss: 0.0012\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0035 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0020 - val_loss: 9.9140e-04\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0018 - val_loss: 9.0679e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0022 - val_loss: 0.0020\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0032 - 352ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0037 - 346ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0030 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0021 - 343ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0093 - 343ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0012 - 343ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0056 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0016 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0024 - 343ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0030 - 345ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 344ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0011 - 105ms/epoch - 501us/step\n",
      "834/834 - 0s - loss: 0.0011 - 231ms/epoch - 277us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0290 - val_loss: 0.1035\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0100 - val_loss: 0.0038\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 554us/step - loss: 0.0069 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0053 - val_loss: 0.0099\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0053 - val_loss: 0.0302\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0044 - val_loss: 0.0149\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0036 - val_loss: 0.0015\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0047 - val_loss: 0.0197\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0018 - val_loss: 0.0014\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0047 - val_loss: 0.0016\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0030 - val_loss: 0.0014\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0045 - 351ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0015 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0045 - 344ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0046 - 346ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0019 - 347ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0061 - 346ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0022 - 374ms/epoch - 449us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0037 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0063 - 346ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0034 - 346ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0015 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0017 - 103ms/epoch - 494us/step\n",
      "834/834 - 0s - loss: 0.0017 - 230ms/epoch - 275us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0285 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0108 - val_loss: 0.0376\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 671us/step - loss: 0.0060 - val_loss: 0.0108\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0046 - val_loss: 0.0156\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0066 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0031 - val_loss: 0.0055\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0024 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0054 - val_loss: 0.0026\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 559us/step - loss: 0.0025 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0020 - val_loss: 9.9628e-04\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0032 - val_loss: 9.7153e-04\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0111 - 350ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0017 - 344ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0028 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0058 - 345ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0039 - 358ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0041 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0028 - 345ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0029 - 346ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0026 - 347ms/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0022 - 345ms/epoch - 414us/step\n",
      "209/209 - 0s - loss: 0.0010 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0010 - 228ms/epoch - 273us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0932 - val_loss: 0.0262\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0370 - val_loss: 0.0082\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0169 - val_loss: 0.0052\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0259 - val_loss: 0.0095\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0276 - val_loss: 0.0130\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0364 - val_loss: 0.0138\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 558us/step - loss: 0.0217 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0214 - val_loss: 0.0038\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0522 - val_loss: 0.0041\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0293 - val_loss: 0.0068\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0220 - val_loss: 0.0050\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0542 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.1068 - 345ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0710 - 347ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0542 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0290 - 344ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0725 - 346ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0229 - 346ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0408 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0608 - 347ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0762 - 346ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0452 - 343ms/epoch - 411us/step\n",
      "209/209 - 0s - loss: 0.0050 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0051 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0937 - val_loss: 0.0294\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0345 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0197 - val_loss: 0.0057\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0283 - val_loss: 0.0257\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0376 - val_loss: 0.4935\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0198 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0351 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0269 - val_loss: 0.0036\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0196 - val_loss: 0.0424\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 616us/step - loss: 0.0198 - val_loss: 0.0033\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0276 - val_loss: 0.0040\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0755 - 354ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0967 - 351ms/epoch - 421us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0351 - 357ms/epoch - 428us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0412 - 348ms/epoch - 418us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0716 - 350ms/epoch - 420us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0417 - 350ms/epoch - 420us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0276 - 349ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 1.5169 - 352ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 1.5450 - 351ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 1.5481 - 351ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 1.5502 - 352ms/epoch - 422us/step\n",
      "209/209 - 0s - loss: 0.8013 - 106ms/epoch - 507us/step\n",
      "834/834 - 0s - loss: 0.8014 - 227ms/epoch - 272us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.1096 - val_loss: 0.1054\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 547us/step - loss: 0.0358 - val_loss: 0.0166\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0253 - val_loss: 0.1548\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0269 - val_loss: 0.0124\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0430 - val_loss: 0.0049\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0157 - val_loss: 0.0038\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0221 - val_loss: 0.0041\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 566us/step - loss: 0.0215 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0313 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0159 - val_loss: 0.0161\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0169 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1425 - 349ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0109 - 349ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0512 - 349ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0479 - 344ms/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.1069 - 342ms/epoch - 410us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0236 - 345ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0566 - 343ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0248 - 343ms/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0627 - 349ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0987 - 345ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0457 - 345ms/epoch - 413us/step\n",
      "209/209 - 0s - loss: 0.0037 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0038 - 230ms/epoch - 275us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.1s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0934 - val_loss: 0.1095\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 553us/step - loss: 0.0387 - val_loss: 0.0405\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0565 - val_loss: 0.0036\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0447 - val_loss: 0.0185\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0229 - val_loss: 0.0273\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0298 - val_loss: 0.0308\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0259 - val_loss: 0.0738\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0297 - val_loss: 0.0254\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 551us/step - loss: 0.0316 - val_loss: 0.0104\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0342 - val_loss: 0.0136\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0268 - val_loss: 0.0074\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.1429 - 345ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0496 - 347ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0444 - 346ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0441 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0174 - 345ms/epoch - 413us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.1005 - 348ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0664 - 345ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0206 - 342ms/epoch - 410us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0744 - 347ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0719 - 343ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.0807 - 348ms/epoch - 417us/step\n",
      "209/209 - 0s - loss: 0.0127 - 104ms/epoch - 496us/step\n",
      "834/834 - 0s - loss: 0.0127 - 228ms/epoch - 273us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.2s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0922 - val_loss: 0.0097\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 591us/step - loss: 0.0301 - val_loss: 0.0130\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 541us/step - loss: 0.0355 - val_loss: 0.0867\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 585us/step - loss: 0.0348 - val_loss: 0.0072\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 639us/step - loss: 0.0178 - val_loss: 0.3569\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 580us/step - loss: 0.0355 - val_loss: 0.0067\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 544us/step - loss: 0.0301 - val_loss: 0.0554\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0603 - val_loss: 0.0047\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 542us/step - loss: 0.0203 - val_loss: 0.0049\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 584us/step - loss: 0.0334 - val_loss: 0.0084\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 644us/step - loss: 0.0160 - val_loss: 0.0043\n",
      "Epoch 1/11\n",
      "834/834 - 0s - loss: 0.0593 - 353ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "834/834 - 0s - loss: 0.0395 - 348ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "834/834 - 0s - loss: 0.0196 - 351ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "834/834 - 0s - loss: 0.0470 - 347ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "834/834 - 0s - loss: 0.0435 - 360ms/epoch - 432us/step\n",
      "Epoch 6/11\n",
      "834/834 - 0s - loss: 0.0784 - 357ms/epoch - 428us/step\n",
      "Epoch 7/11\n",
      "834/834 - 0s - loss: 0.0150 - 347ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "834/834 - 0s - loss: 0.0642 - 348ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "834/834 - 0s - loss: 0.0654 - 348ms/epoch - 418us/step\n",
      "Epoch 10/11\n",
      "834/834 - 0s - loss: 0.0266 - 349ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "834/834 - 0s - loss: 0.1292 - 348ms/epoch - 418us/step\n",
      "209/209 - 0s - loss: 0.0062 - 104ms/epoch - 499us/step\n",
      "834/834 - 0s - loss: 0.0062 - 231ms/epoch - 277us/step\n",
      "[CV] END ..................................learning_rate=0.1; total time=  23.8s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 1\n",
      "n_resources: 99999\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0356 - val_loss: 0.0091\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 555us/step - loss: 0.0111 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 552us/step - loss: 0.0097 - val_loss: 0.0058\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0040 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0042 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0060 - val_loss: 0.0036\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0025 - val_loss: 0.0053\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0025 - val_loss: 0.0013\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 548us/step - loss: 0.0029 - val_loss: 0.0058\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 550us/step - loss: 0.0028 - val_loss: 0.0023\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0044 - 1s/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0047 - 1s/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 416us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0062 - 1s/epoch - 416us/step\n",
      "625/625 - 0s - loss: 0.0013 - 213ms/epoch - 340us/step\n",
      "2500/2500 - 1s - loss: 0.0013 - 664ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.3s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0321 - val_loss: 0.0074\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0107 - val_loss: 0.0040\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 614us/step - loss: 0.0084 - val_loss: 0.0031\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0057 - val_loss: 0.0024\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0045 - val_loss: 0.0025\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0045 - val_loss: 0.0027\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0029 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0030 - val_loss: 0.0027\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0040 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0027 - val_loss: 0.0109\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0038 - 1s/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0020 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 422us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0027 - 1s/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0018 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0015 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0015 - 663ms/epoch - 265us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.7s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0366 - val_loss: 0.0047\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 563us/step - loss: 0.0097 - val_loss: 0.0032\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0084 - val_loss: 0.0027\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 569us/step - loss: 0.0090 - val_loss: 0.0032\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0039 - val_loss: 0.0018\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0036 - val_loss: 0.0016\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 557us/step - loss: 0.0030 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0027 - val_loss: 0.0011\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 564us/step - loss: 0.0039 - val_loss: 0.0011\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 582us/step - loss: 0.0028 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0024 - val_loss: 0.0010\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0055 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0037 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0036 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0043 - 1s/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0025 - 1s/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0035 - 1s/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "625/625 - 0s - loss: 0.0011 - 207ms/epoch - 331us/step\n",
      "2500/2500 - 1s - loss: 0.0011 - 655ms/epoch - 262us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.5s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0361 - val_loss: 0.0061\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0099 - val_loss: 0.0056\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0063 - val_loss: 0.0025\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0066 - val_loss: 0.0020\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 575us/step - loss: 0.0044 - val_loss: 0.0015\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 561us/step - loss: 0.0058 - val_loss: 0.0018\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 562us/step - loss: 0.0039 - val_loss: 0.0014\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0014\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0027 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0042 - val_loss: 0.0014\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 570us/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0058 - 1s/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0033 - 1s/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 420us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0061 - 1s/epoch - 412us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0028 - 1s/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0032 - 1s/epoch - 412us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0029 - 1s/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0031 - 1s/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0075 - 1s/epoch - 422us/step\n",
      "625/625 - 0s - loss: 0.0017 - 209ms/epoch - 335us/step\n",
      "2500/2500 - 1s - loss: 0.0017 - 660ms/epoch - 264us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.6s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0358 - val_loss: 0.0076\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 571us/step - loss: 0.0101 - val_loss: 0.0045\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0153 - val_loss: 0.0080\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 577us/step - loss: 0.0067 - val_loss: 0.0284\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 581us/step - loss: 0.0046 - val_loss: 0.0020\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0063 - val_loss: 0.0019\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 572us/step - loss: 0.0037 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0049 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0031 - val_loss: 0.0013\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 560us/step - loss: 0.0020 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0020 - val_loss: 9.4648e-04\n",
      "Epoch 1/11\n",
      "2500/2500 - 1s - loss: 0.0060 - 1s/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "2500/2500 - 1s - loss: 0.0030 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2500/2500 - 1s - loss: 0.0039 - 1s/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "2500/2500 - 1s - loss: 0.0021 - 1s/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "2500/2500 - 1s - loss: 0.0034 - 1s/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2500/2500 - 1s - loss: 0.0026 - 1s/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "2500/2500 - 1s - loss: 0.0023 - 1s/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "2500/2500 - 1s - loss: 0.0024 - 1s/epoch - 415us/step\n",
      "625/625 - 0s - loss: 7.8604e-04 - 210ms/epoch - 336us/step\n",
      "2500/2500 - 1s - loss: 7.8599e-04 - 666ms/epoch - 266us/step\n",
      "[CV] END .................................learning_rate=0.01; total time=  34.4s\n",
      "Epoch 1/11\n",
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0344 - val_loss: 0.0090\n",
      "Epoch 2/11\n",
      "1563/1563 [==============================] - 1s 556us/step - loss: 0.0090 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "1563/1563 [==============================] - 1s 578us/step - loss: 0.0079 - val_loss: 0.0028\n",
      "Epoch 4/11\n",
      "1563/1563 [==============================] - 1s 568us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 5/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0074 - val_loss: 0.0085\n",
      "Epoch 6/11\n",
      "1563/1563 [==============================] - 1s 567us/step - loss: 0.0047 - val_loss: 0.0074\n",
      "Epoch 7/11\n",
      "1563/1563 [==============================] - 1s 579us/step - loss: 0.0035 - val_loss: 0.0016\n",
      "Epoch 8/11\n",
      "1563/1563 [==============================] - 1s 565us/step - loss: 0.0043 - val_loss: 0.0017\n",
      "Epoch 9/11\n",
      "1563/1563 [==============================] - 1s 574us/step - loss: 0.0019 - val_loss: 0.0017\n",
      "Epoch 10/11\n",
      "1563/1563 [==============================] - 1s 573us/step - loss: 0.0041 - val_loss: 0.0013\n",
      "Epoch 11/11\n",
      "1563/1563 [==============================] - 1s 593us/step - loss: 0.0024 - val_loss: 0.0499\n",
      "Epoch 1/11\n",
      "3125/3125 - 1s - loss: 0.0041 - 1s/epoch - 450us/step\n",
      "Epoch 2/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "3125/3125 - 1s - loss: 0.0024 - 1s/epoch - 454us/step\n",
      "Epoch 4/11\n",
      "3125/3125 - 1s - loss: 0.0031 - 1s/epoch - 445us/step\n",
      "Epoch 5/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 6/11\n",
      "3125/3125 - 1s - loss: 0.0025 - 1s/epoch - 447us/step\n",
      "Epoch 7/11\n",
      "3125/3125 - 1s - loss: 0.0030 - 1s/epoch - 452us/step\n",
      "Epoch 8/11\n",
      "3125/3125 - 1s - loss: 0.0022 - 1s/epoch - 447us/step\n",
      "Epoch 9/11\n",
      "3125/3125 - 1s - loss: 0.0035 - 1s/epoch - 450us/step\n",
      "Epoch 10/11\n",
      "3125/3125 - 1s - loss: 0.0032 - 1s/epoch - 448us/step\n",
      "Epoch 11/11\n",
      "3125/3125 - 1s - loss: 0.0019 - 1s/epoch - 451us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;,\n",
       "                    param_grid={&#x27;learning_rate&#x27;: [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32572bb20>,\n",
       "                    param_grid={'learning_rate': [0.001, 0.01, 0.1]},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search learning rates\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', learning_rate=0.001, kernel_regularizer='l2', batch_size=64):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'learning_rate': 0.01}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0052 - val_loss: 0.0031\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 629us/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 7.4837e-04 - val_loss: 2.9767e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 7.7995e-04 - val_loss: 4.0676e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 6.9497e-04 - val_loss: 0.0013\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 4.3383e-04 - val_loss: 2.5998e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.0091e-04 - val_loss: 2.9744e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.1961e-04 - val_loss: 2.0497e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 4.5444e-04 - val_loss: 6.9645e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 2.9007e-04 - val_loss: 2.3507e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 3.3412e-04 - val_loss: 4.4008e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.01), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.00033411901677027345\n",
      "Best Validation Huber Loss: 0.0004400824836920947\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")\n",
    "\n",
    "plt.plot(best_model.history.history['loss'], label='Training Huber Loss')\n",
    "plt.plot(best_model.history.history['val_loss'], label='Validation Huber Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed PD Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.557262</td>\n",
       "      <td>1.527546</td>\n",
       "      <td>-1.589951</td>\n",
       "      <td>0.003948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.537905</td>\n",
       "      <td>1.506958</td>\n",
       "      <td>-1.590062</td>\n",
       "      <td>0.006862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.507210</td>\n",
       "      <td>1.486314</td>\n",
       "      <td>-1.610694</td>\n",
       "      <td>0.010916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.488253</td>\n",
       "      <td>1.465624</td>\n",
       "      <td>-1.609903</td>\n",
       "      <td>0.013926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.457963</td>\n",
       "      <td>1.444737</td>\n",
       "      <td>-1.629384</td>\n",
       "      <td>0.018048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106692</th>\n",
       "      <td>1.451553</td>\n",
       "      <td>0.063591</td>\n",
       "      <td>2.530965</td>\n",
       "      <td>0.200332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106693</th>\n",
       "      <td>1.476479</td>\n",
       "      <td>0.096361</td>\n",
       "      <td>2.520442</td>\n",
       "      <td>0.153474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106694</th>\n",
       "      <td>1.500933</td>\n",
       "      <td>0.128907</td>\n",
       "      <td>2.509442</td>\n",
       "      <td>0.108536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106695</th>\n",
       "      <td>1.512750</td>\n",
       "      <td>0.161256</td>\n",
       "      <td>2.475801</td>\n",
       "      <td>0.061864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106696</th>\n",
       "      <td>1.536221</td>\n",
       "      <td>0.193442</td>\n",
       "      <td>2.463626</td>\n",
       "      <td>0.001963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106697 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0             0.557262                 1.527546                    -1.589951   \n",
       "1             0.537905                 1.506958                    -1.590062   \n",
       "2             0.507210                 1.486314                    -1.610694   \n",
       "3             0.488253                 1.465624                    -1.609903   \n",
       "4             0.457963                 1.444737                    -1.629384   \n",
       "...                ...                      ...                          ...   \n",
       "106692        1.451553                 0.063591                     2.530965   \n",
       "106693        1.476479                 0.096361                     2.520442   \n",
       "106694        1.500933                 0.128907                     2.509442   \n",
       "106695        1.512750                 0.161256                     2.475801   \n",
       "106696        1.536221                 0.193442                     2.463626   \n",
       "\n",
       "          reward  \n",
       "0       0.003948  \n",
       "1       0.006862  \n",
       "2       0.010916  \n",
       "3       0.013926  \n",
       "4       0.018048  \n",
       "...          ...  \n",
       "106692  0.200332  \n",
       "106693  0.153474  \n",
       "106694  0.108536  \n",
       "106695  0.061864  \n",
       "106696  0.001963  \n",
       "\n",
       "[106697 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/3263550007.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"processed_data_pd.csv\"\n",
    "df = pd.read_csv(f\"train_data/{filename}\")\n",
    "display(df)\n",
    "\n",
    "# split into input and target features\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "# split into training and validation sets\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 9.9188e-04 - val_loss: 1.3656e-06\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 1s 531us/step - loss: 1.2388e-05 - val_loss: 2.1429e-06\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 1s 533us/step - loss: 1.4492e-05 - val_loss: 1.2533e-06\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 1s 533us/step - loss: 6.4045e-06 - val_loss: 4.4246e-06\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 1s 555us/step - loss: 5.4809e-06 - val_loss: 3.8747e-07\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 552us/step - loss: 1.0350e-05 - val_loss: 7.7341e-07\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 1s 532us/step - loss: 3.0779e-06 - val_loss: 8.2746e-07\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 532us/step - loss: 4.0780e-06 - val_loss: 3.2134e-06\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 545us/step - loss: 4.3451e-06 - val_loss: 7.6010e-07\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 1s 528us/step - loss: 1.7232e-06 - val_loss: 1.5008e-06\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 1s 544us/step - loss: 3.5479e-06 - val_loss: 3.4469e-07\n",
      "Training Huber Loss: 3.547896767486236e-06\n",
      "Validation Huber Loss: 3.446943708240724e-07\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# save model\n",
    "base_model.save(\"models/base_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/base_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0254 - val_loss: 0.0050\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 0.0042 - val_loss: 0.0036\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0033 - val_loss: 0.0032\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 1s 559us/step - loss: 0.0028 - val_loss: 0.0027\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 2s 611us/step - loss: 0.0025 - val_loss: 0.0024\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 551us/step - loss: 0.0023 - val_loss: 0.0022\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 1s 560us/step - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 561us/step - loss: 0.0019 - val_loss: 0.0018\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 2s 576us/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0017 - val_loss: 0.0017\n"
     ]
    }
   ],
   "source": [
    "# evaluate regularised model\n",
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Huber Loss: 0.0017054348718374968\n",
      "Validation Huber Loss: 0.001719768508337438\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "base_model.save(\"models/reg_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history\n",
    "with open('models/reg_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/2487668887.py:28: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2136 - val_loss: 0.0134\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0111 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0069 - val_loss: 0.0064\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0058 - val_loss: 0.0055\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0053 - val_loss: 0.0052\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0049 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0039 - val_loss: 0.0039 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0039 - 567ms/epoch - 532us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 565ms/epoch - 530us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 565ms/epoch - 529us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0036 - 565ms/epoch - 529us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 565ms/epoch - 529us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 610ms/epoch - 571us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 564ms/epoch - 529us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 566ms/epoch - 531us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 562ms/epoch - 526us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0033 - 564ms/epoch - 528us/step\n",
      "267/267 - 0s - loss: 0.0034 - 122ms/epoch - 458us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  27.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2226 - val_loss: 0.0151\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0113 - val_loss: 0.0085\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0074 - val_loss: 0.0064\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0037 - val_loss: 0.0037\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0036 - val_loss: 0.0035 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 569ms/epoch - 533us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0036 - 566ms/epoch - 530us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 564ms/epoch - 528us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 564ms/epoch - 528us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0033 - 566ms/epoch - 530us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0033 - 566ms/epoch - 531us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0032 - 568ms/epoch - 532us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0033 - 577ms/epoch - 541us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0032 - 577ms/epoch - 541us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0031 - 565ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0032 - 122ms/epoch - 457us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  27.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2264 - val_loss: 0.0153\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0123 - val_loss: 0.0100\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0087 - val_loss: 0.0075\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0070 - val_loss: 0.0062\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0060 - val_loss: 0.0055\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0046 - val_loss: 0.0045\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0043 - val_loss: 0.0043\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0039 - val_loss: 0.0037 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0037 - 565ms/epoch - 529us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 636ms/epoch - 596us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0036 - 569ms/epoch - 533us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0034 - 570ms/epoch - 535us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0036 - 564ms/epoch - 529us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 561ms/epoch - 525us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 563ms/epoch - 527us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 561ms/epoch - 525us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0034 - 578ms/epoch - 542us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0032 - 599ms/epoch - 561us/step\n",
      "267/267 - 0s - loss: 0.0033 - 137ms/epoch - 513us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  27.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2207 - val_loss: 0.0136\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0098 - val_loss: 0.0077\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0069 - val_loss: 0.0060\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0058 - val_loss: 0.0053\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0044 - val_loss: 0.0041\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0035 - val_loss: 0.0034 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 567ms/epoch - 531us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 564ms/epoch - 528us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0032 - 563ms/epoch - 528us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0034 - 562ms/epoch - 526us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0031 - 566ms/epoch - 530us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0030 - 565ms/epoch - 530us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0030 - 562ms/epoch - 527us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0030 - 566ms/epoch - 531us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0031 - 582ms/epoch - 545us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 566ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0030 - 122ms/epoch - 458us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  27.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2185 - val_loss: 0.0144\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0116 - val_loss: 0.0098\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0088 - val_loss: 0.0078\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0074 - val_loss: 0.0068\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 662us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0057 - val_loss: 0.0056\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0053 - val_loss: 0.0050\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0049 - val_loss: 0.0048\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0041 - val_loss: 0.0041 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0040 - val_loss: 0.0039 - 568ms/epoch - 533us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0039 - val_loss: 0.0040 - 571ms/epoch - 535us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0038 - 569ms/epoch - 533us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0039 - 568ms/epoch - 532us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0037 - 570ms/epoch - 534us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0037 - 568ms/epoch - 532us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0035 - 567ms/epoch - 531us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0036 - 571ms/epoch - 535us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 569ms/epoch - 534us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 565ms/epoch - 530us/step\n",
      "267/267 - 0s - loss: 0.0035 - 121ms/epoch - 452us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  27.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2569 - val_loss: 0.2233\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.2682 - val_loss: 0.2390\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.2423 - val_loss: 0.2546\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.2418 - val_loss: 0.2163\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.2991 - val_loss: 0.2351\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.2585 - val_loss: 0.2706\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.2564 - val_loss: 0.2242\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.2657 - val_loss: 0.2466\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.2543 - val_loss: 0.2226\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.2231 - val_loss: 0.2451\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.2839 - val_loss: 0.2628\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.2408 - val_loss: 0.2230 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.2475 - val_loss: 0.2263 - 560ms/epoch - 525us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.2319 - val_loss: 0.2316 - 566ms/epoch - 531us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.2645 - val_loss: 0.2766 - 561ms/epoch - 526us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.2409 - val_loss: 0.2482 - 562ms/epoch - 527us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.2440 - val_loss: 0.2320 - 558ms/epoch - 523us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.2364 - val_loss: 0.2207 - 564ms/epoch - 528us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.2379 - val_loss: 0.2188 - 564ms/epoch - 529us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.2416 - val_loss: 0.2230 - 562ms/epoch - 527us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.2636 - val_loss: 0.2572 - 564ms/epoch - 529us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.2691 - val_loss: 0.2310 - 561ms/epoch - 526us/step\n",
      "267/267 - 0s - loss: 0.2325 - 122ms/epoch - 455us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  27.3s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2549 - val_loss: 0.2723\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.2458 - val_loss: 0.2315\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.2547 - val_loss: 0.2891\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.2676 - val_loss: 0.2529\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.2371 - val_loss: 0.2255\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.2535 - val_loss: 0.2187\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.2503 - val_loss: 0.2262\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.2713 - val_loss: 0.2152\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.2394 - val_loss: 0.2287\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.2525 - val_loss: 0.2389\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.2547 - val_loss: 0.2671\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.3033 - val_loss: 0.2623 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.2714 - val_loss: 0.2641 - 574ms/epoch - 538us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.2587 - val_loss: 0.2164 - 575ms/epoch - 539us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.2378 - val_loss: 0.2256 - 567ms/epoch - 531us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.3109 - val_loss: 0.2796 - 569ms/epoch - 534us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.2537 - val_loss: 0.2828 - 572ms/epoch - 536us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.2394 - val_loss: 0.2312 - 573ms/epoch - 537us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.2483 - val_loss: 0.2551 - 574ms/epoch - 538us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.2406 - val_loss: 0.2535 - 570ms/epoch - 534us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.2525 - val_loss: 0.2264 - 571ms/epoch - 535us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.2318 - val_loss: 0.2209 - 574ms/epoch - 538us/step\n",
      "267/267 - 0s - loss: 0.2221 - 124ms/epoch - 466us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  27.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2500 - val_loss: 0.2293\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.2561 - val_loss: 0.2522\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.2390 - val_loss: 0.2253\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.2468 - val_loss: 0.2172\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.2657 - val_loss: 0.2602\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.2799 - val_loss: 0.2347\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.2397 - val_loss: 0.2471\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.2523 - val_loss: 0.2430\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.2233 - val_loss: 0.2190\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.3207 - val_loss: 0.2552\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.2607 - val_loss: 0.2549\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.2366 - val_loss: 0.2252 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.2327 - val_loss: 0.2457 - 570ms/epoch - 534us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.2563 - val_loss: 0.2176 - 565ms/epoch - 529us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.2452 - val_loss: 0.2155 - 571ms/epoch - 535us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.2700 - val_loss: 0.2361 - 568ms/epoch - 533us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.2387 - val_loss: 0.2300 - 568ms/epoch - 533us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.2448 - val_loss: 0.9577 - 563ms/epoch - 527us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.2632 - val_loss: 0.2226 - 566ms/epoch - 530us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.2362 - val_loss: 0.2247 - 566ms/epoch - 531us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.2735 - val_loss: 0.2499 - 564ms/epoch - 529us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.2208 - val_loss: 0.2257 - 563ms/epoch - 527us/step\n",
      "267/267 - 0s - loss: 0.2342 - 121ms/epoch - 452us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  27.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2534 - val_loss: 0.2419\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.2528 - val_loss: 0.2093\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.2503 - val_loss: 0.2170\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.2372 - val_loss: 0.2139\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.2339 - val_loss: 0.2171\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.2613 - val_loss: 0.5819\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.2446 - val_loss: 0.2209\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.2793 - val_loss: 0.2702\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.2568 - val_loss: 0.2308\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.2397 - val_loss: 0.6803\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.2684 - val_loss: 0.2413\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.2421 - val_loss: 0.2208 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.2697 - val_loss: 0.2606 - 586ms/epoch - 549us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.2699 - val_loss: 0.2610 - 570ms/epoch - 534us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.2466 - val_loss: 0.2319 - 569ms/epoch - 534us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.2319 - val_loss: 0.2180 - 567ms/epoch - 531us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.2494 - val_loss: 0.2499 - 563ms/epoch - 528us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.2605 - val_loss: 0.2343 - 586ms/epoch - 549us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.2353 - val_loss: 0.2399 - 567ms/epoch - 531us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.3019 - val_loss: 0.2628 - 583ms/epoch - 546us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.2517 - val_loss: 0.2423 - 564ms/epoch - 529us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.2539 - val_loss: 0.2479 - 568ms/epoch - 532us/step\n",
      "267/267 - 0s - loss: 0.2564 - 120ms/epoch - 449us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  27.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2734 - val_loss: 0.2569\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.2389 - val_loss: 0.2386\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.2848 - val_loss: 0.2500\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.2278 - val_loss: 0.2344\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.3082 - val_loss: 0.2867\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.2447 - val_loss: 0.3012\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.2367 - val_loss: 0.2687\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.2550 - val_loss: 0.3281\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.2519 - val_loss: 0.3451\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.2436 - val_loss: 0.2146\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.2908 - val_loss: 0.2536\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.2277 - val_loss: 0.2228 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.2521 - val_loss: 0.3060 - 574ms/epoch - 538us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.2673 - val_loss: 0.2640 - 568ms/epoch - 532us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.2584 - val_loss: 0.2705 - 570ms/epoch - 535us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.2392 - val_loss: 0.2304 - 569ms/epoch - 533us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.2440 - val_loss: 0.2536 - 568ms/epoch - 532us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.3726 - val_loss: 0.2514 - 569ms/epoch - 533us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.2651 - val_loss: 0.2612 - 565ms/epoch - 530us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.2483 - val_loss: 0.2295 - 585ms/epoch - 548us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.2401 - val_loss: 0.2478 - 565ms/epoch - 530us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.2860 - val_loss: 0.2616 - 571ms/epoch - 535us/step\n",
      "267/267 - 0s - loss: 0.2673 - 120ms/epoch - 451us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  27.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.5142 - val_loss: 0.6241\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 454us/step - loss: 0.2345 - val_loss: 0.0622\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.0459 - val_loss: 0.0364\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0310 - val_loss: 0.0270\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 516us/step - loss: 0.0244 - val_loss: 0.0222\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 463us/step - loss: 0.0203 - val_loss: 0.0187\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0175 - val_loss: 0.0166\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 477us/step - loss: 0.0158 - val_loss: 0.0151\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0145 - val_loss: 0.0140\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 461us/step - loss: 0.0133 - val_loss: 0.0127\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.0122 - val_loss: 0.0117\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0114 - val_loss: 0.0110 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0107 - val_loss: 0.0104 - 462ms/epoch - 433us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0101 - val_loss: 0.0098 - 458ms/epoch - 430us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0096 - val_loss: 0.0094 - 457ms/epoch - 429us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0092 - val_loss: 0.0090 - 456ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0089 - val_loss: 0.0086 - 456ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0086 - val_loss: 0.0084 - 457ms/epoch - 429us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0083 - val_loss: 0.0082 - 457ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0080 - val_loss: 0.0078 - 456ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0078 - val_loss: 0.0076 - 454ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0076 - val_loss: 0.0074 - 461ms/epoch - 432us/step\n",
      "267/267 - 0s - loss: 0.0077 - 122ms/epoch - 456us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  24.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4937 - val_loss: 0.6041\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.2239 - val_loss: 0.0634\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 443us/step - loss: 0.0492 - val_loss: 0.0418\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 444us/step - loss: 0.0358 - val_loss: 0.0332\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 466us/step - loss: 0.0294 - val_loss: 0.0279\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 444us/step - loss: 0.0249 - val_loss: 0.0239\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 449us/step - loss: 0.0215 - val_loss: 0.0208\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 451us/step - loss: 0.0188 - val_loss: 0.0181\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0165 - val_loss: 0.0161\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 440us/step - loss: 0.0148 - val_loss: 0.0144\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0132 - val_loss: 0.0131\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0122 - val_loss: 0.0123 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0116 - val_loss: 0.0117 - 452ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0110 - val_loss: 0.0111 - 456ms/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0105 - val_loss: 0.0106 - 453ms/epoch - 424us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0101 - val_loss: 0.0102 - 456ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0097 - val_loss: 0.0099 - 452ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0094 - val_loss: 0.0095 - 456ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0091 - val_loss: 0.0092 - 472ms/epoch - 442us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0089 - val_loss: 0.0090 - 456ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0086 - val_loss: 0.0087 - 455ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0084 - val_loss: 0.0085 - 454ms/epoch - 425us/step\n",
      "267/267 - 0s - loss: 0.0087 - 121ms/epoch - 451us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4948 - val_loss: 0.5963\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 448us/step - loss: 0.2203 - val_loss: 0.0585\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 440us/step - loss: 0.0446 - val_loss: 0.0377\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0325 - val_loss: 0.0293\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 473us/step - loss: 0.0257 - val_loss: 0.0237\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0213 - val_loss: 0.0199\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0185 - val_loss: 0.0177\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 457us/step - loss: 0.0168 - val_loss: 0.0163\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.0155 - val_loss: 0.0151\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0135 - val_loss: 0.0133\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0128 - val_loss: 0.0127 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0122 - val_loss: 0.0121 - 455ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0116 - val_loss: 0.0115 - 454ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0111 - val_loss: 0.0110 - 452ms/epoch - 424us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0107 - val_loss: 0.0106 - 453ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0103 - val_loss: 0.0103 - 451ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0099 - val_loss: 0.0099 - 455ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0096 - val_loss: 0.0097 - 453ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0093 - val_loss: 0.0093 - 470ms/epoch - 440us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0090 - val_loss: 0.0090 - 452ms/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0087 - val_loss: 0.0087 - 452ms/epoch - 423us/step\n",
      "267/267 - 0s - loss: 0.0090 - 122ms/epoch - 457us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4796 - val_loss: 0.5924\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.2211 - val_loss: 0.0665\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 449us/step - loss: 0.0507 - val_loss: 0.0405\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0346 - val_loss: 0.0300\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 469us/step - loss: 0.0270 - val_loss: 0.0246\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0226 - val_loss: 0.0209\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 451us/step - loss: 0.0195 - val_loss: 0.0182\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 474us/step - loss: 0.0172 - val_loss: 0.0162\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 457us/step - loss: 0.0153 - val_loss: 0.0144\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0136 - val_loss: 0.0128\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0112 - val_loss: 0.0108 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0105 - val_loss: 0.0101 - 459ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0099 - val_loss: 0.0095 - 455ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0093 - val_loss: 0.0089 - 456ms/epoch - 427us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0088 - val_loss: 0.0085 - 456ms/epoch - 428us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0084 - val_loss: 0.0081 - 456ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0080 - val_loss: 0.0077 - 456ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0077 - val_loss: 0.0074 - 470ms/epoch - 441us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0074 - val_loss: 0.0072 - 455ms/epoch - 426us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0072 - val_loss: 0.0070 - 453ms/epoch - 425us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0070 - val_loss: 0.0069 - 455ms/epoch - 426us/step\n",
      "267/267 - 0s - loss: 0.0070 - 122ms/epoch - 457us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4918 - val_loss: 0.5971\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 463us/step - loss: 0.2205 - val_loss: 0.0602\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 454us/step - loss: 0.0452 - val_loss: 0.0364\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0311 - val_loss: 0.0279\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 470us/step - loss: 0.0250 - val_loss: 0.0233\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.0214 - val_loss: 0.0202\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0185 - val_loss: 0.0177\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0165 - val_loss: 0.0161\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.0154 - val_loss: 0.0151\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 458us/step - loss: 0.0144 - val_loss: 0.0143\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 470us/step - loss: 0.0137 - val_loss: 0.0135\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0131 - val_loss: 0.0130 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0126 - val_loss: 0.0126 - 457ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0122 - val_loss: 0.0121 - 453ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0117 - val_loss: 0.0117 - 456ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0114 - val_loss: 0.0113 - 458ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0110 - val_loss: 0.0110 - 456ms/epoch - 427us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0106 - val_loss: 0.0107 - 456ms/epoch - 428us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0103 - val_loss: 0.0103 - 457ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0100 - val_loss: 0.0100 - 456ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0097 - val_loss: 0.0097 - 455ms/epoch - 427us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0093 - val_loss: 0.0093 - 456ms/epoch - 427us/step\n",
      "267/267 - 0s - loss: 0.0096 - 121ms/epoch - 452us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  24.4s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "5335/5335 [==============================] - 4s 797us/step - loss: 0.1101 - val_loss: 0.0585\n",
      "Epoch 2/11\n",
      "5335/5335 [==============================] - 3s 541us/step - loss: 0.0735 - val_loss: 0.1431\n",
      "Epoch 3/11\n",
      "5335/5335 [==============================] - 3s 547us/step - loss: 0.0805 - val_loss: 0.0422\n",
      "Epoch 4/11\n",
      "5335/5335 [==============================] - 3s 550us/step - loss: 0.0639 - val_loss: 0.0825\n",
      "Epoch 5/11\n",
      "5335/5335 [==============================] - 3s 545us/step - loss: 0.0742 - val_loss: 0.1414\n",
      "Epoch 6/11\n",
      "5335/5335 [==============================] - 3s 553us/step - loss: 0.0683 - val_loss: 0.0235\n",
      "Epoch 7/11\n",
      "5335/5335 [==============================] - 3s 548us/step - loss: 0.0705 - val_loss: 0.0941\n",
      "Epoch 8/11\n",
      "5335/5335 [==============================] - 3s 549us/step - loss: 0.0677 - val_loss: 0.1525\n",
      "Epoch 9/11\n",
      "5335/5335 [==============================] - 3s 546us/step - loss: 0.0698 - val_loss: 0.0609\n",
      "Epoch 10/11\n",
      "5335/5335 [==============================] - 3s 544us/step - loss: 0.0699 - val_loss: 0.0838\n",
      "Epoch 11/11\n",
      "5335/5335 [==============================] - 3s 548us/step - loss: 0.0698 - val_loss: 0.0450\n",
      "Epoch 1/11\n",
      "4268/4268 - 3s - loss: 0.0637 - val_loss: 0.0929 - 3s/epoch - 814us/step\n",
      "Epoch 2/11\n",
      "4268/4268 - 2s - loss: 0.0735 - val_loss: 0.0636 - 2s/epoch - 506us/step\n",
      "Epoch 3/11\n",
      "4268/4268 - 2s - loss: 0.0656 - val_loss: 0.0292 - 2s/epoch - 502us/step\n",
      "Epoch 4/11\n",
      "4268/4268 - 2s - loss: 0.0847 - val_loss: 0.0291 - 2s/epoch - 505us/step\n",
      "Epoch 5/11\n",
      "4268/4268 - 2s - loss: 0.0805 - val_loss: 0.0415 - 2s/epoch - 505us/step\n",
      "Epoch 6/11\n",
      "4268/4268 - 2s - loss: 0.0688 - val_loss: 0.0283 - 2s/epoch - 506us/step\n",
      "Epoch 7/11\n",
      "4268/4268 - 2s - loss: 0.0740 - val_loss: 0.0239 - 2s/epoch - 501us/step\n",
      "Epoch 8/11\n",
      "4268/4268 - 2s - loss: 0.0715 - val_loss: 0.0999 - 2s/epoch - 506us/step\n",
      "Epoch 9/11\n",
      "4268/4268 - 2s - loss: 0.0772 - val_loss: 0.0197 - 2s/epoch - 503us/step\n",
      "Epoch 10/11\n",
      "4268/4268 - 2s - loss: 0.0829 - val_loss: 0.0487 - 2s/epoch - 509us/step\n",
      "Epoch 11/11\n",
      "4268/4268 - 2s - loss: 0.0706 - val_loss: 0.0365 - 2s/epoch - 502us/step\n",
      "1067/1067 - 0s - loss: 0.0475 - 316ms/epoch - 296us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.1min\n",
      "Epoch 1/11\n",
      "5335/5335 [==============================] - 5s 843us/step - loss: 0.0504 - val_loss: 0.0215\n",
      "Epoch 2/11\n",
      "5335/5335 [==============================] - 3s 543us/step - loss: 0.0541 - val_loss: 0.0925\n",
      "Epoch 3/11\n",
      "5335/5335 [==============================] - 3s 568us/step - loss: 0.0352 - val_loss: 0.0296\n",
      "Epoch 4/11\n",
      "5335/5335 [==============================] - 3s 571us/step - loss: 0.0445 - val_loss: 0.0362\n",
      "Epoch 5/11\n",
      "5335/5335 [==============================] - 3s 546us/step - loss: 0.0374 - val_loss: 0.0938\n",
      "Epoch 6/11\n",
      "5335/5335 [==============================] - 3s 542us/step - loss: 0.0367 - val_loss: 0.0245\n",
      "Epoch 7/11\n",
      "5335/5335 [==============================] - 3s 547us/step - loss: 0.0406 - val_loss: 0.2174\n",
      "Epoch 8/11\n",
      "5335/5335 [==============================] - 3s 541us/step - loss: 0.0372 - val_loss: 0.0393\n",
      "Epoch 9/11\n",
      "5335/5335 [==============================] - 3s 548us/step - loss: 0.0439 - val_loss: 0.0279\n",
      "Epoch 10/11\n",
      "5335/5335 [==============================] - 3s 544us/step - loss: 0.0420 - val_loss: 0.1127\n",
      "Epoch 11/11\n",
      "5335/5335 [==============================] - 3s 543us/step - loss: 0.0393 - val_loss: 0.0540\n",
      "Epoch 1/11\n",
      "4268/4268 - 4s - loss: 0.0486 - val_loss: 0.0462 - 4s/epoch - 820us/step\n",
      "Epoch 2/11\n",
      "4268/4268 - 2s - loss: 0.0442 - val_loss: 0.0172 - 2s/epoch - 500us/step\n",
      "Epoch 3/11\n",
      "4268/4268 - 2s - loss: 0.0345 - val_loss: 0.0310 - 2s/epoch - 503us/step\n",
      "Epoch 4/11\n",
      "4268/4268 - 2s - loss: 0.0374 - val_loss: 0.0372 - 2s/epoch - 517us/step\n",
      "Epoch 5/11\n",
      "4268/4268 - 2s - loss: 0.0684 - val_loss: 0.0377 - 2s/epoch - 505us/step\n",
      "Epoch 6/11\n",
      "4268/4268 - 2s - loss: 0.0550 - val_loss: 0.0326 - 2s/epoch - 499us/step\n",
      "Epoch 7/11\n",
      "4268/4268 - 2s - loss: 0.0487 - val_loss: 0.1160 - 2s/epoch - 503us/step\n",
      "Epoch 8/11\n",
      "4268/4268 - 2s - loss: 0.0675 - val_loss: 0.0700 - 2s/epoch - 501us/step\n",
      "Epoch 9/11\n",
      "4268/4268 - 2s - loss: 0.0679 - val_loss: 0.0520 - 2s/epoch - 503us/step\n",
      "Epoch 10/11\n",
      "4268/4268 - 2s - loss: 0.0721 - val_loss: 0.0478 - 2s/epoch - 497us/step\n",
      "Epoch 11/11\n",
      "4268/4268 - 2s - loss: 0.0674 - val_loss: 0.0298 - 2s/epoch - 503us/step\n",
      "1067/1067 - 0s - loss: 0.0397 - 316ms/epoch - 296us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.1min\n",
      "Epoch 1/11\n",
      "5335/5335 [==============================] - 4s 808us/step - loss: 0.0467 - val_loss: 0.0996\n",
      "Epoch 2/11\n",
      "5335/5335 [==============================] - 3s 544us/step - loss: 0.0602 - val_loss: 0.1301\n",
      "Epoch 3/11\n",
      "5335/5335 [==============================] - 3s 546us/step - loss: 0.0472 - val_loss: 0.0285\n",
      "Epoch 4/11\n",
      "5335/5335 [==============================] - 3s 542us/step - loss: 0.0527 - val_loss: 0.0392\n",
      "Epoch 5/11\n",
      "5335/5335 [==============================] - 3s 545us/step - loss: 0.0364 - val_loss: 0.0281\n",
      "Epoch 6/11\n",
      "5335/5335 [==============================] - 3s 546us/step - loss: 0.0452 - val_loss: 0.0412\n",
      "Epoch 7/11\n",
      "5335/5335 [==============================] - 3s 548us/step - loss: 0.0401 - val_loss: 0.0392\n",
      "Epoch 8/11\n",
      "5335/5335 [==============================] - 3s 543us/step - loss: 0.0444 - val_loss: 0.0134\n",
      "Epoch 9/11\n",
      "5335/5335 [==============================] - 3s 549us/step - loss: 0.0419 - val_loss: 0.1143\n",
      "Epoch 10/11\n",
      "5335/5335 [==============================] - 3s 543us/step - loss: 0.0412 - val_loss: 0.0146\n",
      "Epoch 11/11\n",
      "5335/5335 [==============================] - 3s 548us/step - loss: 0.0480 - val_loss: 0.0148\n",
      "Epoch 1/11\n",
      "4268/4268 - 3s - loss: 0.0441 - val_loss: 0.0407 - 3s/epoch - 809us/step\n",
      "Epoch 2/11\n",
      "4268/4268 - 2s - loss: 0.0409 - val_loss: 0.0228 - 2s/epoch - 502us/step\n",
      "Epoch 3/11\n",
      "4268/4268 - 2s - loss: 0.0487 - val_loss: 0.0545 - 2s/epoch - 499us/step\n",
      "Epoch 4/11\n",
      "4268/4268 - 2s - loss: 0.0470 - val_loss: 0.0205 - 2s/epoch - 502us/step\n",
      "Epoch 5/11\n",
      "4268/4268 - 2s - loss: 0.0439 - val_loss: 0.0297 - 2s/epoch - 498us/step\n",
      "Epoch 6/11\n",
      "4268/4268 - 2s - loss: 0.0385 - val_loss: 0.0250 - 2s/epoch - 500us/step\n",
      "Epoch 7/11\n",
      "4268/4268 - 2s - loss: 0.0691 - val_loss: 0.0631 - 2s/epoch - 498us/step\n",
      "Epoch 8/11\n",
      "4268/4268 - 2s - loss: 0.0530 - val_loss: 0.0137 - 2s/epoch - 503us/step\n",
      "Epoch 9/11\n",
      "4268/4268 - 2s - loss: 0.0445 - val_loss: 0.0894 - 2s/epoch - 499us/step\n",
      "Epoch 10/11\n",
      "4268/4268 - 2s - loss: 0.0568 - val_loss: 0.1414 - 2s/epoch - 503us/step\n",
      "Epoch 11/11\n",
      "4268/4268 - 2s - loss: 0.0541 - val_loss: 0.0178 - 2s/epoch - 515us/step\n",
      "1067/1067 - 0s - loss: 0.0247 - 322ms/epoch - 302us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.1min\n",
      "Epoch 1/11\n",
      "5335/5335 [==============================] - 4s 807us/step - loss: 0.0526 - val_loss: 0.0384\n",
      "Epoch 2/11\n",
      "5335/5335 [==============================] - 3s 534us/step - loss: 0.0447 - val_loss: 0.0815\n",
      "Epoch 3/11\n",
      "5335/5335 [==============================] - 3s 611us/step - loss: 0.0381 - val_loss: 0.0123\n",
      "Epoch 4/11\n",
      "5335/5335 [==============================] - 3s 539us/step - loss: 0.0404 - val_loss: 0.0334\n",
      "Epoch 5/11\n",
      "5335/5335 [==============================] - 3s 545us/step - loss: 0.0383 - val_loss: 0.0407\n",
      "Epoch 6/11\n",
      "5335/5335 [==============================] - 3s 554us/step - loss: 0.0429 - val_loss: 0.0176\n",
      "Epoch 7/11\n",
      "5335/5335 [==============================] - 3s 564us/step - loss: 0.0382 - val_loss: 0.0613\n",
      "Epoch 8/11\n",
      "5335/5335 [==============================] - 3s 554us/step - loss: 0.0387 - val_loss: 0.0108\n",
      "Epoch 9/11\n",
      "5335/5335 [==============================] - 3s 552us/step - loss: 0.0381 - val_loss: 0.0745\n",
      "Epoch 10/11\n",
      "5335/5335 [==============================] - 3s 578us/step - loss: 0.0355 - val_loss: 0.0413\n",
      "Epoch 11/11\n",
      "5335/5335 [==============================] - 3s 551us/step - loss: 0.0430 - val_loss: 0.0399\n",
      "Epoch 1/11\n",
      "4268/4268 - 3s - loss: 0.0429 - val_loss: 0.0082 - 3s/epoch - 814us/step\n",
      "Epoch 2/11\n",
      "4268/4268 - 2s - loss: 0.0453 - val_loss: 0.0142 - 2s/epoch - 532us/step\n",
      "Epoch 3/11\n",
      "4268/4268 - 2s - loss: 0.0361 - val_loss: 0.0439 - 2s/epoch - 504us/step\n",
      "Epoch 4/11\n",
      "4268/4268 - 2s - loss: 0.0369 - val_loss: 0.0166 - 2s/epoch - 512us/step\n",
      "Epoch 5/11\n",
      "4268/4268 - 2s - loss: 0.0359 - val_loss: 0.0267 - 2s/epoch - 506us/step\n",
      "Epoch 6/11\n",
      "4268/4268 - 2s - loss: 0.0361 - val_loss: 0.0339 - 2s/epoch - 514us/step\n",
      "Epoch 7/11\n",
      "4268/4268 - 2s - loss: 0.0367 - val_loss: 0.0238 - 2s/epoch - 508us/step\n",
      "Epoch 8/11\n",
      "4268/4268 - 2s - loss: 0.0414 - val_loss: 0.0802 - 2s/epoch - 512us/step\n",
      "Epoch 9/11\n",
      "4268/4268 - 2s - loss: 0.0397 - val_loss: 0.0137 - 2s/epoch - 500us/step\n",
      "Epoch 10/11\n",
      "4268/4268 - 2s - loss: 0.0401 - val_loss: 0.0237 - 2s/epoch - 503us/step\n",
      "Epoch 11/11\n",
      "4268/4268 - 2s - loss: 0.0431 - val_loss: 0.0858 - 2s/epoch - 506us/step\n",
      "1067/1067 - 0s - loss: 0.1114 - 319ms/epoch - 299us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time= 1.6min\n",
      "Epoch 1/11\n",
      "5335/5335 [==============================] - 4s 803us/step - loss: 0.0517 - val_loss: 0.0159\n",
      "Epoch 2/11\n",
      "5335/5335 [==============================] - 3s 550us/step - loss: 0.0487 - val_loss: 0.0310\n",
      "Epoch 3/11\n",
      "5335/5335 [==============================] - 3s 555us/step - loss: 0.0456 - val_loss: 0.0440\n",
      "Epoch 4/11\n",
      "5335/5335 [==============================] - 552s 103ms/step - loss: 0.0364 - val_loss: 0.0328\n",
      "Epoch 5/11\n",
      "5335/5335 [==============================] - 4s 732us/step - loss: 0.0367 - val_loss: 0.0299\n",
      "Epoch 6/11\n",
      "5335/5335 [==============================] - 3s 617us/step - loss: 0.0384 - val_loss: 0.0202\n",
      "Epoch 7/11\n",
      "5335/5335 [==============================] - 3s 606us/step - loss: 0.0428 - val_loss: 0.0417\n",
      "Epoch 8/11\n",
      "5335/5335 [==============================] - 3s 585us/step - loss: 0.0327 - val_loss: 0.0271\n",
      "Epoch 9/11\n",
      "5335/5335 [==============================] - 3s 569us/step - loss: 0.0554 - val_loss: 0.0509\n",
      "Epoch 10/11\n",
      "5335/5335 [==============================] - 3s 614us/step - loss: 0.0514 - val_loss: 0.0539\n",
      "Epoch 11/11\n",
      "5335/5335 [==============================] - 3s 557us/step - loss: 0.0410 - val_loss: 0.0284\n",
      "Epoch 1/11\n",
      "4268/4268 - 4s - loss: 0.0518 - val_loss: 0.0488 - 4s/epoch - 890us/step\n",
      "Epoch 2/11\n",
      "4268/4268 - 2s - loss: 0.0511 - val_loss: 0.0190 - 2s/epoch - 539us/step\n",
      "Epoch 3/11\n",
      "4268/4268 - 2s - loss: 0.0525 - val_loss: 0.0535 - 2s/epoch - 544us/step\n",
      "Epoch 4/11\n",
      "4268/4268 - 2s - loss: 0.0411 - val_loss: 0.0195 - 2s/epoch - 539us/step\n",
      "Epoch 5/11\n",
      "4268/4268 - 2s - loss: 0.0418 - val_loss: 0.0158 - 2s/epoch - 517us/step\n",
      "Epoch 6/11\n",
      "4268/4268 - 2s - loss: 0.0397 - val_loss: 0.0253 - 2s/epoch - 501us/step\n",
      "Epoch 7/11\n",
      "4268/4268 - 2s - loss: 0.0413 - val_loss: 0.0123 - 2s/epoch - 503us/step\n",
      "Epoch 8/11\n",
      "4268/4268 - 2s - loss: 0.0505 - val_loss: 0.1848 - 2s/epoch - 517us/step\n",
      "Epoch 9/11\n",
      "4268/4268 - 2s - loss: 0.0486 - val_loss: 0.0577 - 2s/epoch - 511us/step\n",
      "Epoch 10/11\n",
      "4268/4268 - 2s - loss: 0.0431 - val_loss: 0.0333 - 2s/epoch - 538us/step\n",
      "Epoch 11/11\n",
      "4268/4268 - 2s - loss: 0.0503 - val_loss: 0.1565 - 2s/epoch - 512us/step\n",
      "1067/1067 - 0s - loss: 0.1973 - 320ms/epoch - 300us/step\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=10.4min\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0426 - val_loss: 0.0179\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 1s 530us/step - loss: 0.0138 - val_loss: 0.0071\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 1s 532us/step - loss: 0.0303 - val_loss: 0.0331\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 0.0187 - val_loss: 0.0326\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 1s 529us/step - loss: 0.0390 - val_loss: 0.0220\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 521us/step - loss: 0.0310 - val_loss: 0.0269\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 1s 531us/step - loss: 0.0203 - val_loss: 0.0069\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 536us/step - loss: 0.0661 - val_loss: 0.0615\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 535us/step - loss: 0.0315 - val_loss: 0.0199\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 1s 534us/step - loss: 0.0273 - val_loss: 0.0340\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 1s 560us/step - loss: 0.0190 - val_loss: 0.0106\n",
      "Epoch 1/11\n",
      "2134/2134 - 3s - loss: 0.0184 - val_loss: 0.0060 - 3s/epoch - 1ms/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 0.0202 - val_loss: 0.0052 - 1s/epoch - 512us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 0.0191 - val_loss: 0.0084 - 1s/epoch - 539us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 0.0205 - val_loss: 0.0654 - 1s/epoch - 568us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 0.0192 - val_loss: 0.0183 - 1s/epoch - 542us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 0.0233 - val_loss: 0.0086 - 1s/epoch - 526us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 0.0179 - val_loss: 0.0168 - 1s/epoch - 561us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 0.0184 - val_loss: 0.4906 - 1s/epoch - 532us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 0.0599 - val_loss: 0.0109 - 1s/epoch - 580us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 0.0198 - val_loss: 0.0166 - 1s/epoch - 575us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 0.0213 - val_loss: 0.0289 - 1s/epoch - 547us/step\n",
      "534/534 - 0s - loss: 0.0407 - 188ms/epoch - 352us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  41.4s\n",
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0547 - val_loss: 0.0323\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 1s 527us/step - loss: 0.0180 - val_loss: 0.0074\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 1s 529us/step - loss: 0.0325 - val_loss: 0.0062\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 1s 527us/step - loss: 0.0185 - val_loss: 0.0155\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 1s 527us/step - loss: 0.0297 - val_loss: 0.0276\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 527us/step - loss: 0.0211 - val_loss: 0.0153\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 0.0408 - val_loss: 0.0298\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 554us/step - loss: 0.0215 - val_loss: 0.0237\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 532us/step - loss: 0.0165 - val_loss: 0.2248\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 1s 554us/step - loss: 0.0224 - val_loss: 0.0036\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 1s 559us/step - loss: 0.0299 - val_loss: 0.0080\n",
      "Epoch 1/11\n",
      "2134/2134 - 3s - loss: 0.0200 - val_loss: 0.0127 - 3s/epoch - 1ms/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 0.0178 - val_loss: 0.0069 - 1s/epoch - 534us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 0.0200 - val_loss: 0.0125 - 1s/epoch - 508us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 0.0225 - val_loss: 0.0066 - 1s/epoch - 514us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 0.0384 - val_loss: 0.0683 - 1s/epoch - 509us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 0.0212 - val_loss: 0.0066 - 1s/epoch - 510us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 0.0174 - val_loss: 0.0136 - 1s/epoch - 528us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 0.0175 - val_loss: 0.0090 - 1s/epoch - 566us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 0.0214 - val_loss: 0.0162 - 1s/epoch - 522us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 0.0177 - val_loss: 0.2827 - 1s/epoch - 551us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 0.0370 - val_loss: 0.0077 - 1s/epoch - 564us/step\n",
      "534/534 - 0s - loss: 0.0110 - 205ms/epoch - 383us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  41.2s\n",
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0686 - val_loss: 0.0090\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 1s 543us/step - loss: 0.0181 - val_loss: 0.0063\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 1s 531us/step - loss: 0.0237 - val_loss: 0.1978\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 1s 528us/step - loss: 0.0275 - val_loss: 0.0052\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 1s 558us/step - loss: 0.0232 - val_loss: 0.0076\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 2s 599us/step - loss: 0.0198 - val_loss: 0.0110\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 2s 581us/step - loss: 0.0469 - val_loss: 0.0364\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 553us/step - loss: 0.0205 - val_loss: 0.0092\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 544us/step - loss: 0.0169 - val_loss: 0.1906\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 2s 568us/step - loss: 0.0218 - val_loss: 0.0273\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 1s 530us/step - loss: 0.0182 - val_loss: 0.0095\n",
      "Epoch 1/11\n",
      "2134/2134 - 2s - loss: 0.0183 - val_loss: 0.0065 - 2s/epoch - 1ms/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 0.0167 - val_loss: 0.0261 - 1s/epoch - 509us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 0.0348 - val_loss: 0.1114 - 1s/epoch - 515us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 0.0203 - val_loss: 0.0238 - 1s/epoch - 513us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 0.0626 - val_loss: 0.0235 - 1s/epoch - 510us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 0.0225 - val_loss: 0.0149 - 1s/epoch - 519us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 0.0183 - val_loss: 0.0427 - 1s/epoch - 507us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 0.0360 - val_loss: 0.0095 - 1s/epoch - 543us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 0.0173 - val_loss: 0.0158 - 1s/epoch - 529us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 0.0353 - val_loss: 0.1561 - 1s/epoch - 531us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 0.0206 - val_loss: 0.0081 - 1s/epoch - 521us/step\n",
      "534/534 - 0s - loss: 0.0103 - 200ms/epoch - 375us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  41.3s\n",
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0361 - val_loss: 0.0309\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 2s 569us/step - loss: 0.0266 - val_loss: 0.0061\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 2s 567us/step - loss: 0.0753 - val_loss: 0.0114\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 2s 575us/step - loss: 0.0171 - val_loss: 0.0130\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 2s 589us/step - loss: 0.0231 - val_loss: 0.0099\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 558us/step - loss: 0.0246 - val_loss: 0.0246\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 1s 546us/step - loss: 0.0206 - val_loss: 0.0368\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 554us/step - loss: 0.0296 - val_loss: 0.1196\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 2s 565us/step - loss: 0.0237 - val_loss: 0.0145\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 1s 555us/step - loss: 0.0187 - val_loss: 0.0127\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 2s 570us/step - loss: 0.0210 - val_loss: 0.0158\n",
      "Epoch 1/11\n",
      "2134/2134 - 2s - loss: 0.0284 - val_loss: 0.0327 - 2s/epoch - 1ms/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 0.0195 - val_loss: 0.0100 - 1s/epoch - 508us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 0.0173 - val_loss: 0.0125 - 1s/epoch - 506us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 0.0238 - val_loss: 0.0154 - 1s/epoch - 516us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 0.0191 - val_loss: 0.0145 - 1s/epoch - 512us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 0.0191 - val_loss: 0.0204 - 1s/epoch - 508us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 0.0204 - val_loss: 0.0054 - 1s/epoch - 520us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 0.0227 - val_loss: 0.0184 - 1s/epoch - 504us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 0.0188 - val_loss: 0.0163 - 1s/epoch - 514us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 0.0191 - val_loss: 0.0343 - 1s/epoch - 506us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 0.0257 - val_loss: 0.0090 - 1s/epoch - 544us/step\n",
      "534/534 - 0s - loss: 0.0123 - 186ms/epoch - 348us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  41.5s\n",
      "Epoch 1/11\n",
      "2668/2668 [==============================] - 3s 1ms/step - loss: 0.0343 - val_loss: 0.0215\n",
      "Epoch 2/11\n",
      "2668/2668 [==============================] - 1s 535us/step - loss: 0.0275 - val_loss: 0.0340\n",
      "Epoch 3/11\n",
      "2668/2668 [==============================] - 1s 530us/step - loss: 0.0353 - val_loss: 0.0507\n",
      "Epoch 4/11\n",
      "2668/2668 [==============================] - 1s 546us/step - loss: 0.0202 - val_loss: 0.0712\n",
      "Epoch 5/11\n",
      "2668/2668 [==============================] - 1s 554us/step - loss: 0.0268 - val_loss: 0.0055\n",
      "Epoch 6/11\n",
      "2668/2668 [==============================] - 1s 546us/step - loss: 0.0281 - val_loss: 0.0191\n",
      "Epoch 7/11\n",
      "2668/2668 [==============================] - 2s 568us/step - loss: 0.0316 - val_loss: 0.0180\n",
      "Epoch 8/11\n",
      "2668/2668 [==============================] - 1s 542us/step - loss: 0.0176 - val_loss: 0.0336\n",
      "Epoch 9/11\n",
      "2668/2668 [==============================] - 1s 539us/step - loss: 0.0261 - val_loss: 0.0543\n",
      "Epoch 10/11\n",
      "2668/2668 [==============================] - 1s 529us/step - loss: 0.0230 - val_loss: 0.0135\n",
      "Epoch 11/11\n",
      "2668/2668 [==============================] - 1s 534us/step - loss: 0.0205 - val_loss: 0.0073\n",
      "Epoch 1/11\n",
      "2134/2134 - 2s - loss: 0.0316 - val_loss: 0.0075 - 2s/epoch - 1ms/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 0.0222 - val_loss: 0.0093 - 1s/epoch - 507us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 0.0324 - val_loss: 0.0666 - 1s/epoch - 515us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 0.0228 - val_loss: 0.0476 - 1s/epoch - 507us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 0.0220 - val_loss: 0.0059 - 1s/epoch - 512us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 0.0407 - val_loss: 0.0128 - 1s/epoch - 511us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 0.0160 - val_loss: 0.0178 - 1s/epoch - 507us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 0.0270 - val_loss: 0.2265 - 1s/epoch - 515us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 0.0271 - val_loss: 0.0119 - 1s/epoch - 508us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 0.0177 - val_loss: 0.0084 - 1s/epoch - 511us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 0.0229 - val_loss: 0.0666 - 1s/epoch - 538us/step\n",
      "534/534 - 0s - loss: 0.1045 - 198ms/epoch - 371us/step\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  40.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1946 - val_loss: 0.0209\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0173 - val_loss: 0.0153\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0140 - val_loss: 0.0131\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0122 - val_loss: 0.0117\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0110 - val_loss: 0.0104\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0102 - val_loss: 0.0095\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0095 - val_loss: 0.0089\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0089 - val_loss: 0.0084\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 536us/step - loss: 0.0083 - val_loss: 0.0079\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0064 - val_loss: 0.0061 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0061 - val_loss: 0.0061 - 558ms/epoch - 523us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0059 - 558ms/epoch - 523us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0058 - 556ms/epoch - 521us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 558ms/epoch - 523us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0055 - 559ms/epoch - 523us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0054 - 559ms/epoch - 524us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0053 - val_loss: 0.0053 - 559ms/epoch - 524us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0051 - 558ms/epoch - 523us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0051 - val_loss: 0.0052 - 556ms/epoch - 521us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0050 - val_loss: 0.0049 - 578ms/epoch - 541us/step\n",
      "267/267 - 0s - loss: 0.0050 - 118ms/epoch - 440us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  26.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1892 - val_loss: 0.0209\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0180 - val_loss: 0.0163\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0144 - val_loss: 0.0141\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 538us/step - loss: 0.0124 - val_loss: 0.0119\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0111 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 535us/step - loss: 0.0100 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0090 - val_loss: 0.0090\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0076 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 531us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 533us/step - loss: 0.0062 - val_loss: 0.0060\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0059 - val_loss: 0.0058 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0058 - 572ms/epoch - 536us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 562ms/epoch - 527us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0052 - 558ms/epoch - 523us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0051 - 562ms/epoch - 527us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0051 - val_loss: 0.0051 - 558ms/epoch - 523us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0049 - val_loss: 0.0048 - 553ms/epoch - 518us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0048 - val_loss: 0.0048 - 556ms/epoch - 521us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0047 - val_loss: 0.0047 - 556ms/epoch - 521us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0047 - val_loss: 0.0047 - 557ms/epoch - 522us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0046 - val_loss: 0.0045 - 569ms/epoch - 533us/step\n",
      "267/267 - 0s - loss: 0.0046 - 123ms/epoch - 459us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  26.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1996 - val_loss: 0.0200\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0172 - val_loss: 0.0156\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0142 - val_loss: 0.0130\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0125 - val_loss: 0.0114\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0111 - val_loss: 0.0107\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0099 - val_loss: 0.0089\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0084 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0077 - val_loss: 0.0073\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0072 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0068 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0065 - val_loss: 0.0065\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0062 - val_loss: 0.0060 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0059 - val_loss: 0.0058 - 572ms/epoch - 536us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0055 - 573ms/epoch - 537us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0056 - 574ms/epoch - 538us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 577ms/epoch - 541us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0051 - 569ms/epoch - 534us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0051 - val_loss: 0.0050 - 577ms/epoch - 541us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0050 - val_loss: 0.0049 - 571ms/epoch - 535us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0049 - val_loss: 0.0048 - 573ms/epoch - 537us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0048 - val_loss: 0.0050 - 574ms/epoch - 538us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0047 - val_loss: 0.0046 - 574ms/epoch - 538us/step\n",
      "267/267 - 0s - loss: 0.0047 - 125ms/epoch - 467us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  27.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2029 - val_loss: 0.0221\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0185 - val_loss: 0.0164\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0147 - val_loss: 0.0138\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0126 - val_loss: 0.0115\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0112 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0102 - val_loss: 0.0101\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0079 - val_loss: 0.0072\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 546us/step - loss: 0.0070 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0066 - val_loss: 0.0063\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0063 - val_loss: 0.0060\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0060 - val_loss: 0.0058 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0057 - val_loss: 0.0056 - 566ms/epoch - 530us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0055 - val_loss: 0.0054 - 566ms/epoch - 530us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0052 - 572ms/epoch - 536us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0050 - 569ms/epoch - 534us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0051 - val_loss: 0.0051 - 568ms/epoch - 533us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0050 - val_loss: 0.0049 - 566ms/epoch - 531us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0048 - val_loss: 0.0050 - 569ms/epoch - 533us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0048 - val_loss: 0.0046 - 568ms/epoch - 532us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0047 - val_loss: 0.0046 - 566ms/epoch - 531us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0046 - val_loss: 0.0046 - 569ms/epoch - 533us/step\n",
      "267/267 - 0s - loss: 0.0046 - 127ms/epoch - 475us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  27.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2027 - val_loss: 0.0216\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0184 - val_loss: 0.0166\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0147 - val_loss: 0.0133\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0122 - val_loss: 0.0122\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 0.0108 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 0.0099 - val_loss: 0.0093\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0092 - val_loss: 0.0087\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 0.0086 - val_loss: 0.0080\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0078 - val_loss: 0.0072\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0068 - val_loss: 0.0064\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0060 - val_loss: 0.0061 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0058 - val_loss: 0.0055 - 562ms/epoch - 526us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0056 - val_loss: 0.0055 - 561ms/epoch - 526us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0054 - val_loss: 0.0053 - 568ms/epoch - 532us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0052 - val_loss: 0.0052 - 571ms/epoch - 535us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0051 - val_loss: 0.0050 - 639ms/epoch - 599us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0050 - val_loss: 0.0048 - 562ms/epoch - 527us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0048 - val_loss: 0.0047 - 564ms/epoch - 528us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0047 - val_loss: 0.0046 - 561ms/epoch - 526us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0046 - val_loss: 0.0046 - 561ms/epoch - 526us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0046 - val_loss: 0.0045 - 560ms/epoch - 525us/step\n",
      "267/267 - 0s - loss: 0.0045 - 119ms/epoch - 446us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1, learning_rate=0.001, optimizer=adam; total time=  26.8s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0146 - val_loss: 0.0070\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0580 - val_loss: 0.0218\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0093 - val_loss: 0.0042\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0079 - val_loss: 0.0065\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0410 - val_loss: 0.0125\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0103 - val_loss: 0.0123\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0288 - val_loss: 0.0135\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0153 - val_loss: 0.0425\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0095 - val_loss: 0.0064\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0127 - val_loss: 0.0086\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0152 - val_loss: 0.0082\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0101 - val_loss: 0.0036 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0635 - val_loss: 0.0285 - 566ms/epoch - 530us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0110 - val_loss: 0.0053 - 563ms/epoch - 528us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0118 - val_loss: 0.0040 - 563ms/epoch - 528us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0129 - val_loss: 0.0093 - 564ms/epoch - 529us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0107 - val_loss: 0.0089 - 567ms/epoch - 532us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0135 - val_loss: 0.0197 - 570ms/epoch - 534us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0152 - val_loss: 0.0093 - 562ms/epoch - 527us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0157 - val_loss: 0.0629 - 566ms/epoch - 530us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0090 - val_loss: 0.0208 - 565ms/epoch - 529us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0140 - val_loss: 0.1492 - 595ms/epoch - 558us/step\n",
      "267/267 - 0s - loss: 0.1600 - 120ms/epoch - 449us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  26.8s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0341 - val_loss: 0.0054\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0090 - val_loss: 0.0059\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0174 - val_loss: 0.0075\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 0.0074 - val_loss: 0.0107\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0512 - val_loss: 0.0065\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0087 - val_loss: 0.0097\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0694 - val_loss: 0.0272\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 0.0134 - val_loss: 0.0101\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0102 - val_loss: 0.0045\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 539us/step - loss: 0.0170 - val_loss: 0.0076\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0100 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0136 - val_loss: 0.0281 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0155 - val_loss: 0.0069 - 563ms/epoch - 528us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0113 - val_loss: 0.0046 - 558ms/epoch - 523us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0182 - val_loss: 0.0032 - 563ms/epoch - 528us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0118 - val_loss: 0.0077 - 560ms/epoch - 525us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0153 - val_loss: 0.2040 - 561ms/epoch - 525us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0156 - val_loss: 0.0058 - 560ms/epoch - 524us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0105 - val_loss: 0.0071 - 562ms/epoch - 527us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0264 - val_loss: 0.0159 - 602ms/epoch - 564us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0088 - val_loss: 0.0051 - 591ms/epoch - 554us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0149 - val_loss: 0.0084 - 586ms/epoch - 550us/step\n",
      "267/267 - 0s - loss: 0.0111 - 131ms/epoch - 489us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  26.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0185 - val_loss: 0.0108\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 549us/step - loss: 0.0405 - val_loss: 0.0067\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 0.0091 - val_loss: 0.0136\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0275 - val_loss: 0.0044\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 0.0114 - val_loss: 0.0044\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0329 - val_loss: 0.0075\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0104 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0277 - val_loss: 0.0099\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0088 - val_loss: 0.0063\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0334 - val_loss: 0.0044\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 0.0082 - val_loss: 0.0108\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0081 - val_loss: 0.0084 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0669 - val_loss: 0.0138 - 578ms/epoch - 542us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0107 - val_loss: 0.0065 - 565ms/epoch - 530us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0119 - val_loss: 0.0062 - 593ms/epoch - 555us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0136 - val_loss: 0.0160 - 596ms/epoch - 559us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0097 - val_loss: 0.0049 - 574ms/epoch - 538us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0149 - val_loss: 0.0039 - 584ms/epoch - 547us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0378 - val_loss: 0.0244 - 597ms/epoch - 559us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0097 - val_loss: 0.0112 - 600ms/epoch - 562us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0174 - val_loss: 0.0041 - 607ms/epoch - 569us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0088 - val_loss: 0.0045 - 584ms/epoch - 547us/step\n",
      "267/267 - 0s - loss: 0.0059 - 126ms/epoch - 471us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  27.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0211 - val_loss: 0.0049\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0261 - val_loss: 0.0085\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0079 - val_loss: 0.0066\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 0.0371 - val_loss: 0.0046\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 0.0110 - val_loss: 0.0229\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0079 - val_loss: 0.0168\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0298 - val_loss: 0.0095\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0116 - val_loss: 0.1074\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0295 - val_loss: 0.0143\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0108 - val_loss: 0.0081\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0215 - val_loss: 0.0522\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0132 - val_loss: 0.0033 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0094 - val_loss: 0.0050 - 635ms/epoch - 595us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0166 - val_loss: 0.0055 - 618ms/epoch - 579us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0076 - val_loss: 0.0047 - 608ms/epoch - 570us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0207 - val_loss: 0.0045 - 642ms/epoch - 602us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0106 - val_loss: 0.3592 - 606ms/epoch - 568us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0584 - val_loss: 0.0076 - 588ms/epoch - 551us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0119 - val_loss: 0.0129 - 591ms/epoch - 553us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0098 - val_loss: 0.0085 - 620ms/epoch - 581us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0168 - val_loss: 0.0062 - 614ms/epoch - 575us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0112 - val_loss: 0.0070 - 598ms/epoch - 560us/step\n",
      "267/267 - 0s - loss: 0.0093 - 125ms/epoch - 469us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  28.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0164 - val_loss: 0.0048\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0521 - val_loss: 0.0121\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0097 - val_loss: 0.0078\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0272 - val_loss: 0.0696\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0096 - val_loss: 0.0056\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 0.0253 - val_loss: 0.0185\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 0.0105 - val_loss: 0.0089\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0244 - val_loss: 0.0334\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0089 - val_loss: 0.0037\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0172 - val_loss: 0.0047\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0453 - val_loss: 0.0046\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0128 - val_loss: 0.0084 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0110 - val_loss: 0.0050 - 564ms/epoch - 528us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0185 - val_loss: 0.0076 - 558ms/epoch - 523us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0086 - val_loss: 0.0065 - 563ms/epoch - 528us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0139 - val_loss: 0.0040 - 562ms/epoch - 527us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0130 - val_loss: 0.0093 - 562ms/epoch - 527us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0204 - val_loss: 0.0370 - 562ms/epoch - 527us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0102 - val_loss: 0.0096 - 560ms/epoch - 525us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0248 - val_loss: 0.0756 - 559ms/epoch - 524us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0237 - val_loss: 0.0064 - 561ms/epoch - 526us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0151 - val_loss: 0.0173 - 589ms/epoch - 552us/step\n",
      "267/267 - 0s - loss: 0.0275 - 119ms/epoch - 448us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=adam; total time=  27.5s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3283 - val_loss: 0.2468\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 480us/step - loss: 0.1915 - val_loss: 0.1453\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 509us/step - loss: 0.1130 - val_loss: 0.0859\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 481us/step - loss: 0.0670 - val_loss: 0.0512\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0401 - val_loss: 0.0308\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 501us/step - loss: 0.0243 - val_loss: 0.0188\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0150 - val_loss: 0.0118\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 496us/step - loss: 0.0095 - val_loss: 0.0077\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.0063 - val_loss: 0.0052\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 479us/step - loss: 0.0044 - val_loss: 0.0038\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 467us/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0026 - val_loss: 0.0024 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0023 - val_loss: 0.0021 - 457ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0020 - val_loss: 0.0019 - 479ms/epoch - 449us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0018 - val_loss: 0.0018 - 466ms/epoch - 437us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 521ms/epoch - 488us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 481ms/epoch - 451us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 480ms/epoch - 450us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 465ms/epoch - 436us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0014 - val_loss: 0.0014 - 538ms/epoch - 504us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0014 - val_loss: 0.0014 - 521ms/epoch - 489us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0014 - val_loss: 0.0014 - 531ms/epoch - 498us/step\n",
      "267/267 - 0s - loss: 0.0014 - 129ms/epoch - 482us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  25.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3339 - val_loss: 0.2495\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 452us/step - loss: 0.1938 - val_loss: 0.1471\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 464us/step - loss: 0.1144 - val_loss: 0.0871\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 449us/step - loss: 0.0679 - val_loss: 0.0519\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0407 - val_loss: 0.0313\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0247 - val_loss: 0.0192\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 460us/step - loss: 0.0153 - val_loss: 0.0121\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0098 - val_loss: 0.0079\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 457us/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0027 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0023 - val_loss: 0.0022 - 474ms/epoch - 444us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0021 - val_loss: 0.0020 - 458ms/epoch - 430us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0018 - 541ms/epoch - 507us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0018 - val_loss: 0.0017 - 491ms/epoch - 460us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0017 - val_loss: 0.0016 - 500ms/epoch - 469us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 472ms/epoch - 443us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0015 - val_loss: 0.0015 - 504ms/epoch - 472us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0015 - val_loss: 0.0015 - 525ms/epoch - 492us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0014 - val_loss: 0.0014 - 512ms/epoch - 480us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0014 - val_loss: 0.0014 - 493ms/epoch - 462us/step\n",
      "267/267 - 0s - loss: 0.0015 - 131ms/epoch - 489us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  25.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3521 - val_loss: 0.2594\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 470us/step - loss: 0.2015 - val_loss: 0.1530\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 482us/step - loss: 0.1191 - val_loss: 0.0907\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 470us/step - loss: 0.0708 - val_loss: 0.0542\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0425 - val_loss: 0.0328\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 470us/step - loss: 0.0259 - val_loss: 0.0202\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 472us/step - loss: 0.0161 - val_loss: 0.0127\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 433us/step - loss: 0.0103 - val_loss: 0.0083\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 448us/step - loss: 0.0069 - val_loss: 0.0057\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 431us/step - loss: 0.0049 - val_loss: 0.0042\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 445us/step - loss: 0.0036 - val_loss: 0.0032\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0029 - val_loss: 0.0027 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0025 - val_loss: 0.0024 - 458ms/epoch - 429us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0022 - val_loss: 0.0021 - 456ms/epoch - 427us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0020 - val_loss: 0.0020 - 458ms/epoch - 429us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0019 - val_loss: 0.0019 - 470ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0018 - val_loss: 0.0018 - 502ms/epoch - 470us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0017 - val_loss: 0.0017 - 466ms/epoch - 437us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 458ms/epoch - 429us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 457ms/epoch - 428us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 453ms/epoch - 425us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 455ms/epoch - 427us/step\n",
      "267/267 - 0s - loss: 0.0016 - 117ms/epoch - 439us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  24.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 0.3271 - val_loss: 0.2473\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 461us/step - loss: 0.1920 - val_loss: 0.1457\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 467us/step - loss: 0.1134 - val_loss: 0.0863\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 487us/step - loss: 0.0673 - val_loss: 0.0514\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 457us/step - loss: 0.0403 - val_loss: 0.0310\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 435us/step - loss: 0.0244 - val_loss: 0.0190\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0151 - val_loss: 0.0119\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 442us/step - loss: 0.0096 - val_loss: 0.0078\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0064 - val_loss: 0.0053\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 498us/step - loss: 0.0045 - val_loss: 0.0038\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 483us/step - loss: 0.0033 - val_loss: 0.0029\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0027 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0023 - val_loss: 0.0022 - 512ms/epoch - 480us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0020 - val_loss: 0.0020 - 470ms/epoch - 440us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0019 - val_loss: 0.0018 - 521ms/epoch - 489us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0017 - val_loss: 0.0017 - 510ms/epoch - 478us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 483ms/epoch - 453us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 487ms/epoch - 457us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0015 - val_loss: 0.0015 - 508ms/epoch - 476us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 468ms/epoch - 438us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0014 - val_loss: 0.0014 - 483ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0014 - val_loss: 0.0014 - 613ms/epoch - 575us/step\n",
      "267/267 - 0s - loss: 0.0014 - 127ms/epoch - 477us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  25.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3338 - val_loss: 0.2496\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 494us/step - loss: 0.1937 - val_loss: 0.1470\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 504us/step - loss: 0.1143 - val_loss: 0.0870\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0679 - val_loss: 0.0519\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 467us/step - loss: 0.0406 - val_loss: 0.0313\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 463us/step - loss: 0.0247 - val_loss: 0.0192\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 455us/step - loss: 0.0153 - val_loss: 0.0120\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 437us/step - loss: 0.0097 - val_loss: 0.0078\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 451us/step - loss: 0.0065 - val_loss: 0.0054\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 443us/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0027 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0023 - val_loss: 0.0022 - 452ms/epoch - 423us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0021 - val_loss: 0.0020 - 453ms/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0019 - val_loss: 0.0018 - 451ms/epoch - 423us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0018 - val_loss: 0.0017 - 450ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0017 - val_loss: 0.0017 - 452ms/epoch - 423us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0016 - val_loss: 0.0016 - 453ms/epoch - 425us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 451ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 457ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0015 - val_loss: 0.0015 - 451ms/epoch - 423us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0014 - val_loss: 0.0014 - 464ms/epoch - 435us/step\n",
      "267/267 - 0s - loss: 0.0015 - 118ms/epoch - 444us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l2, learning_rate=0.1, optimizer=sgd; total time=  24.6s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4110 - val_loss: 0.3443\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 436us/step - loss: 0.1121 - val_loss: 0.0506\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 448us/step - loss: 0.0416 - val_loss: 0.0357\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 435us/step - loss: 0.0320 - val_loss: 0.0295\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0273 - val_loss: 0.0260\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 493us/step - loss: 0.0245 - val_loss: 0.0236\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0224 - val_loss: 0.0220\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 434us/step - loss: 0.0210 - val_loss: 0.0207\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0199 - val_loss: 0.0198\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 435us/step - loss: 0.0190 - val_loss: 0.0192\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 449us/step - loss: 0.0184 - val_loss: 0.0183\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0178 - val_loss: 0.0179 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0174 - val_loss: 0.0174 - 455ms/epoch - 427us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0171 - val_loss: 0.0172 - 460ms/epoch - 431us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0167 - val_loss: 0.0168 - 457ms/epoch - 429us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0164 - val_loss: 0.0166 - 447ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0161 - val_loss: 0.0162 - 427ms/epoch - 400us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0159 - val_loss: 0.0160 - 454ms/epoch - 426us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0156 - val_loss: 0.0156 - 454ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0154 - val_loss: 0.0154 - 469ms/epoch - 439us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0151 - val_loss: 0.0152 - 453ms/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0149 - val_loss: 0.0150 - 454ms/epoch - 426us/step\n",
      "267/267 - 0s - loss: 0.0156 - 126ms/epoch - 473us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  24.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.3936 - val_loss: 0.3394\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 434us/step - loss: 0.1127 - val_loss: 0.0516\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0422 - val_loss: 0.0366\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 434us/step - loss: 0.0330 - val_loss: 0.0310\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 448us/step - loss: 0.0290 - val_loss: 0.0279\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 434us/step - loss: 0.0264 - val_loss: 0.0259\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 449us/step - loss: 0.0245 - val_loss: 0.0241\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 433us/step - loss: 0.0230 - val_loss: 0.0230\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0218 - val_loss: 0.0217\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 432us/step - loss: 0.0208 - val_loss: 0.0207\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0200 - val_loss: 0.0199\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0192 - val_loss: 0.0194 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0187 - val_loss: 0.0189 - 459ms/epoch - 431us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0182 - val_loss: 0.0184 - 454ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0178 - val_loss: 0.0180 - 450ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0174 - val_loss: 0.0176 - 454ms/epoch - 425us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0170 - val_loss: 0.0173 - 449ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0167 - val_loss: 0.0168 - 478ms/epoch - 448us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0164 - val_loss: 0.0165 - 453ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0160 - val_loss: 0.0163 - 476ms/epoch - 446us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0158 - val_loss: 0.0160 - 453ms/epoch - 425us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0155 - val_loss: 0.0157 - 451ms/epoch - 423us/step\n",
      "267/267 - 0s - loss: 0.0169 - 120ms/epoch - 450us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  24.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 1ms/step - loss: 1.4066 - val_loss: 0.3381\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 433us/step - loss: 0.1141 - val_loss: 0.0563\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0459 - val_loss: 0.0390\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 431us/step - loss: 0.0344 - val_loss: 0.0313\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0287 - val_loss: 0.0272\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 432us/step - loss: 0.0254 - val_loss: 0.0245\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 447us/step - loss: 0.0232 - val_loss: 0.0226\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 429us/step - loss: 0.0216 - val_loss: 0.0212\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 445us/step - loss: 0.0203 - val_loss: 0.0200\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 432us/step - loss: 0.0192 - val_loss: 0.0190\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 446us/step - loss: 0.0182 - val_loss: 0.0181\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0175 - val_loss: 0.0175 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0169 - val_loss: 0.0170 - 453ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0165 - val_loss: 0.0165 - 460ms/epoch - 431us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0160 - val_loss: 0.0161 - 467ms/epoch - 437us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0157 - val_loss: 0.0157 - 456ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0153 - val_loss: 0.0157 - 453ms/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 0s - loss: 0.0150 - val_loss: 0.0152 - 456ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 0s - loss: 0.0147 - val_loss: 0.0148 - 453ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 0s - loss: 0.0145 - val_loss: 0.0145 - 453ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0142 - val_loss: 0.0143 - 453ms/epoch - 425us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0140 - val_loss: 0.0141 - 453ms/epoch - 424us/step\n",
      "267/267 - 0s - loss: 0.0149 - 128ms/epoch - 481us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  24.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.3920 - val_loss: 0.3343\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 508us/step - loss: 0.1121 - val_loss: 0.0520\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 505us/step - loss: 0.0418 - val_loss: 0.0355\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 540us/step - loss: 0.0317 - val_loss: 0.0295\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0275 - val_loss: 0.0264\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 488us/step - loss: 0.0249 - val_loss: 0.0243\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 475us/step - loss: 0.0232 - val_loss: 0.0228\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0217 - val_loss: 0.0215\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 517us/step - loss: 0.0206 - val_loss: 0.0204\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 507us/step - loss: 0.0195 - val_loss: 0.0194\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.0186 - val_loss: 0.0184\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0180 - val_loss: 0.0179 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0175 - val_loss: 0.0173 - 493ms/epoch - 462us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0170 - val_loss: 0.0168 - 481ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0165 - val_loss: 0.0164 - 463ms/epoch - 434us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0161 - val_loss: 0.0160 - 494ms/epoch - 463us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0157 - val_loss: 0.0157 - 510ms/epoch - 478us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0154 - val_loss: 0.0154 - 589ms/epoch - 552us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0151 - val_loss: 0.0150 - 511ms/epoch - 479us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0148 - val_loss: 0.0148 - 505ms/epoch - 473us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 0s - loss: 0.0145 - val_loss: 0.0145 - 478ms/epoch - 448us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 0s - loss: 0.0143 - val_loss: 0.0144 - 467ms/epoch - 438us/step\n",
      "267/267 - 0s - loss: 0.0149 - 124ms/epoch - 463us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  25.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 1.3790 - val_loss: 0.3273\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 453us/step - loss: 0.1103 - val_loss: 0.0514\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 462us/step - loss: 0.0424 - val_loss: 0.0369\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 450us/step - loss: 0.0333 - val_loss: 0.0311\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 479us/step - loss: 0.0290 - val_loss: 0.0280\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 488us/step - loss: 0.0263 - val_loss: 0.0257\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 475us/step - loss: 0.0243 - val_loss: 0.0238\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 465us/step - loss: 0.0228 - val_loss: 0.0225\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 512us/step - loss: 0.0216 - val_loss: 0.0214\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 468us/step - loss: 0.0205 - val_loss: 0.0204\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 451us/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0190 - val_loss: 0.0190 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 0s - loss: 0.0185 - val_loss: 0.0186 - 481ms/epoch - 451us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 0s - loss: 0.0180 - val_loss: 0.0180 - 480ms/epoch - 449us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 0s - loss: 0.0176 - val_loss: 0.0176 - 461ms/epoch - 432us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 0s - loss: 0.0172 - val_loss: 0.0173 - 476ms/epoch - 446us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 0s - loss: 0.0169 - val_loss: 0.0169 - 472ms/epoch - 442us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0166 - val_loss: 0.0166 - 505ms/epoch - 473us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0163 - val_loss: 0.0164 - 518ms/epoch - 485us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0160 - val_loss: 0.0161 - 503ms/epoch - 471us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0158 - val_loss: 0.0158 - 557ms/epoch - 522us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0156 - val_loss: 0.0157 - 535ms/epoch - 502us/step\n",
      "267/267 - 0s - loss: 0.0162 - 131ms/epoch - 491us/step\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.01, optimizer=sgd; total time=  25.2s\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2232 - val_loss: 0.0152\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 641us/step - loss: 0.0118 - val_loss: 0.0094\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0079 - val_loss: 0.0070\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0063 - val_loss: 0.0059\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0055 - val_loss: 0.0051\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0036 - val_loss: 0.0036 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 677ms/epoch - 635us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 605ms/epoch - 567us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 601ms/epoch - 564us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 590ms/epoch - 553us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0033 - 627ms/epoch - 588us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0032 - 599ms/epoch - 561us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0034 - 595ms/epoch - 558us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0032 - 610ms/epoch - 572us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0031 - 631ms/epoch - 591us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0031 - 605ms/epoch - 567us/step\n",
      "267/267 - 0s - loss: 0.0032 - 128ms/epoch - 478us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  28.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2260 - val_loss: 0.0142\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0114 - val_loss: 0.0094\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0083 - val_loss: 0.0073\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0053 - val_loss: 0.0051\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0046 - val_loss: 0.0043\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0039 - val_loss: 0.0038 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0038 - 630ms/epoch - 590us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 579ms/epoch - 542us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0036 - 610ms/epoch - 572us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0035 - 600ms/epoch - 562us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 604ms/epoch - 566us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 584ms/epoch - 548us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 594ms/epoch - 557us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 638ms/epoch - 598us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0034 - 599ms/epoch - 561us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0032 - 591ms/epoch - 554us/step\n",
      "267/267 - 0s - loss: 0.0032 - 127ms/epoch - 477us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  28.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2141 - val_loss: 0.0159\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0119 - val_loss: 0.0096\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0086 - val_loss: 0.0076\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0070 - val_loss: 0.0063\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0061 - val_loss: 0.0061\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0055 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0051 - val_loss: 0.0049\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0043 - val_loss: 0.0042\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 624us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0040 - val_loss: 0.0042 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0039 - val_loss: 0.0039 - 608ms/epoch - 570us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0037 - 600ms/epoch - 563us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 601ms/epoch - 563us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 593ms/epoch - 555us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0036 - 586ms/epoch - 549us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0035 - 618ms/epoch - 579us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 590ms/epoch - 553us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 598ms/epoch - 560us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0035 - 604ms/epoch - 566us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 622ms/epoch - 583us/step\n",
      "267/267 - 0s - loss: 0.0034 - 126ms/epoch - 472us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  28.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.2251 - val_loss: 0.0147\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0109 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 640us/step - loss: 0.0063 - val_loss: 0.0058\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0048 - val_loss: 0.0045\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0034 - val_loss: 0.0036 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 566ms/epoch - 531us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0033 - 564ms/epoch - 529us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0032 - 567ms/epoch - 531us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0032 - val_loss: 0.0031 - 591ms/epoch - 554us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0031 - 573ms/epoch - 537us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0031 - 603ms/epoch - 565us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0031 - val_loss: 0.0030 - 569ms/epoch - 533us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 570ms/epoch - 534us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0030 - 580ms/epoch - 544us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0030 - val_loss: 0.0029 - 589ms/epoch - 552us/step\n",
      "267/267 - 0s - loss: 0.0029 - 130ms/epoch - 486us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  28.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2171 - val_loss: 0.0143\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0117 - val_loss: 0.0097\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0085 - val_loss: 0.0074\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0068 - val_loss: 0.0063\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0059 - val_loss: 0.0062\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 610us/step - loss: 0.0053 - val_loss: 0.0054\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 0.0046 - val_loss: 0.0044\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 618us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 646us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 1/11\n",
      "1067/1067 - 2s - loss: 0.0039 - val_loss: 0.0038 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1067/1067 - 1s - loss: 0.0038 - val_loss: 0.0037 - 608ms/epoch - 570us/step\n",
      "Epoch 3/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0037 - 622ms/epoch - 583us/step\n",
      "Epoch 4/11\n",
      "1067/1067 - 1s - loss: 0.0037 - val_loss: 0.0036 - 626ms/epoch - 586us/step\n",
      "Epoch 5/11\n",
      "1067/1067 - 1s - loss: 0.0036 - val_loss: 0.0035 - 596ms/epoch - 559us/step\n",
      "Epoch 6/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0035 - 600ms/epoch - 563us/step\n",
      "Epoch 7/11\n",
      "1067/1067 - 1s - loss: 0.0035 - val_loss: 0.0034 - 668ms/epoch - 626us/step\n",
      "Epoch 8/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 627ms/epoch - 588us/step\n",
      "Epoch 9/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0033 - 576ms/epoch - 540us/step\n",
      "Epoch 10/11\n",
      "1067/1067 - 1s - loss: 0.0034 - val_loss: 0.0034 - 574ms/epoch - 538us/step\n",
      "Epoch 11/11\n",
      "1067/1067 - 1s - loss: 0.0033 - val_loss: 0.0034 - 579ms/epoch - 543us/step\n",
      "267/267 - 0s - loss: 0.0034 - 126ms/epoch - 471us/step\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1_l2, learning_rate=0.001, optimizer=adam; total time=  28.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.3385 - val_loss: 0.2539\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 448us/step - loss: 0.1971 - val_loss: 0.1495\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 491us/step - loss: 0.1163 - val_loss: 0.0885\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 478us/step - loss: 0.0690 - val_loss: 0.0527\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 457us/step - loss: 0.0413 - val_loss: 0.0317\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 471us/step - loss: 0.0250 - val_loss: 0.0194\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 481us/step - loss: 0.0155 - val_loss: 0.0122\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 480us/step - loss: 0.0099 - val_loss: 0.0079\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 484us/step - loss: 0.0066 - val_loss: 0.0054\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 492us/step - loss: 0.0046 - val_loss: 0.0039\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 483us/step - loss: 0.0034 - val_loss: 0.0030\n",
      "Epoch 1/11\n",
      "1334/1334 - 2s - loss: 0.0027 - val_loss: 0.0025 - 2s/epoch - 2ms/step\n",
      "Epoch 2/11\n",
      "1334/1334 - 1s - loss: 0.0023 - val_loss: 0.0021 - 588ms/epoch - 441us/step\n",
      "Epoch 3/11\n",
      "1334/1334 - 1s - loss: 0.0020 - val_loss: 0.0019 - 562ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "1334/1334 - 1s - loss: 0.0018 - val_loss: 0.0017 - 595ms/epoch - 446us/step\n",
      "Epoch 5/11\n",
      "1334/1334 - 1s - loss: 0.0017 - val_loss: 0.0016 - 570ms/epoch - 428us/step\n",
      "Epoch 6/11\n",
      "1334/1334 - 1s - loss: 0.0016 - val_loss: 0.0016 - 561ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "1334/1334 - 1s - loss: 0.0015 - val_loss: 0.0015 - 576ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "1334/1334 - 1s - loss: 0.0015 - val_loss: 0.0015 - 553ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "1334/1334 - 1s - loss: 0.0014 - val_loss: 0.0014 - 556ms/epoch - 417us/step\n",
      "Epoch 10/11\n",
      "1334/1334 - 1s - loss: 0.0014 - val_loss: 0.0014 - 561ms/epoch - 421us/step\n",
      "Epoch 11/11\n",
      "1334/1334 - 1s - loss: 0.0014 - val_loss: 0.0014 - 578ms/epoch - 433us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b07c730&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BayesSearchCV</label><div class=\"sk-toggleable__content\"><pre>BayesSearchCV(cv=5,\n",
       "              estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b07c730&gt;,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                             &#x27;batch_size&#x27;: [16, 32, 64],\n",
       "                             &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;],\n",
       "                             &#x27;learning_rate&#x27;: [0.001, 0.01, 0.1],\n",
       "                             &#x27;optimizer&#x27;: [&#x27;sgd&#x27;, &#x27;adam&#x27;]},\n",
       "              verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b07c730&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32b07c730&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "BayesSearchCV(cv=5,\n",
       "              estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32b07c730>,\n",
       "              n_iter=10, random_state=42,\n",
       "              search_spaces={'activation': ['relu', 'tanh'],\n",
       "                             'batch_size': [16, 32, 64],\n",
       "                             'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
       "                             'learning_rate': [0.001, 0.01, 0.1],\n",
       "                             'optimizer': ['sgd', 'adam']},\n",
       "              verbose=2)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for bayes search\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=batch_size, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "bayes_search = BayesSearchCV(estimator=keras_reg, search_spaces=param_dist, n_iter=10, cv=5, verbose=2, random_state=42, error_score='raise')\n",
    "bayes_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_batch_size</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_optimizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.021677</td>\n",
       "      <td>0.163531</td>\n",
       "      <td>0.140168</td>\n",
       "      <td>0.008184</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.003398</td>\n",
       "      <td>-0.003198</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>-0.003018</td>\n",
       "      <td>-0.003532</td>\n",
       "      <td>-0.003286</td>\n",
       "      <td>0.000175</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27.137795</td>\n",
       "      <td>0.111458</td>\n",
       "      <td>0.135636</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.232466</td>\n",
       "      <td>-0.222146</td>\n",
       "      <td>-0.234215</td>\n",
       "      <td>-0.256379</td>\n",
       "      <td>-0.267332</td>\n",
       "      <td>-0.242508</td>\n",
       "      <td>0.016690</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.153611</td>\n",
       "      <td>0.102178</td>\n",
       "      <td>0.135748</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.007653</td>\n",
       "      <td>-0.008666</td>\n",
       "      <td>-0.009028</td>\n",
       "      <td>-0.006957</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>-0.008383</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>184.689155</td>\n",
       "      <td>218.299531</td>\n",
       "      <td>0.332540</td>\n",
       "      <td>0.003031</td>\n",
       "      <td>tanh</td>\n",
       "      <td>16</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 16, 'kern...</td>\n",
       "      <td>-0.047537</td>\n",
       "      <td>-0.039715</td>\n",
       "      <td>-0.024670</td>\n",
       "      <td>-0.111357</td>\n",
       "      <td>-0.197292</td>\n",
       "      <td>-0.084114</td>\n",
       "      <td>0.063866</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.971558</td>\n",
       "      <td>0.337272</td>\n",
       "      <td>0.226165</td>\n",
       "      <td>0.034683</td>\n",
       "      <td>tanh</td>\n",
       "      <td>32</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 32, 'kern...</td>\n",
       "      <td>-0.040701</td>\n",
       "      <td>-0.011026</td>\n",
       "      <td>-0.010311</td>\n",
       "      <td>-0.012328</td>\n",
       "      <td>-0.104470</td>\n",
       "      <td>-0.035767</td>\n",
       "      <td>0.036205</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26.738787</td>\n",
       "      <td>0.126787</td>\n",
       "      <td>0.136814</td>\n",
       "      <td>0.003917</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>-0.004611</td>\n",
       "      <td>-0.004663</td>\n",
       "      <td>-0.004564</td>\n",
       "      <td>-0.004512</td>\n",
       "      <td>-0.004665</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27.204236</td>\n",
       "      <td>0.460855</td>\n",
       "      <td>0.138459</td>\n",
       "      <td>0.004516</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.159960</td>\n",
       "      <td>-0.011104</td>\n",
       "      <td>-0.005900</td>\n",
       "      <td>-0.009345</td>\n",
       "      <td>-0.027475</td>\n",
       "      <td>-0.042757</td>\n",
       "      <td>0.059071</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24.789019</td>\n",
       "      <td>0.331448</td>\n",
       "      <td>0.139847</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.001435</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>-0.001553</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>-0.001484</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.431403</td>\n",
       "      <td>0.602278</td>\n",
       "      <td>0.139786</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>tanh</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>sgd</td>\n",
       "      <td>{'activation': 'tanh', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.015601</td>\n",
       "      <td>-0.016905</td>\n",
       "      <td>-0.014890</td>\n",
       "      <td>-0.014851</td>\n",
       "      <td>-0.016239</td>\n",
       "      <td>-0.015697</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28.111176</td>\n",
       "      <td>0.231685</td>\n",
       "      <td>0.142179</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>relu</td>\n",
       "      <td>64</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>0.001</td>\n",
       "      <td>adam</td>\n",
       "      <td>{'activation': 'relu', 'batch_size': 64, 'kern...</td>\n",
       "      <td>-0.003191</td>\n",
       "      <td>-0.003236</td>\n",
       "      <td>-0.003447</td>\n",
       "      <td>-0.002913</td>\n",
       "      <td>-0.003354</td>\n",
       "      <td>-0.003228</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      27.021677      0.163531         0.140168        0.008184   \n",
       "1      27.137795      0.111458         0.135636        0.001802   \n",
       "2      24.153611      0.102178         0.135748        0.001332   \n",
       "3     184.689155    218.299531         0.332540        0.003031   \n",
       "4      40.971558      0.337272         0.226165        0.034683   \n",
       "5      26.738787      0.126787         0.136814        0.003917   \n",
       "6      27.204236      0.460855         0.138459        0.004516   \n",
       "7      24.789019      0.331448         0.139847        0.007393   \n",
       "8      24.431403      0.602278         0.139786        0.004051   \n",
       "9      28.111176      0.231685         0.142179        0.002106   \n",
       "\n",
       "  param_activation param_batch_size param_kernel_regularizer  \\\n",
       "0             relu               64                    l1_l2   \n",
       "1             tanh               64                       l1   \n",
       "2             relu               64                       l1   \n",
       "3             tanh               16                       l2   \n",
       "4             tanh               32                       l2   \n",
       "5             tanh               64                       l1   \n",
       "6             tanh               64                       l2   \n",
       "7             tanh               64                       l2   \n",
       "8             tanh               64                    l1_l2   \n",
       "9             relu               64                    l1_l2   \n",
       "\n",
       "  param_learning_rate param_optimizer  \\\n",
       "0               0.001            adam   \n",
       "1                 0.1            adam   \n",
       "2                0.01             sgd   \n",
       "3                 0.1            adam   \n",
       "4                 0.1            adam   \n",
       "5               0.001            adam   \n",
       "6                 0.1            adam   \n",
       "7                 0.1             sgd   \n",
       "8                0.01             sgd   \n",
       "9               0.001            adam   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.003398   \n",
       "1  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.232466   \n",
       "2  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.007653   \n",
       "3  {'activation': 'tanh', 'batch_size': 16, 'kern...          -0.047537   \n",
       "4  {'activation': 'tanh', 'batch_size': 32, 'kern...          -0.040701   \n",
       "5  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.004974   \n",
       "6  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.159960   \n",
       "7  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.001435   \n",
       "8  {'activation': 'tanh', 'batch_size': 64, 'kern...          -0.015601   \n",
       "9  {'activation': 'relu', 'batch_size': 64, 'kern...          -0.003191   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.003198          -0.003284          -0.003018          -0.003532   \n",
       "1          -0.222146          -0.234215          -0.256379          -0.267332   \n",
       "2          -0.008666          -0.009028          -0.006957          -0.009612   \n",
       "3          -0.039715          -0.024670          -0.111357          -0.197292   \n",
       "4          -0.011026          -0.010311          -0.012328          -0.104470   \n",
       "5          -0.004611          -0.004663          -0.004564          -0.004512   \n",
       "6          -0.011104          -0.005900          -0.009345          -0.027475   \n",
       "7          -0.001507          -0.001553          -0.001412          -0.001484   \n",
       "8          -0.016905          -0.014890          -0.014851          -0.016239   \n",
       "9          -0.003236          -0.003447          -0.002913          -0.003354   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.003286        0.000175                3  \n",
       "1        -0.242508        0.016690               10  \n",
       "2        -0.008383        0.000956                5  \n",
       "3        -0.084114        0.063866                9  \n",
       "4        -0.035767        0.036205                7  \n",
       "5        -0.004665        0.000162                4  \n",
       "6        -0.042757        0.059071                8  \n",
       "7        -0.001478        0.000051                1  \n",
       "8        -0.015697        0.000791                6  \n",
       "9        -0.003228        0.000181                2  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bayes_search_df = pd.DataFrame(bayes_search.cv_results_)\n",
    "\n",
    "display(bayes_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/txb22f353wl_gt1f7mbxzq8h0000gn/T/ipykernel_99292/3282579545.py:23: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 2\n",
      "n_required_iterations: 2\n",
      "n_possible_iterations: 2\n",
      "min_resources_: 28452\n",
      "max_resources_: 85357\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 6\n",
      "n_resources: 28452\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2035 - val_loss: 0.0220\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0189 - val_loss: 0.0168\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0150 - val_loss: 0.0138\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 0.0129 - val_loss: 0.0119\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0116 - val_loss: 0.0107\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 645us/step - loss: 0.0105 - val_loss: 0.0098\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0096 - val_loss: 0.0091\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 638us/step - loss: 0.0089 - val_loss: 0.0082\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 0.0072 - val_loss: 0.0069\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 652us/step - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0064 - 303ms/epoch - 425us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0062 - 302ms/epoch - 425us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0061 - 303ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0059 - 303ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0059 - 301ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0058 - 300ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0057 - 302ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0056 - 301ms/epoch - 423us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 305ms/epoch - 428us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0054 - 305ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0053 - 302ms/epoch - 424us/step\n",
      "178/178 - 0s - loss: 0.0055 - 97ms/epoch - 546us/step\n",
      "712/712 - 0s - loss: 0.0055 - 201ms/epoch - 282us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  21.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2007 - val_loss: 0.0203\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0174 - val_loss: 0.0152\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0121 - val_loss: 0.0120\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0108 - val_loss: 0.0102\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0095 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0082 - val_loss: 0.0078\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 633us/step - loss: 0.0070 - val_loss: 0.0072\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0066 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 0.0063 - val_loss: 0.0059\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0061 - 349ms/epoch - 491us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0059 - 354ms/epoch - 497us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0058 - 350ms/epoch - 492us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0056 - 317ms/epoch - 446us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0055 - 340ms/epoch - 478us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0054 - 349ms/epoch - 490us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0053 - 345ms/epoch - 485us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0053 - 309ms/epoch - 434us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0052 - 347ms/epoch - 487us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0051 - 323ms/epoch - 453us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0050 - 310ms/epoch - 436us/step\n",
      "178/178 - 0s - loss: 0.0051 - 97ms/epoch - 544us/step\n",
      "712/712 - 0s - loss: 0.0051 - 202ms/epoch - 283us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  20.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1916 - val_loss: 0.0195\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 704us/step - loss: 0.0171 - val_loss: 0.0162\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0140 - val_loss: 0.0124\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0121 - val_loss: 0.0114\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 634us/step - loss: 0.0109 - val_loss: 0.0105\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0101 - val_loss: 0.0099\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 0.0092 - val_loss: 0.0085\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 637us/step - loss: 0.0083 - val_loss: 0.0078\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 616us/step - loss: 0.0074 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0069 - val_loss: 0.0070\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 615us/step - loss: 0.0065 - val_loss: 0.0070\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0063 - 352ms/epoch - 494us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0061 - 319ms/epoch - 449us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0060 - 320ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0059 - 312ms/epoch - 438us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0058 - 314ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0057 - 344ms/epoch - 483us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0056 - 362ms/epoch - 509us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0055 - 377ms/epoch - 529us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 359ms/epoch - 504us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0053 - 331ms/epoch - 465us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0052 - 337ms/epoch - 474us/step\n",
      "178/178 - 0s - loss: 0.0055 - 109ms/epoch - 612us/step\n",
      "712/712 - 0s - loss: 0.0055 - 218ms/epoch - 306us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  21.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1989 - val_loss: 0.0220\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0186 - val_loss: 0.0159\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0148 - val_loss: 0.0136\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0126 - val_loss: 0.0115\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 636us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 668us/step - loss: 0.0094 - val_loss: 0.0085\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 0.0082 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 620us/step - loss: 0.0076 - val_loss: 0.0082\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0067 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 552us/step - loss: 0.0063 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0061 - 302ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0059 - 344ms/epoch - 483us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0058 - 307ms/epoch - 431us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0056 - 319ms/epoch - 448us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0056 - 309ms/epoch - 434us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0054 - 306ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0054 - 298ms/epoch - 419us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0053 - 313ms/epoch - 440us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0052 - 299ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0051 - 297ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0051 - 299ms/epoch - 419us/step\n",
      "178/178 - 0s - loss: 0.0050 - 97ms/epoch - 547us/step\n",
      "712/712 - 0s - loss: 0.0051 - 198ms/epoch - 278us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2028 - val_loss: 0.0209\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 0.0177 - val_loss: 0.0159\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0143 - val_loss: 0.0133\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0124 - val_loss: 0.0118\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 656us/step - loss: 0.0111 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 0.0094 - val_loss: 0.0088\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0087 - val_loss: 0.0083\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0078 - val_loss: 0.0073\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0061 - 298ms/epoch - 419us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0059 - 299ms/epoch - 420us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0058 - 297ms/epoch - 418us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0057 - 299ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0056 - 299ms/epoch - 420us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0055 - 343ms/epoch - 482us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0054 - 371ms/epoch - 522us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0053 - 334ms/epoch - 469us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0053 - 305ms/epoch - 429us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0051 - 312ms/epoch - 438us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0051 - 341ms/epoch - 479us/step\n",
      "178/178 - 0s - loss: 0.0049 - 108ms/epoch - 608us/step\n",
      "712/712 - 0s - loss: 0.0050 - 228ms/epoch - 320us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l1; total time=  20.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 0.0012\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 8.8521e-04 - val_loss: 8.4470e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 548us/step - loss: 7.0814e-04 - val_loss: 0.0017\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 5.8825e-04 - val_loss: 4.6867e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 5.1917e-04 - val_loss: 4.3317e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 4.7070e-04 - val_loss: 4.4854e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 4.4282e-04 - val_loss: 4.5470e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 4.2926e-04 - val_loss: 3.1610e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 541us/step - loss: 4.1936e-04 - val_loss: 3.0706e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 3.8729e-04 - val_loss: 4.5888e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 542us/step - loss: 3.7386e-04 - val_loss: 4.6562e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 4.8110e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 4.0630e-04 - 296ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 3.9690e-04 - 294ms/epoch - 412us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 4.2387e-04 - 290ms/epoch - 407us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 4.2214e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 4.5071e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 5.3129e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 3.9939e-04 - 296ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 4.3793e-04 - 325ms/epoch - 457us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 4.0990e-04 - 340ms/epoch - 478us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 4.3291e-04 - 312ms/epoch - 438us/step\n",
      "178/178 - 0s - loss: 7.6709e-04 - 98ms/epoch - 548us/step\n",
      "712/712 - 0s - loss: 7.6170e-04 - 200ms/epoch - 282us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0337 - val_loss: 9.7725e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 9.0043e-04 - val_loss: 6.9072e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 7.0098e-04 - val_loss: 5.9233e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 650us/step - loss: 6.1601e-04 - val_loss: 8.2817e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 5.3415e-04 - val_loss: 4.5559e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 4.7917e-04 - val_loss: 3.7500e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 4.6588e-04 - val_loss: 3.5926e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 4.1074e-04 - val_loss: 3.7639e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.9603e-04 - val_loss: 4.6327e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 3.9943e-04 - val_loss: 0.0011\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 3.7095e-04 - val_loss: 3.4890e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.5963e-04 - 329ms/epoch - 463us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 3.9846e-04 - 309ms/epoch - 435us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 5.0580e-04 - 305ms/epoch - 428us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 5.0585e-04 - 311ms/epoch - 436us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 4.0611e-04 - 306ms/epoch - 430us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 4.4734e-04 - 303ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 4.4259e-04 - 308ms/epoch - 432us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 4.0517e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 3.7425e-04 - 308ms/epoch - 433us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 4.4107e-04 - 313ms/epoch - 439us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 4.2855e-04 - 346ms/epoch - 485us/step\n",
      "178/178 - 0s - loss: 6.2699e-04 - 101ms/epoch - 568us/step\n",
      "712/712 - 0s - loss: 6.0705e-04 - 214ms/epoch - 301us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0365 - val_loss: 0.0013\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0010 - val_loss: 8.6101e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 7.2747e-04 - val_loss: 0.0010\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 6.1893e-04 - val_loss: 4.6770e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 5.7330e-04 - val_loss: 4.1600e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 4.8583e-04 - val_loss: 6.4141e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 4.5799e-04 - val_loss: 3.8677e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 4.2661e-04 - val_loss: 3.3096e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 3.9768e-04 - val_loss: 4.3500e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 4.0663e-04 - val_loss: 2.9984e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 3.9805e-04 - val_loss: 2.9505e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 4.6814e-04 - 318ms/epoch - 447us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 4.4377e-04 - 343ms/epoch - 482us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 4.0006e-04 - 320ms/epoch - 450us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 4.8580e-04 - 323ms/epoch - 453us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 4.4244e-04 - 317ms/epoch - 445us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 4.1901e-04 - 308ms/epoch - 433us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 4.6385e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 3.7359e-04 - 307ms/epoch - 430us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 5.5167e-04 - 304ms/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 3.9586e-04 - 311ms/epoch - 437us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 4.5008e-04 - 307ms/epoch - 431us/step\n",
      "178/178 - 0s - loss: 0.0017 - 100ms/epoch - 560us/step\n",
      "712/712 - 0s - loss: 0.0017 - 222ms/epoch - 312us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  20.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0319 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 8.6143e-04 - val_loss: 7.1861e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 6.6399e-04 - val_loss: 9.0041e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 663us/step - loss: 5.7815e-04 - val_loss: 6.6760e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 700us/step - loss: 5.3205e-04 - val_loss: 4.4191e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 5.0104e-04 - val_loss: 3.6635e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 4.7887e-04 - val_loss: 5.2918e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 4.2228e-04 - val_loss: 3.1303e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 3.9464e-04 - val_loss: 3.9284e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 673us/step - loss: 3.8226e-04 - val_loss: 3.5555e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 609us/step - loss: 3.8946e-04 - val_loss: 4.4033e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.5665e-04 - 351ms/epoch - 493us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 4.4601e-04 - 331ms/epoch - 464us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 4.7362e-04 - 345ms/epoch - 485us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 3.8792e-04 - 339ms/epoch - 476us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 5.0333e-04 - 335ms/epoch - 471us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 4.3284e-04 - 344ms/epoch - 483us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 4.4697e-04 - 361ms/epoch - 507us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 4.2588e-04 - 360ms/epoch - 505us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 4.4051e-04 - 325ms/epoch - 457us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 4.5625e-04 - 352ms/epoch - 495us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 3.8958e-04 - 335ms/epoch - 471us/step\n",
      "178/178 - 0s - loss: 7.4722e-04 - 110ms/epoch - 617us/step\n",
      "712/712 - 0s - loss: 7.3512e-04 - 230ms/epoch - 323us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  21.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0334 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 9.5067e-04 - val_loss: 7.0782e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 613us/step - loss: 7.0160e-04 - val_loss: 5.5093e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 6.0965e-04 - val_loss: 5.1694e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 5.1901e-04 - val_loss: 4.3317e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 4.8425e-04 - val_loss: 3.8166e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 4.4803e-04 - val_loss: 3.3534e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 4.2447e-04 - val_loss: 3.3235e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 3.9605e-04 - val_loss: 3.0328e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 560us/step - loss: 3.7555e-04 - val_loss: 3.3776e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.4849e-04 - val_loss: 3.0271e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.0270e-04 - 305ms/epoch - 428us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 4.3002e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 3.7449e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 5.2108e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 4.3390e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 3.9353e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 3.9136e-04 - 299ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 4.2144e-04 - 303ms/epoch - 425us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 3.7752e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 4.4555e-04 - 301ms/epoch - 422us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 4.0887e-04 - 300ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 2.7332e-04 - 98ms/epoch - 549us/step\n",
      "712/712 - 0s - loss: 2.7699e-04 - 199ms/epoch - 280us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  20.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2134 - val_loss: 0.0213\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0179 - val_loss: 0.0156\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0141 - val_loss: 0.0136\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 556us/step - loss: 0.0122 - val_loss: 0.0115\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 0.0099 - val_loss: 0.0100\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0090 - val_loss: 0.0084\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0079 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0073 - val_loss: 0.0071\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0069 - val_loss: 0.0065\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0064 - val_loss: 0.0061\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0063 - 314ms/epoch - 440us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0061 - 302ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0059 - 301ms/epoch - 422us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0058 - 303ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0057 - 302ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0056 - 300ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0055 - 305ms/epoch - 428us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0054 - 301ms/epoch - 422us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0053 - 298ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0053 - 303ms/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0052 - 300ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0051 - 101ms/epoch - 569us/step\n",
      "712/712 - 0s - loss: 0.0051 - 201ms/epoch - 282us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2165 - val_loss: 0.0208\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0177 - val_loss: 0.0153\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0141 - val_loss: 0.0130\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0123 - val_loss: 0.0115\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0110 - val_loss: 0.0103\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0097 - val_loss: 0.0093\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0085 - val_loss: 0.0079\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0078 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0073 - val_loss: 0.0070\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0069 - val_loss: 0.0066\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0065 - val_loss: 0.0062\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0064 - 304ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0062 - 302ms/epoch - 423us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0061 - 299ms/epoch - 421us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0060 - 301ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0059 - 299ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0058 - 300ms/epoch - 422us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0056 - 298ms/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0056 - 303ms/epoch - 426us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 298ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0054 - 299ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0053 - 297ms/epoch - 418us/step\n",
      "178/178 - 0s - loss: 0.0055 - 102ms/epoch - 574us/step\n",
      "712/712 - 0s - loss: 0.0054 - 200ms/epoch - 281us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2227 - val_loss: 0.0214\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0182 - val_loss: 0.0159\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0126 - val_loss: 0.0125\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0113 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0102 - val_loss: 0.0094\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0090 - val_loss: 0.0082\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0081 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0075 - val_loss: 0.0071\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0070 - val_loss: 0.0069\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0067 - val_loss: 0.0064\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0065 - 304ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0063 - 305ms/epoch - 428us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0062 - 303ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0060 - 304ms/epoch - 427us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0059 - 301ms/epoch - 423us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0058 - 305ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0057 - 300ms/epoch - 422us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0056 - 302ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 302ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0055 - 302ms/epoch - 424us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0053 - 302ms/epoch - 424us/step\n",
      "178/178 - 0s - loss: 0.0054 - 100ms/epoch - 563us/step\n",
      "712/712 - 0s - loss: 0.0053 - 199ms/epoch - 280us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2217 - val_loss: 0.0222\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 0.0190 - val_loss: 0.0164\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0146 - val_loss: 0.0132\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0124 - val_loss: 0.0121\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0112 - val_loss: 0.0105\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0102 - val_loss: 0.0096\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0090 - val_loss: 0.0089\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0081 - val_loss: 0.0075\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0074 - val_loss: 0.0069\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0070 - val_loss: 0.0067\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0065 - val_loss: 0.0063\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0065 - 302ms/epoch - 424us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0061 - 300ms/epoch - 422us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0061 - 298ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0059 - 300ms/epoch - 422us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0058 - 300ms/epoch - 422us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0058 - 371ms/epoch - 521us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0057 - 304ms/epoch - 427us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0055 - 300ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 302ms/epoch - 425us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0054 - 297ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0053 - 299ms/epoch - 419us/step\n",
      "178/178 - 0s - loss: 0.0052 - 101ms/epoch - 567us/step\n",
      "712/712 - 0s - loss: 0.0053 - 200ms/epoch - 281us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2263 - val_loss: 0.0214\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0182 - val_loss: 0.0159\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0149 - val_loss: 0.0135\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0129 - val_loss: 0.0118\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0115 - val_loss: 0.0106\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0101 - val_loss: 0.0097\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 0.0088 - val_loss: 0.0085\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0080 - val_loss: 0.0076\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0075 - val_loss: 0.0072\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0071 - val_loss: 0.0068\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0066 - val_loss: 0.0067\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0065 - 304ms/epoch - 427us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0063 - 302ms/epoch - 425us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0061 - 301ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0060 - 304ms/epoch - 428us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0059 - 302ms/epoch - 424us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0058 - 303ms/epoch - 425us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0057 - 302ms/epoch - 424us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0056 - 299ms/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0055 - 301ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0054 - 297ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0054 - 299ms/epoch - 419us/step\n",
      "178/178 - 0s - loss: 0.0052 - 99ms/epoch - 554us/step\n",
      "712/712 - 0s - loss: 0.0053 - 201ms/epoch - 282us/step\n",
      "[CV] END ..........activation=tanh, kernel_regularizer=l1_l2; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.1925 - val_loss: 0.0145\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0119 - val_loss: 0.0103\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0088 - val_loss: 0.0076\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0069 - val_loss: 0.0061\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0059 - val_loss: 0.0053\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0052 - val_loss: 0.0049\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0048 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 0.0041 - val_loss: 0.0042\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0039 - val_loss: 0.0038\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0038 - 301ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0037 - 298ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0036 - 298ms/epoch - 419us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0036 - 296ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0035 - 298ms/epoch - 419us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0035 - 297ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0035 - 299ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0034 - 298ms/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0034 - 307ms/epoch - 431us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0034 - 299ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0033 - 297ms/epoch - 417us/step\n",
      "178/178 - 0s - loss: 0.0033 - 101ms/epoch - 567us/step\n",
      "712/712 - 0s - loss: 0.0033 - 198ms/epoch - 279us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  20.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1965 - val_loss: 0.0147\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0120 - val_loss: 0.0102\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0093 - val_loss: 0.0085\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0079 - val_loss: 0.0076\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0068 - val_loss: 0.0065\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 0.0059 - val_loss: 0.0057\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0054 - val_loss: 0.0051\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 606us/step - loss: 0.0050 - val_loss: 0.0048\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0042 - 308ms/epoch - 433us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0041 - 317ms/epoch - 445us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0040 - 313ms/epoch - 440us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0039 - 312ms/epoch - 438us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0039 - 315ms/epoch - 443us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0038 - 318ms/epoch - 447us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0038 - 352ms/epoch - 494us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0037 - 355ms/epoch - 498us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0037 - 333ms/epoch - 467us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0037 - 340ms/epoch - 477us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0036 - 345ms/epoch - 484us/step\n",
      "178/178 - 0s - loss: 0.0037 - 109ms/epoch - 612us/step\n",
      "712/712 - 0s - loss: 0.0037 - 221ms/epoch - 310us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  20.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1941 - val_loss: 0.0154\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 655us/step - loss: 0.0112 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 631us/step - loss: 0.0073 - val_loss: 0.0062\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 605us/step - loss: 0.0057 - val_loss: 0.0053\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 630us/step - loss: 0.0050 - val_loss: 0.0046\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0045 - val_loss: 0.0043\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 0.0039 - val_loss: 0.0039\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0034 - 328ms/epoch - 461us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0033 - 319ms/epoch - 448us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0033 - 335ms/epoch - 470us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0033 - 340ms/epoch - 478us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 429us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0032 - 305ms/epoch - 429us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0031 - 385ms/epoch - 541us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0031 - 328ms/epoch - 460us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0031 - 394ms/epoch - 553us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0031 - 435ms/epoch - 611us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0030 - 331ms/epoch - 465us/step\n",
      "178/178 - 0s - loss: 0.0031 - 127ms/epoch - 714us/step\n",
      "712/712 - 0s - loss: 0.0030 - 242ms/epoch - 341us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  21.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1938 - val_loss: 0.0140\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0115 - val_loss: 0.0099\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0086 - val_loss: 0.0073\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 0.0066 - val_loss: 0.0060\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0047 - val_loss: 0.0043\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 0.0033 - val_loss: 0.0031\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0031 - val_loss: 0.0030\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0031 - 305ms/epoch - 429us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0030 - 302ms/epoch - 424us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0030 - 304ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 425us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0029 - 303ms/epoch - 426us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0029 - 302ms/epoch - 424us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0029 - 306ms/epoch - 430us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0028 - 301ms/epoch - 423us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0028 - 301ms/epoch - 423us/step\n",
      "178/178 - 0s - loss: 0.0028 - 96ms/epoch - 540us/step\n",
      "712/712 - 0s - loss: 0.0028 - 198ms/epoch - 279us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  20.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.1897 - val_loss: 0.0150\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0115 - val_loss: 0.0089\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0076 - val_loss: 0.0066\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0059 - val_loss: 0.0054\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0051 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 0.0043 - val_loss: 0.0041\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 658us/step - loss: 0.0040 - val_loss: 0.0039\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 0.0036 - val_loss: 0.0035\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0034 - 301ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0033 - 302ms/epoch - 425us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0033 - 303ms/epoch - 425us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0032 - 299ms/epoch - 420us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0032 - 300ms/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0032 - 302ms/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0031 - 303ms/epoch - 425us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0031 - 300ms/epoch - 421us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0031 - 302ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0031 - 304ms/epoch - 428us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0030 - 301ms/epoch - 422us/step\n",
      "178/178 - 0s - loss: 0.0030 - 98ms/epoch - 549us/step\n",
      "712/712 - 0s - loss: 0.0030 - 199ms/epoch - 280us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l1; total time=  20.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0299 - val_loss: 7.5311e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 671us/step - loss: 5.3841e-04 - val_loss: 3.6977e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 614us/step - loss: 3.6784e-04 - val_loss: 2.5326e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 2.9635e-04 - val_loss: 2.1081e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 639us/step - loss: 2.5433e-04 - val_loss: 2.5887e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 732us/step - loss: 2.5198e-04 - val_loss: 7.1836e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 1.7909e-04 - val_loss: 2.9931e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 1.9741e-04 - val_loss: 1.1730e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 1.9886e-04 - val_loss: 1.1293e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 1.8615e-04 - val_loss: 1.0678e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 1.8554e-04 - val_loss: 1.0232e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.2517e-04 - 316ms/epoch - 443us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 1.4161e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 1.9191e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 2.6761e-04 - 296ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 2.4338e-04 - 320ms/epoch - 449us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 3.6417e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 1.2453e-04 - 274ms/epoch - 385us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 2.0573e-04 - 289ms/epoch - 406us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 2.2806e-04 - 296ms/epoch - 415us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 1.9679e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 4.3461e-04 - 296ms/epoch - 415us/step\n",
      "178/178 - 0s - loss: 1.5371e-04 - 96ms/epoch - 540us/step\n",
      "712/712 - 0s - loss: 1.5361e-04 - 194ms/epoch - 272us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  20.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 3s 2ms/step - loss: 0.0320 - val_loss: 9.3795e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 6.8210e-04 - val_loss: 4.7676e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.9422e-04 - val_loss: 2.8130e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 3.7710e-04 - val_loss: 2.2883e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 626us/step - loss: 2.7670e-04 - val_loss: 1.9333e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 611us/step - loss: 2.3629e-04 - val_loss: 2.1652e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 3.7212e-04 - val_loss: 2.0011e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 617us/step - loss: 1.7090e-04 - val_loss: 1.4317e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 2.3332e-04 - val_loss: 2.5442e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 824us/step - loss: 2.3482e-04 - val_loss: 1.2807e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 551us/step - loss: 2.1625e-04 - val_loss: 1.1828e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.7221e-04 - 319ms/epoch - 448us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 1.8663e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 5.1969e-04 - 301ms/epoch - 423us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 1.9163e-04 - 298ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 2.0812e-04 - 317ms/epoch - 446us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 2.6172e-04 - 302ms/epoch - 424us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 2.8703e-04 - 338ms/epoch - 475us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 2.0137e-04 - 454ms/epoch - 638us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 2.6486e-04 - 315ms/epoch - 442us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 2.4249e-04 - 299ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 2.7962e-04 - 299ms/epoch - 421us/step\n",
      "178/178 - 0s - loss: 1.4598e-04 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 1.4479e-04 - 197ms/epoch - 277us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  21.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0322 - val_loss: 9.4341e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 6.3688e-04 - val_loss: 4.9391e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 3.9102e-04 - val_loss: 4.0392e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 3.1593e-04 - val_loss: 2.1369e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 553us/step - loss: 2.6386e-04 - val_loss: 1.8166e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 2.8722e-04 - val_loss: 1.6153e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 555us/step - loss: 1.9579e-04 - val_loss: 1.3866e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 2.3667e-04 - val_loss: 1.2936e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 2.2055e-04 - val_loss: 1.3000e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 1.7345e-04 - val_loss: 1.1477e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 550us/step - loss: 1.7658e-04 - val_loss: 2.1797e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 4.2093e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 4.2154e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 2.0721e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 1.8219e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 2.4679e-04 - 297ms/epoch - 418us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 1.8854e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 2.5950e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 1.5028e-04 - 296ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 2.5348e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 2.8836e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 1.2415e-04 - 296ms/epoch - 416us/step\n",
      "178/178 - 0s - loss: 1.1481e-04 - 94ms/epoch - 526us/step\n",
      "712/712 - 0s - loss: 1.1463e-04 - 194ms/epoch - 272us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0326 - val_loss: 0.0010\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 557us/step - loss: 7.0904e-04 - val_loss: 5.7569e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 4.1455e-04 - val_loss: 2.9931e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.4257e-04 - val_loss: 2.4799e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 2.5444e-04 - val_loss: 1.9224e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 2.3977e-04 - val_loss: 1.6197e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 2.7182e-04 - val_loss: 2.0971e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 1.9989e-04 - val_loss: 1.2966e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 1.8607e-04 - val_loss: 1.2520e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 644us/step - loss: 2.4141e-04 - val_loss: 1.1613e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 1.5495e-04 - val_loss: 1.0520e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 4.8006e-04 - 300ms/epoch - 421us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 2.3915e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 2.0229e-04 - 294ms/epoch - 413us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 1.9028e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 2.5076e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 1.9583e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 4.8838e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 2.2496e-04 - 295ms/epoch - 415us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 1.3318e-04 - 293ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 2.1813e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 3.8002e-04 - 299ms/epoch - 421us/step\n",
      "178/178 - 0s - loss: 1.4502e-04 - 96ms/epoch - 541us/step\n",
      "712/712 - 0s - loss: 1.4533e-04 - 194ms/epoch - 272us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  20.2s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0322 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 7.3535e-04 - val_loss: 4.6353e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 4.7753e-04 - val_loss: 3.3062e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.3208e-04 - val_loss: 2.3276e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.7314e-04 - val_loss: 1.8640e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 2.1865e-04 - val_loss: 1.8662e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 2.0074e-04 - val_loss: 2.4064e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 2.1677e-04 - val_loss: 1.2696e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 559us/step - loss: 1.7817e-04 - val_loss: 3.2473e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 1.7444e-04 - val_loss: 1.0638e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 1.7292e-04 - val_loss: 1.0529e-04\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 5.2891e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 1.5807e-04 - 298ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 2.9088e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 1.9327e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 1.9883e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 3.9310e-04 - 295ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 1.2132e-04 - 297ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 2.0828e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 1.4572e-04 - 296ms/epoch - 416us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 2.8694e-04 - 298ms/epoch - 419us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 2.9133e-04 - 297ms/epoch - 417us/step\n",
      "178/178 - 0s - loss: 1.3043e-04 - 95ms/epoch - 536us/step\n",
      "712/712 - 0s - loss: 1.3061e-04 - 195ms/epoch - 275us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  20.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2340 - val_loss: 0.0140\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 0.0113 - val_loss: 0.0096\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 0.0084 - val_loss: 0.0076\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0069 - val_loss: 0.0063\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0061 - val_loss: 0.0059\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0056 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 0.0052 - val_loss: 0.0052\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 0.0046 - val_loss: 0.0046\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 0.0042 - val_loss: 0.0041\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0041 - 300ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0041 - 298ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0040 - 297ms/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0039 - 297ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0039 - 300ms/epoch - 421us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0039 - 295ms/epoch - 414us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0038 - 300ms/epoch - 421us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0038 - 296ms/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0037 - 299ms/epoch - 421us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0037 - 292ms/epoch - 411us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0037 - 296ms/epoch - 416us/step\n",
      "178/178 - 0s - loss: 0.0038 - 98ms/epoch - 548us/step\n",
      "712/712 - 0s - loss: 0.0038 - 200ms/epoch - 281us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  20.3s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2202 - val_loss: 0.0143\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0114 - val_loss: 0.0095\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0083 - val_loss: 0.0074\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0068 - val_loss: 0.0061\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 562us/step - loss: 0.0053 - val_loss: 0.0053\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0047 - val_loss: 0.0046\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 0.0044 - val_loss: 0.0044\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0042 - val_loss: 0.0040\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0041 - val_loss: 0.0040\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0041 - 343ms/epoch - 481us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0040 - 327ms/epoch - 459us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0039 - 474ms/epoch - 666us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0039 - 313ms/epoch - 440us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0039 - 311ms/epoch - 437us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0038 - 313ms/epoch - 439us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0038 - 331ms/epoch - 465us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0037 - 326ms/epoch - 457us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0037 - 322ms/epoch - 452us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0037 - 327ms/epoch - 459us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0037 - 318ms/epoch - 447us/step\n",
      "178/178 - 0s - loss: 0.0036 - 100ms/epoch - 564us/step\n",
      "712/712 - 0s - loss: 0.0036 - 255ms/epoch - 358us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  20.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2263 - val_loss: 0.0143\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 0.0109 - val_loss: 0.0086\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 0.0076 - val_loss: 0.0067\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 0.0062 - val_loss: 0.0056\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0053 - val_loss: 0.0048\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 0.0047 - val_loss: 0.0044\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 0.0041 - val_loss: 0.0041\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0038 - val_loss: 0.0037\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 683us/step - loss: 0.0037 - val_loss: 0.0035\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0034 - 297ms/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0034 - 296ms/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0033 - 297ms/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 419us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0031 - 296ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0031 - 295ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0031 - 302ms/epoch - 424us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0031 - 299ms/epoch - 420us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0031 - 298ms/epoch - 419us/step\n",
      "178/178 - 0s - loss: 0.0031 - 96ms/epoch - 541us/step\n",
      "712/712 - 0s - loss: 0.0030 - 195ms/epoch - 274us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  20.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2222 - val_loss: 0.0135\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0097 - val_loss: 0.0076\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0067 - val_loss: 0.0059\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0056 - val_loss: 0.0052\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 0.0044 - val_loss: 0.0043\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 0.0041 - val_loss: 0.0039\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0039 - val_loss: 0.0037\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 574us/step - loss: 0.0037 - val_loss: 0.0036\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 0.0035 - val_loss: 0.0034\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0034 - val_loss: 0.0034\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0034 - 301ms/epoch - 423us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0033 - 298ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 418us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0032 - 298ms/epoch - 419us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0032 - 297ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0031 - 297ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0031 - 297ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0031 - 316ms/epoch - 444us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0030 - 298ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0030 - 299ms/epoch - 419us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0030 - 296ms/epoch - 415us/step\n",
      "178/178 - 0s - loss: 0.0029 - 101ms/epoch - 565us/step\n",
      "712/712 - 0s - loss: 0.0029 - 196ms/epoch - 275us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  20.1s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.2229 - val_loss: 0.0143\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 0.0114 - val_loss: 0.0093\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0082 - val_loss: 0.0071\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 0.0067 - val_loss: 0.0062\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 0.0059 - val_loss: 0.0056\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 684us/step - loss: 0.0054 - val_loss: 0.0050\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 0.0044 - val_loss: 0.0042\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 0.0042 - val_loss: 0.0042\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 0.0040 - val_loss: 0.0041\n",
      "Epoch 1/11\n",
      "712/712 - 0s - loss: 0.0039 - 301ms/epoch - 422us/step\n",
      "Epoch 2/11\n",
      "712/712 - 0s - loss: 0.0039 - 298ms/epoch - 419us/step\n",
      "Epoch 3/11\n",
      "712/712 - 0s - loss: 0.0038 - 303ms/epoch - 426us/step\n",
      "Epoch 4/11\n",
      "712/712 - 0s - loss: 0.0038 - 297ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "712/712 - 0s - loss: 0.0037 - 305ms/epoch - 428us/step\n",
      "Epoch 6/11\n",
      "712/712 - 0s - loss: 0.0037 - 298ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "712/712 - 0s - loss: 0.0036 - 300ms/epoch - 422us/step\n",
      "Epoch 8/11\n",
      "712/712 - 0s - loss: 0.0036 - 298ms/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "712/712 - 0s - loss: 0.0035 - 297ms/epoch - 417us/step\n",
      "Epoch 10/11\n",
      "712/712 - 0s - loss: 0.0035 - 297ms/epoch - 417us/step\n",
      "Epoch 11/11\n",
      "712/712 - 0s - loss: 0.0035 - 297ms/epoch - 417us/step\n",
      "178/178 - 0s - loss: 0.0034 - 96ms/epoch - 539us/step\n",
      "712/712 - 0s - loss: 0.0034 - 191ms/epoch - 269us/step\n",
      "[CV] END ..........activation=relu, kernel_regularizer=l1_l2; total time=  20.2s\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 2\n",
      "n_resources: 85356\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0332 - val_loss: 9.4576e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 9.0080e-04 - val_loss: 7.7306e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 6.8889e-04 - val_loss: 5.6343e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 5.8981e-04 - val_loss: 4.8156e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 5.2662e-04 - val_loss: 4.0342e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 4.8183e-04 - val_loss: 3.6623e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 4.4025e-04 - val_loss: 7.4203e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 4.2943e-04 - val_loss: 3.2246e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 4.1148e-04 - val_loss: 6.0901e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 3.7602e-04 - val_loss: 8.3260e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 3.7291e-04 - val_loss: 5.0993e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 4.3763e-04 - 925ms/epoch - 434us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 4.2851e-04 - 910ms/epoch - 426us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 4.8184e-04 - 915ms/epoch - 429us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 4.1863e-04 - 990ms/epoch - 464us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 4.1567e-04 - 942ms/epoch - 441us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 4.2803e-04 - 985ms/epoch - 462us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 4.2921e-04 - 1s/epoch - 475us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 4.0746e-04 - 1s/epoch - 475us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 3.8655e-04 - 1s/epoch - 471us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 3.7556e-04 - 1s/epoch - 519us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 3.5293e-04 - 982ms/epoch - 460us/step\n",
      "534/534 - 0s - loss: 4.1029e-04 - 201ms/epoch - 377us/step\n",
      "2134/2134 - 1s - loss: 4.1634e-04 - 570ms/epoch - 267us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  30.8s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0336 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 9.0375e-04 - val_loss: 6.6194e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 6.7786e-04 - val_loss: 5.4726e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 6.0915e-04 - val_loss: 4.6977e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 572us/step - loss: 5.2497e-04 - val_loss: 4.0898e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 5.0529e-04 - val_loss: 4.4126e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 4.6680e-04 - val_loss: 3.5455e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 4.1490e-04 - val_loss: 3.4233e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 564us/step - loss: 4.0243e-04 - val_loss: 3.2277e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 3.9058e-04 - val_loss: 2.8240e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 3.7437e-04 - val_loss: 3.2627e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 4.7194e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 4.2167e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 4.1726e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 4.0571e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 3.9772e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 3.9715e-04 - 878ms/epoch - 412us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 4.0485e-04 - 885ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 3.6967e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 4.2244e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 3.9096e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 4.0474e-04 - 881ms/epoch - 413us/step\n",
      "534/534 - 0s - loss: 2.7850e-04 - 184ms/epoch - 344us/step\n",
      "2134/2134 - 1s - loss: 2.7605e-04 - 564ms/epoch - 264us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0331 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 8.6454e-04 - val_loss: 0.0012\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 573us/step - loss: 6.8417e-04 - val_loss: 6.6575e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 5.8319e-04 - val_loss: 5.1722e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 5.2906e-04 - val_loss: 5.6679e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.7108e-04 - val_loss: 3.7200e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 571us/step - loss: 4.6181e-04 - val_loss: 7.9946e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 4.5653e-04 - val_loss: 0.0020\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 4.0500e-04 - val_loss: 5.8176e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.9163e-04 - val_loss: 3.8097e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 3.7678e-04 - val_loss: 3.8333e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 4.3263e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 4.3314e-04 - 889ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 4.5591e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 3.9844e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 4.3139e-04 - 879ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 3.9478e-04 - 778ms/epoch - 365us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 3.8076e-04 - 997ms/epoch - 467us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 3.8493e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 3.8185e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 3.9203e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 3.4619e-04 - 878ms/epoch - 412us/step\n",
      "534/534 - 0s - loss: 2.6725e-04 - 189ms/epoch - 353us/step\n",
      "2134/2134 - 1s - loss: 2.6388e-04 - 563ms/epoch - 264us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.4s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0348 - val_loss: 0.0010\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 9.1918e-04 - val_loss: 6.8974e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 7.0995e-04 - val_loss: 6.2975e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 5.7411e-04 - val_loss: 6.2511e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 5.2354e-04 - val_loss: 4.2370e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 4.9572e-04 - val_loss: 4.4147e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 4.5787e-04 - val_loss: 3.3873e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 4.3048e-04 - val_loss: 3.3775e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 4.3009e-04 - val_loss: 3.0359e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 585us/step - loss: 3.9431e-04 - val_loss: 2.8968e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 3.9345e-04 - val_loss: 3.7257e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 5.0568e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 4.4910e-04 - 891ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 4.2978e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 4.0479e-04 - 887ms/epoch - 416us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 4.1189e-04 - 910ms/epoch - 427us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 4.1193e-04 - 875ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 3.7916e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 4.0077e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 3.6849e-04 - 875ms/epoch - 410us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 4.1096e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 3.3659e-04 - 880ms/epoch - 412us/step\n",
      "534/534 - 0s - loss: 2.5298e-04 - 196ms/epoch - 366us/step\n",
      "2134/2134 - 1s - loss: 2.5544e-04 - 569ms/epoch - 266us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0356 - val_loss: 0.0010\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 9.0866e-04 - val_loss: 7.1311e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 584us/step - loss: 6.9605e-04 - val_loss: 5.6886e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 5.7912e-04 - val_loss: 6.6290e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 5.4080e-04 - val_loss: 4.0904e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 561us/step - loss: 4.7175e-04 - val_loss: 6.9301e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 4.3508e-04 - val_loss: 6.0479e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 4.2928e-04 - val_loss: 3.1547e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 4.1470e-04 - val_loss: 3.1815e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 3.7228e-04 - val_loss: 2.7999e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 565us/step - loss: 3.9495e-04 - val_loss: 3.0173e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 4.7766e-04 - 1s/epoch - 474us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 4.4584e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 4.4520e-04 - 885ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 4.4207e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 4.2817e-04 - 981ms/epoch - 460us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 4.1652e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 4.2131e-04 - 891ms/epoch - 417us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 3.6114e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 3.8230e-04 - 903ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 3.8390e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 3.7573e-04 - 891ms/epoch - 417us/step\n",
      "534/534 - 0s - loss: 2.6077e-04 - 186ms/epoch - 347us/step\n",
      "2134/2134 - 1s - loss: 2.6217e-04 - 569ms/epoch - 267us/step\n",
      "[CV] END .............activation=tanh, kernel_regularizer=l2; total time=  29.7s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0314 - val_loss: 9.2266e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 6.7440e-04 - val_loss: 4.7617e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 4.0926e-04 - val_loss: 3.2268e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 580us/step - loss: 3.5149e-04 - val_loss: 3.8808e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 2.4773e-04 - val_loss: 1.7979e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 2.4980e-04 - val_loss: 1.5248e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 570us/step - loss: 2.0512e-04 - val_loss: 1.3627e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 599us/step - loss: 3.6028e-04 - val_loss: 1.4219e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 578us/step - loss: 1.8905e-04 - val_loss: 1.2696e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 1.8132e-04 - val_loss: 1.1587e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 1.6700e-04 - val_loss: 1.1511e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 4.4893e-04 - 891ms/epoch - 417us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 2.2563e-04 - 879ms/epoch - 412us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 2.7128e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 2.1193e-04 - 876ms/epoch - 410us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 2.2582e-04 - 878ms/epoch - 411us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 2.1097e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 2.2673e-04 - 874ms/epoch - 410us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 2.0510e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 1.8128e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 2.0306e-04 - 871ms/epoch - 408us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 2.1355e-04 - 865ms/epoch - 405us/step\n",
      "534/534 - 0s - loss: 1.0188e-04 - 180ms/epoch - 338us/step\n",
      "2134/2134 - 1s - loss: 1.0194e-04 - 568ms/epoch - 266us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  29.5s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0305 - val_loss: 9.5534e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 6.7932e-04 - val_loss: 4.3769e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 566us/step - loss: 4.6024e-04 - val_loss: 3.1830e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 594us/step - loss: 2.9052e-04 - val_loss: 2.8184e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 569us/step - loss: 2.9287e-04 - val_loss: 1.8455e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 595us/step - loss: 2.6273e-04 - val_loss: 1.6037e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 583us/step - loss: 2.4252e-04 - val_loss: 1.7377e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 1.9653e-04 - val_loss: 0.0016\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 2.1285e-04 - val_loss: 1.2312e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 567us/step - loss: 1.6875e-04 - val_loss: 1.1688e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 588us/step - loss: 1.8395e-04 - val_loss: 1.0452e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 3.1679e-04 - 998ms/epoch - 467us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 2.4474e-04 - 884ms/epoch - 414us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 2.4188e-04 - 885ms/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 2.1241e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 2.0995e-04 - 880ms/epoch - 412us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 2.5229e-04 - 870ms/epoch - 408us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 1.9474e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 1.7539e-04 - 876ms/epoch - 411us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 2.3556e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 1.8352e-04 - 879ms/epoch - 412us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 1.8427e-04 - 873ms/epoch - 409us/step\n",
      "534/534 - 0s - loss: 9.7540e-05 - 186ms/epoch - 348us/step\n",
      "2134/2134 - 1s - loss: 9.7308e-05 - 561ms/epoch - 263us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  29.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0300 - val_loss: 8.7217e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 6.2538e-04 - val_loss: 4.6504e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 3.8211e-04 - val_loss: 2.6462e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 2.8475e-04 - val_loss: 2.6786e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 2.8722e-04 - val_loss: 2.0230e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 2.5482e-04 - val_loss: 1.5840e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 800us/step - loss: 2.2166e-04 - val_loss: 1.3938e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 716us/step - loss: 1.9313e-04 - val_loss: 1.2265e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 635us/step - loss: 2.1757e-04 - val_loss: 1.1791e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 1.7654e-04 - val_loss: 1.2626e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 754us/step - loss: 1.9301e-04 - val_loss: 1.0516e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 3.2635e-04 - 977ms/epoch - 458us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 2.3660e-04 - 889ms/epoch - 417us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 2.4976e-04 - 877ms/epoch - 411us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 2.3117e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 2.8073e-04 - 890ms/epoch - 417us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 1.5622e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 2.2429e-04 - 881ms/epoch - 413us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 2.1182e-04 - 883ms/epoch - 414us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 2.3628e-04 - 882ms/epoch - 413us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 2.4392e-04 - 885ms/epoch - 415us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 1.9490e-04 - 889ms/epoch - 417us/step\n",
      "534/534 - 0s - loss: 1.2774e-04 - 192ms/epoch - 360us/step\n",
      "2134/2134 - 1s - loss: 1.2759e-04 - 564ms/epoch - 264us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.6s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0315 - val_loss: 0.0011\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 7.8834e-04 - val_loss: 5.6815e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 603us/step - loss: 4.6124e-04 - val_loss: 3.2741e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 3.2467e-04 - val_loss: 2.3242e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 596us/step - loss: 3.3090e-04 - val_loss: 1.9021e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 2.8407e-04 - val_loss: 1.6669e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 581us/step - loss: 2.2437e-04 - val_loss: 1.5466e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 2.7033e-04 - val_loss: 1.6320e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 604us/step - loss: 2.4442e-04 - val_loss: 1.2988e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 587us/step - loss: 2.2842e-04 - val_loss: 1.2202e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 2.0882e-04 - val_loss: 1.1478e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 5.1024e-04 - 917ms/epoch - 430us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 2.5158e-04 - 954ms/epoch - 447us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 2.7099e-04 - 986ms/epoch - 462us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 2.4137e-04 - 899ms/epoch - 421us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 3.4080e-04 - 896ms/epoch - 420us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 1.9598e-04 - 893ms/epoch - 418us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 2.0641e-04 - 897ms/epoch - 420us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 2.6451e-04 - 893ms/epoch - 418us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 1.8996e-04 - 902ms/epoch - 423us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 2.1947e-04 - 892ms/epoch - 418us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 2.3279e-04 - 895ms/epoch - 420us/step\n",
      "534/534 - 0s - loss: 1.2229e-04 - 186ms/epoch - 349us/step\n",
      "2134/2134 - 1s - loss: 1.2208e-04 - 567ms/epoch - 265us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.0s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0310 - val_loss: 9.3214e-04\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 598us/step - loss: 6.4253e-04 - val_loss: 4.4593e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 576us/step - loss: 3.7803e-04 - val_loss: 2.8435e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 593us/step - loss: 3.0362e-04 - val_loss: 2.7844e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 597us/step - loss: 2.3109e-04 - val_loss: 1.9081e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 579us/step - loss: 2.8845e-04 - val_loss: 1.5663e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 590us/step - loss: 2.3454e-04 - val_loss: 3.7743e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 575us/step - loss: 2.3882e-04 - val_loss: 1.3278e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 1.9880e-04 - val_loss: 1.3008e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 591us/step - loss: 2.1777e-04 - val_loss: 1.2584e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 582us/step - loss: 2.1553e-04 - val_loss: 1.3032e-04\n",
      "Epoch 1/11\n",
      "2134/2134 - 1s - loss: 3.4196e-04 - 922ms/epoch - 432us/step\n",
      "Epoch 2/11\n",
      "2134/2134 - 1s - loss: 3.0673e-04 - 929ms/epoch - 436us/step\n",
      "Epoch 3/11\n",
      "2134/2134 - 1s - loss: 3.3728e-04 - 976ms/epoch - 457us/step\n",
      "Epoch 4/11\n",
      "2134/2134 - 1s - loss: 2.1733e-04 - 1s/epoch - 576us/step\n",
      "Epoch 5/11\n",
      "2134/2134 - 1s - loss: 2.5708e-04 - 1s/epoch - 516us/step\n",
      "Epoch 6/11\n",
      "2134/2134 - 1s - loss: 2.0262e-04 - 909ms/epoch - 426us/step\n",
      "Epoch 7/11\n",
      "2134/2134 - 1s - loss: 2.1115e-04 - 886ms/epoch - 415us/step\n",
      "Epoch 8/11\n",
      "2134/2134 - 1s - loss: 2.0355e-04 - 895ms/epoch - 420us/step\n",
      "Epoch 9/11\n",
      "2134/2134 - 1s - loss: 2.6146e-04 - 895ms/epoch - 419us/step\n",
      "Epoch 10/11\n",
      "2134/2134 - 1s - loss: 2.0982e-04 - 986ms/epoch - 462us/step\n",
      "Epoch 11/11\n",
      "2134/2134 - 1s - loss: 2.2498e-04 - 945ms/epoch - 443us/step\n",
      "534/534 - 0s - loss: 1.1348e-04 - 198ms/epoch - 372us/step\n",
      "2134/2134 - 1s - loss: 1.1344e-04 - 598ms/epoch - 280us/step\n",
      "[CV] END .............activation=relu, kernel_regularizer=l2; total time=  30.9s\n",
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0327 - val_loss: 0.0010\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 592us/step - loss: 6.8995e-04 - val_loss: 4.5561e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 602us/step - loss: 3.9343e-04 - val_loss: 4.2863e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 600us/step - loss: 3.4133e-04 - val_loss: 2.5157e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 628us/step - loss: 2.6128e-04 - val_loss: 3.3242e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 643us/step - loss: 2.5546e-04 - val_loss: 1.6616e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 612us/step - loss: 2.4423e-04 - val_loss: 1.5295e-04\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 589us/step - loss: 1.7450e-04 - val_loss: 1.4963e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 608us/step - loss: 2.4823e-04 - val_loss: 1.2927e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 621us/step - loss: 1.8897e-04 - val_loss: 1.5776e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 586us/step - loss: 1.6812e-04 - val_loss: 1.1492e-04\n",
      "Epoch 1/11\n",
      "2668/2668 - 1s - loss: 3.2241e-04 - 1s/epoch - 415us/step\n",
      "Epoch 2/11\n",
      "2668/2668 - 1s - loss: 2.4416e-04 - 1s/epoch - 416us/step\n",
      "Epoch 3/11\n",
      "2668/2668 - 1s - loss: 2.2000e-04 - 1s/epoch - 415us/step\n",
      "Epoch 4/11\n",
      "2668/2668 - 1s - loss: 2.4678e-04 - 1s/epoch - 415us/step\n",
      "Epoch 5/11\n",
      "2668/2668 - 1s - loss: 2.3255e-04 - 1s/epoch - 414us/step\n",
      "Epoch 6/11\n",
      "2668/2668 - 1s - loss: 1.9736e-04 - 1s/epoch - 416us/step\n",
      "Epoch 7/11\n",
      "2668/2668 - 1s - loss: 2.1444e-04 - 1s/epoch - 418us/step\n",
      "Epoch 8/11\n",
      "2668/2668 - 1s - loss: 2.3059e-04 - 1s/epoch - 416us/step\n",
      "Epoch 9/11\n",
      "2668/2668 - 1s - loss: 2.1962e-04 - 1s/epoch - 427us/step\n",
      "Epoch 10/11\n",
      "2668/2668 - 1s - loss: 1.8202e-04 - 1s/epoch - 426us/step\n",
      "Epoch 11/11\n",
      "2668/2668 - 1s - loss: 1.7153e-04 - 1s/epoch - 415us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32f3b2610&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HalvingGridSearchCV</label><div class=\"sk-toggleable__content\"><pre>HalvingGridSearchCV(error_score=&#x27;raise&#x27;,\n",
       "                    estimator=&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32f3b2610&gt;,\n",
       "                    param_grid={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                &#x27;kernel_regularizer&#x27;: [&#x27;l1&#x27;, &#x27;l2&#x27;, &#x27;l1_l2&#x27;]},\n",
       "                    verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32f3b2610&gt;</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KerasRegressor</label><div class=\"sk-toggleable__content\"><pre>&lt;keras.wrappers.scikit_learn.KerasRegressor object at 0x32f3b2610&gt;</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "HalvingGridSearchCV(error_score='raise',\n",
       "                    estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x32f3b2610>,\n",
       "                    param_grid={'activation': ['tanh', 'relu'],\n",
       "                                'kernel_regularizer': ['l1', 'l2', 'l1_l2']},\n",
       "                    verbose=2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grid search selected hyperparameters\n",
    "\n",
    "# define base model function for grid search\n",
    "def create_model(activation='relu', kernel_regularizer='l2'):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "    model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'kernel_regularizer': ['l1', 'l2', 'l1_l2']\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=early_stop.stopped_epoch, verbose=2)\n",
    "\n",
    "# perform grid search hyperparameter tuning\n",
    "grid_search = HalvingGridSearchCV(estimator=keras_reg, param_grid=param_dist, cv=5, verbose=2, error_score='raise')\n",
    "grid_search.fit(Xtrain, ytrain, sample_weight=wtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>n_resources</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_kernel_regularizer</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.950220</td>\n",
       "      <td>0.452401</td>\n",
       "      <td>0.116844</td>\n",
       "      <td>0.006168</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.005451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005191</td>\n",
       "      <td>0.000244</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>-0.005051</td>\n",
       "      <td>-0.005458</td>\n",
       "      <td>-0.005075</td>\n",
       "      <td>-0.005014</td>\n",
       "      <td>-0.005211</td>\n",
       "      <td>0.000202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.649533</td>\n",
       "      <td>0.482654</td>\n",
       "      <td>0.115406</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000814</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>4</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>-0.000607</td>\n",
       "      <td>-0.001662</td>\n",
       "      <td>-0.000735</td>\n",
       "      <td>-0.000277</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.000460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.125970</td>\n",
       "      <td>0.082518</td>\n",
       "      <td>0.114232</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.005114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005273</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.005112</td>\n",
       "      <td>-0.005425</td>\n",
       "      <td>-0.005275</td>\n",
       "      <td>-0.005330</td>\n",
       "      <td>-0.005281</td>\n",
       "      <td>-0.005285</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.700405</td>\n",
       "      <td>0.421137</td>\n",
       "      <td>0.120795</td>\n",
       "      <td>0.012812</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.003319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003156</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.003300</td>\n",
       "      <td>-0.003653</td>\n",
       "      <td>-0.003035</td>\n",
       "      <td>-0.002761</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.003147</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.469185</td>\n",
       "      <td>0.573452</td>\n",
       "      <td>0.109322</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.000154</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000115</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.000131</td>\n",
       "      <td>-0.000138</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>28452</td>\n",
       "      <td>20.246628</td>\n",
       "      <td>0.202227</td>\n",
       "      <td>0.111645</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>relu</td>\n",
       "      <td>l1_l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003355</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.003803</td>\n",
       "      <td>-0.003649</td>\n",
       "      <td>-0.003020</td>\n",
       "      <td>-0.002915</td>\n",
       "      <td>-0.003420</td>\n",
       "      <td>-0.003362</td>\n",
       "      <td>0.000345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>29.615367</td>\n",
       "      <td>0.477214</td>\n",
       "      <td>0.205791</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>tanh</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'tanh', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000294</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>-0.000276</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.000255</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>-0.000295</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>85356</td>\n",
       "      <td>29.898158</td>\n",
       "      <td>0.558511</td>\n",
       "      <td>0.202779</td>\n",
       "      <td>0.006975</td>\n",
       "      <td>relu</td>\n",
       "      <td>l2</td>\n",
       "      <td>{'activation': 'relu', 'kernel_regularizer': '...</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.000128</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000112</td>\n",
       "      <td>0.000012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "0     0        28452      20.950220      0.452401         0.116844   \n",
       "1     0        28452      20.649533      0.482654         0.115406   \n",
       "2     0        28452      20.125970      0.082518         0.114232   \n",
       "3     0        28452      20.700405      0.421137         0.120795   \n",
       "4     0        28452      20.469185      0.573452         0.109322   \n",
       "5     0        28452      20.246628      0.202227         0.111645   \n",
       "6     1        85356      29.615367      0.477214         0.205791   \n",
       "7     1        85356      29.898158      0.558511         0.202779   \n",
       "\n",
       "   std_score_time param_activation param_kernel_regularizer  \\\n",
       "0        0.006168             tanh                       l1   \n",
       "1        0.004933             tanh                       l2   \n",
       "2        0.001357             tanh                    l1_l2   \n",
       "3        0.012812             relu                       l1   \n",
       "4        0.001294             relu                       l2   \n",
       "5        0.002266             relu                    l1_l2   \n",
       "6        0.007126             tanh                       l2   \n",
       "7        0.006975             relu                       l2   \n",
       "\n",
       "                                              params  split0_test_score  ...  \\\n",
       "0  {'activation': 'tanh', 'kernel_regularizer': '...          -0.005451  ...   \n",
       "1  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000767  ...   \n",
       "2  {'activation': 'tanh', 'kernel_regularizer': '...          -0.005114  ...   \n",
       "3  {'activation': 'relu', 'kernel_regularizer': '...          -0.003319  ...   \n",
       "4  {'activation': 'relu', 'kernel_regularizer': '...          -0.000154  ...   \n",
       "5  {'activation': 'relu', 'kernel_regularizer': '...          -0.003819  ...   \n",
       "6  {'activation': 'tanh', 'kernel_regularizer': '...          -0.000410  ...   \n",
       "7  {'activation': 'relu', 'kernel_regularizer': '...          -0.000102  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0        -0.005191        0.000244                7           -0.005458   \n",
       "1        -0.000814        0.000456                4           -0.000762   \n",
       "2        -0.005273        0.000120                8           -0.005112   \n",
       "3        -0.003156        0.000313                5           -0.003300   \n",
       "4        -0.000138        0.000014                2           -0.000154   \n",
       "5        -0.003355        0.000345                6           -0.003803   \n",
       "6        -0.000294        0.000059                3           -0.000416   \n",
       "7        -0.000113        0.000012                1           -0.000102   \n",
       "\n",
       "   split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0           -0.005051           -0.005458           -0.005075   \n",
       "1           -0.000607           -0.001662           -0.000735   \n",
       "2           -0.005425           -0.005275           -0.005330   \n",
       "3           -0.003653           -0.003035           -0.002761   \n",
       "4           -0.000145           -0.000115           -0.000145   \n",
       "5           -0.003649           -0.003020           -0.002915   \n",
       "6           -0.000276           -0.000264           -0.000255   \n",
       "7           -0.000097           -0.000128           -0.000122   \n",
       "\n",
       "   split4_train_score  mean_train_score  std_train_score  \n",
       "0           -0.005014         -0.005211         0.000202  \n",
       "1           -0.000277         -0.000809         0.000460  \n",
       "2           -0.005281         -0.005285         0.000102  \n",
       "3           -0.002986         -0.003147         0.000306  \n",
       "4           -0.000131         -0.000138         0.000014  \n",
       "5           -0.003420         -0.003362         0.000345  \n",
       "6           -0.000262         -0.000295         0.000061  \n",
       "7           -0.000113         -0.000112         0.000012  \n",
       "\n",
       "[8 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_df = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "display(grid_search_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'activation': 'relu', 'kernel_regularizer': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "1334/1334 [==============================] - 2s 2ms/step - loss: 0.0336 - val_loss: 0.0010\n",
      "Epoch 2/11\n",
      "1334/1334 [==============================] - 1s 563us/step - loss: 6.8210e-04 - val_loss: 4.9702e-04\n",
      "Epoch 3/11\n",
      "1334/1334 [==============================] - 1s 545us/step - loss: 4.0508e-04 - val_loss: 4.5743e-04\n",
      "Epoch 4/11\n",
      "1334/1334 [==============================] - 1s 547us/step - loss: 3.2358e-04 - val_loss: 2.1312e-04\n",
      "Epoch 5/11\n",
      "1334/1334 [==============================] - 1s 544us/step - loss: 2.8813e-04 - val_loss: 2.1988e-04\n",
      "Epoch 6/11\n",
      "1334/1334 [==============================] - 1s 558us/step - loss: 2.2055e-04 - val_loss: 1.8436e-04\n",
      "Epoch 7/11\n",
      "1334/1334 [==============================] - 1s 554us/step - loss: 1.7595e-04 - val_loss: 0.0011\n",
      "Epoch 8/11\n",
      "1334/1334 [==============================] - 1s 543us/step - loss: 2.2542e-04 - val_loss: 1.3764e-04\n",
      "Epoch 9/11\n",
      "1334/1334 [==============================] - 1s 577us/step - loss: 2.1667e-04 - val_loss: 1.3593e-04\n",
      "Epoch 10/11\n",
      "1334/1334 [==============================] - 1s 568us/step - loss: 1.6674e-04 - val_loss: 1.0885e-04\n",
      "Epoch 11/11\n",
      "1334/1334 [==============================] - 1s 601us/step - loss: 1.6693e-04 - val_loss: 1.1224e-04\n"
     ]
    }
   ],
   "source": [
    "# evaluate best model\n",
    "best_model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "    Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta), weighted_metrics=[])\n",
    "\n",
    "history = best_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=64, validation_data=(Xval, yval, wval))\n",
    "\n",
    "# save the best model\n",
    "best_model.save(\"models/best_dnn_model_processed_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/best_dnn_model_processed_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training Huber Loss: 0.00016692593635525554\n",
      "Best Validation Huber Loss: 0.00011223854380659759\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_train_hl = best_model.history.history[\"loss\"][-1]\n",
    "best_val_hl = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training Huber Loss: {best_train_hl}\")\n",
    "print(f\"Best Validation Huber Loss: {best_val_hl}\")\n",
    "\n",
    "plt.plot(best_model.history.history['loss'], label='Training Huber Loss')\n",
    "plt.plot(best_model.history.history['val_loss'], label='Validation Huber Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Huber Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 64-bit ('tf-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff044f01a3488bc48e7d3679a79037bfca5d9a382741a26c6ac0d6891b8e9545"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
