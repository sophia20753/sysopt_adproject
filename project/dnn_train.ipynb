{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model Training\n",
    "\n",
    "This notebook achieves the following:\n",
    "\n",
    "1. Prepares the generated training data for model training.\n",
    "2. Trains various model architectures.\n",
    "3. Plots training results to determine best model architecture. \n",
    "4. Saves best DNN model for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, RandomizedSearchCV\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# import relevant libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber, mean_squared_error\n",
    "from tensorflow.keras.callbacks import EarlyStopping, History\n",
    "import matplotlib.pyplot as plt\n",
    "from project_functions import save_data_to_csv\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>steering_angle</th>\n",
       "      <th>distance_to_road_center</th>\n",
       "      <th>angle_from_straight_in_rads</th>\n",
       "      <th>reward</th>\n",
       "      <th>total_reward</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.017271</td>\n",
       "      <td>-1.449541</td>\n",
       "      <td>-1.197695</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.282823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.017271</td>\n",
       "      <td>-1.449541</td>\n",
       "      <td>-1.197695</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.282576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.017271</td>\n",
       "      <td>-1.449541</td>\n",
       "      <td>-1.197695</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.282330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.017271</td>\n",
       "      <td>-1.449541</td>\n",
       "      <td>-1.197695</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>-0.282083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.017271</td>\n",
       "      <td>-1.449541</td>\n",
       "      <td>-1.197695</td>\n",
       "      <td>0.022279</td>\n",
       "      <td>-0.281837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124995</th>\n",
       "      <td>0.144081</td>\n",
       "      <td>0.147693</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>-3.835282</td>\n",
       "      <td>-4.523919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124996</th>\n",
       "      <td>0.142465</td>\n",
       "      <td>0.148484</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>-3.813875</td>\n",
       "      <td>-4.527890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124997</th>\n",
       "      <td>0.140811</td>\n",
       "      <td>0.149215</td>\n",
       "      <td>0.001745</td>\n",
       "      <td>-3.792289</td>\n",
       "      <td>-4.531838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124998</th>\n",
       "      <td>0.139120</td>\n",
       "      <td>0.149889</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>-3.770526</td>\n",
       "      <td>-4.535762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124999</th>\n",
       "      <td>0.137394</td>\n",
       "      <td>0.150505</td>\n",
       "      <td>-0.006668</td>\n",
       "      <td>-0.201773</td>\n",
       "      <td>-4.535762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        steering_angle  distance_to_road_center  angle_from_straight_in_rads  \\\n",
       "0            -2.017271                -1.449541                    -1.197695   \n",
       "1            -2.017271                -1.449541                    -1.197695   \n",
       "2            -2.017271                -1.449541                    -1.197695   \n",
       "3            -2.017271                -1.449541                    -1.197695   \n",
       "4            -2.017271                -1.449541                    -1.197695   \n",
       "...                ...                      ...                          ...   \n",
       "124995        0.144081                 0.147693                     0.010285   \n",
       "124996        0.142465                 0.148484                     0.006000   \n",
       "124997        0.140811                 0.149215                     0.001745   \n",
       "124998        0.139120                 0.149889                    -0.002478   \n",
       "124999        0.137394                 0.150505                    -0.006668   \n",
       "\n",
       "          reward  total_reward  \n",
       "0       0.022491     -0.282823  \n",
       "1       0.022491     -0.282576  \n",
       "2       0.022491     -0.282330  \n",
       "3       0.022491     -0.282083  \n",
       "4       0.022279     -0.281837  \n",
       "...          ...           ...  \n",
       "124995 -3.835282     -4.523919  \n",
       "124996 -3.813875     -4.527890  \n",
       "124997 -3.792289     -4.531838  \n",
       "124998 -3.770526     -4.535762  \n",
       "124999 -0.201773     -4.535762  \n",
       "\n",
       "[125000 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sophi\\anaconda3\\envs\\ad_project\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# import data from csv to dataframe\n",
    "filename = \"raw_training_data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "display(df)\n",
    "\n",
    "# split into input and target features\n",
    "X = df[['distance_to_road_center', 'angle_from_straight_in_rads']].values\n",
    "y = df['steering_angle'].values\n",
    "r = df['reward'].values\n",
    "\n",
    "# normalize rewards to [0, 1] range\n",
    "norm_r = (r - np.min(r)) / (np.max(r) - np.min(r))\n",
    "\n",
    "# Calculate weights based on normalized rewards\n",
    "weights = np.where(r < 0, 1 / (1 - norm_r), 1 / (1 + norm_r))\n",
    "\n",
    "Xtrain, Xval, ytrain, yval, wtrain, wval = train_test_split(X, y, weights, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model Training\n",
    "\n",
    "Base Model Hyperparameters:\n",
    "- Activation functions: ReLu\n",
    "- Optimizer: Adam\n",
    "    - Learning Rate: 0.001\n",
    "- Batch Size = 32\n",
    "- Loss Function: Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "   1/3125 [..............................] - ETA: 0s - loss: 2.8948WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_train_batch_end` time: 0.0055s). Check your callbacks.\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0263 - val_loss: 0.0021\n",
      "Epoch 2/30\n",
      "3125/3125 [==============================] - 2s 766us/step - loss: 5.1962e-04 - val_loss: 2.4100e-04\n",
      "Epoch 3/30\n",
      "3125/3125 [==============================] - 2s 708us/step - loss: 5.5770e-04 - val_loss: 1.5672e-04\n",
      "Epoch 4/30\n",
      "3125/3125 [==============================] - 2s 726us/step - loss: 3.8755e-04 - val_loss: 2.2672e-05\n",
      "Epoch 5/30\n",
      "3125/3125 [==============================] - 2s 690us/step - loss: 2.5710e-04 - val_loss: 1.3552e-04\n",
      "Epoch 6/30\n",
      "3125/3125 [==============================] - 2s 650us/step - loss: 1.8323e-04 - val_loss: 6.1668e-04\n",
      "Epoch 7/30\n",
      "3125/3125 [==============================] - 2s 693us/step - loss: 1.9857e-04 - val_loss: 4.7195e-05\n",
      "Epoch 8/30\n",
      "3125/3125 [==============================] - 2s 751us/step - loss: 1.3506e-04 - val_loss: 9.8655e-06\n",
      "Epoch 9/30\n",
      "3125/3125 [==============================] - 2s 693us/step - loss: 2.7123e-04 - val_loss: 1.5246e-05\n",
      "Epoch 10/30\n",
      "3125/3125 [==============================] - 2s 677us/step - loss: 1.6244e-04 - val_loss: 1.7128e-04\n",
      "Epoch 11/30\n",
      "3125/3125 [==============================] - 2s 732us/step - loss: 1.4335e-04 - val_loss: 4.2849e-05\n",
      "Epoch 12/30\n",
      "3125/3125 [==============================] - 2s 709us/step - loss: 1.2691e-04 - val_loss: 4.1232e-05\n",
      "Epoch 13/30\n",
      "3125/3125 [==============================] - 2s 685us/step - loss: 1.8360e-04 - val_loss: 1.2302e-05\n"
     ]
    }
   ],
   "source": [
    "# find optimal number of epochs using early stopping\n",
    "\n",
    "# define base model with default Huber loss delta = 1.0\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Output layer for steering angle prediction\n",
    "])\n",
    "\n",
    "# compile with custom loss function\n",
    "model.compile(loss=Huber(delta=1.0), optimizer=Adam(lr=0.001))\n",
    "\n",
    "# define early stopping callback\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, mode='min', restore_best_weights=True)\n",
    "\n",
    "# fit the model\n",
    "history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=30, batch_size=32, validation_data = (Xval, yval, wval), callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0267 - mae: 0.0224 - mse: 0.0095 - val_loss: 9.4628e-04 - val_mae: 0.0166 - val_mse: 3.9362e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 2s 780us/step - loss: 8.7165e-04 - mae: 0.0100 - mse: 3.2695e-04 - val_loss: 5.0341e-04 - val_mae: 0.0062 - val_mse: 1.1496e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 789us/step - loss: 5.2693e-04 - mae: 0.0089 - mse: 2.2218e-04 - val_loss: 3.0750e-04 - val_mae: 0.0058 - val_mse: 8.2185e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 781us/step - loss: 3.4809e-04 - mae: 0.0078 - mse: 1.4829e-04 - val_loss: 7.0597e-05 - val_mae: 0.0051 - val_mse: 4.4250e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 789us/step - loss: 3.5162e-04 - mae: 0.0077 - mse: 1.6933e-04 - val_loss: 5.1933e-05 - val_mae: 0.0031 - val_mse: 2.2418e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 756us/step - loss: 3.0285e-04 - mae: 0.0070 - mse: 1.5706e-04 - val_loss: 2.1971e-05 - val_mae: 0.0042 - val_mse: 2.2413e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 698us/step - loss: 2.1145e-04 - mae: 0.0057 - mse: 9.9018e-05 - val_loss: 2.8080e-05 - val_mae: 0.0024 - val_mse: 1.0217e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 717us/step - loss: 2.6147e-04 - mae: 0.0054 - mse: 1.5109e-04 - val_loss: 1.1118e-04 - val_mae: 0.0037 - val_mse: 3.2210e-05\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 814us/step - loss: 2.1074e-04 - mae: 0.0049 - mse: 8.5237e-05 - val_loss: 3.7067e-05 - val_mae: 0.0050 - val_mse: 3.7551e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 768us/step - loss: 1.4450e-04 - mae: 0.0044 - mse: 7.9606e-05 - val_loss: 1.2744e-05 - val_mae: 0.0021 - val_mse: 1.2338e-05\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 3s 835us/step - loss: 1.6217e-04 - mae: 0.0047 - mse: 7.7841e-05 - val_loss: 1.3609e-04 - val_mae: 0.0054 - val_mse: 5.8700e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 743us/step - loss: 1.3351e-04 - mae: 0.0041 - mse: 6.3010e-05 - val_loss: 7.0398e-05 - val_mae: 0.0041 - val_mse: 3.2576e-05\n",
      "Iteration 1: delta = 1.0, val_mae = 0.0040528797544538975, val_mse = 3.257569915149361e-05\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0188 - mae: 0.0252 - mse: 0.0102 - val_loss: 3.7685e-04 - val_mae: 0.0110 - val_mse: 2.1133e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 839us/step - loss: 5.2231e-04 - mae: 0.0090 - mse: 2.2646e-04 - val_loss: 2.2120e-04 - val_mae: 0.0044 - val_mse: 5.7705e-05\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 3s 830us/step - loss: 3.5890e-04 - mae: 0.0074 - mse: 1.5182e-04 - val_loss: 8.6657e-05 - val_mae: 0.0047 - val_mse: 4.5168e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 3s 879us/step - loss: 2.9388e-04 - mae: 0.0068 - mse: 1.2608e-04 - val_loss: 5.2848e-05 - val_mae: 0.0027 - val_mse: 1.5438e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 3s 805us/step - loss: 2.4239e-04 - mae: 0.0058 - mse: 1.2563e-04 - val_loss: 1.1505e-04 - val_mae: 0.0047 - val_mse: 5.1608e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 778us/step - loss: 2.2168e-04 - mae: 0.0049 - mse: 1.1838e-04 - val_loss: 4.9079e-05 - val_mae: 0.0027 - val_mse: 1.6478e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 3s 833us/step - loss: 1.9921e-04 - mae: 0.0044 - mse: 7.8657e-05 - val_loss: 1.3281e-05 - val_mae: 0.0023 - val_mse: 9.5264e-06\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 774us/step - loss: 1.5173e-04 - mae: 0.0041 - mse: 8.2081e-05 - val_loss: 1.4543e-05 - val_mae: 0.0024 - val_mse: 9.3563e-06\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 793us/step - loss: 1.3095e-04 - mae: 0.0038 - mse: 6.5337e-05 - val_loss: 1.2128e-05 - val_mae: 0.0017 - val_mse: 5.6942e-06\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 800us/step - loss: 1.3191e-04 - mae: 0.0044 - mse: 6.7744e-05 - val_loss: 1.6152e-05 - val_mae: 0.0015 - val_mse: 4.8521e-06\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 799us/step - loss: 1.1552e-04 - mae: 0.0034 - mse: 4.8545e-05 - val_loss: 1.8667e-05 - val_mae: 0.0035 - val_mse: 2.6632e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 3s 819us/step - loss: 1.2086e-04 - mae: 0.0031 - mse: 5.6280e-05 - val_loss: 5.6996e-06 - val_mae: 0.0013 - val_mse: 2.7824e-06\n",
      "Iteration 2: delta = 1.1, val_mae = 0.0012802626006305218, val_mse = 2.7824337394122267e-06\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0265 - mae: 0.0259 - mse: 0.0109 - val_loss: 1.4978e-04 - val_mae: 0.0073 - val_mse: 9.6221e-05\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 830us/step - loss: 6.8096e-04 - mae: 0.0083 - mse: 2.6209e-04 - val_loss: 5.1248e-05 - val_mae: 0.0041 - val_mse: 3.3526e-05\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 783us/step - loss: 5.5069e-04 - mae: 0.0084 - mse: 2.2582e-04 - val_loss: 1.0932e-04 - val_mae: 0.0060 - val_mse: 6.7874e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 766us/step - loss: 4.0697e-04 - mae: 0.0078 - mse: 1.7666e-04 - val_loss: 5.4415e-04 - val_mae: 0.0072 - val_mse: 1.3190e-04\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 774us/step - loss: 2.6939e-04 - mae: 0.0062 - mse: 1.1813e-04 - val_loss: 1.7055e-05 - val_mae: 0.0030 - val_mse: 1.5383e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 774us/step - loss: 2.6337e-04 - mae: 0.0052 - mse: 1.1684e-04 - val_loss: 1.8694e-04 - val_mae: 0.0080 - val_mse: 8.7099e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 759us/step - loss: 2.2375e-04 - mae: 0.0055 - mse: 1.2994e-04 - val_loss: 8.3365e-04 - val_mae: 0.0218 - val_mse: 7.2553e-04\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 762us/step - loss: 1.6899e-04 - mae: 0.0035 - mse: 7.4149e-05 - val_loss: 6.2092e-05 - val_mae: 0.0050 - val_mse: 5.3714e-05\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 759us/step - loss: 1.5755e-04 - mae: 0.0049 - mse: 9.0054e-05 - val_loss: 0.0014 - val_mae: 0.0169 - val_mse: 5.3422e-04\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 760us/step - loss: 1.4108e-04 - mae: 0.0042 - mse: 7.4059e-05 - val_loss: 1.5787e-04 - val_mae: 0.0090 - val_mse: 1.1047e-04\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 768us/step - loss: 1.6264e-04 - mae: 0.0039 - mse: 8.4647e-05 - val_loss: 5.5685e-06 - val_mae: 0.0013 - val_mse: 3.3457e-06\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 746us/step - loss: 9.1921e-05 - mae: 0.0031 - mse: 4.3472e-05 - val_loss: 5.0262e-06 - val_mae: 0.0013 - val_mse: 2.5508e-06\n",
      "Iteration 3: delta = 1.2000000000000002, val_mae = 0.0012556943111121655, val_mse = 2.5508288672426715e-06\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0321 - mae: 0.0249 - mse: 0.0104 - val_loss: 1.5289e-04 - val_mae: 0.0073 - val_mse: 1.0183e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 805us/step - loss: 8.6198e-04 - mae: 0.0109 - mse: 3.4336e-04 - val_loss: 3.7205e-04 - val_mae: 0.0089 - val_mse: 1.5503e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 3s 803us/step - loss: 4.9852e-04 - mae: 0.0083 - mse: 2.1526e-04 - val_loss: 1.1748e-04 - val_mae: 0.0060 - val_mse: 7.8959e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 747us/step - loss: 4.1197e-04 - mae: 0.0078 - mse: 1.7541e-04 - val_loss: 3.5576e-05 - val_mae: 0.0028 - val_mse: 1.5574e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 757us/step - loss: 2.7105e-04 - mae: 0.0066 - mse: 1.3253e-04 - val_loss: 1.2437e-04 - val_mae: 0.0058 - val_mse: 8.1872e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 749us/step - loss: 2.9119e-04 - mae: 0.0066 - mse: 1.3264e-04 - val_loss: 2.0947e-04 - val_mae: 0.0102 - val_mse: 1.9991e-04\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 795us/step - loss: 1.5178e-04 - mae: 0.0047 - mse: 7.6616e-05 - val_loss: 3.4535e-04 - val_mae: 0.0064 - val_mse: 8.6940e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 729us/step - loss: 2.2882e-04 - mae: 0.0057 - mse: 9.9085e-05 - val_loss: 5.3601e-04 - val_mae: 0.0094 - val_mse: 2.0467e-04\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 801us/step - loss: 1.6126e-04 - mae: 0.0048 - mse: 1.0042e-04 - val_loss: 2.3004e-05 - val_mae: 0.0023 - val_mse: 1.3020e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 809us/step - loss: 1.2728e-04 - mae: 0.0041 - mse: 6.0141e-05 - val_loss: 1.1822e-05 - val_mae: 0.0028 - val_mse: 1.3307e-05\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 789us/step - loss: 1.1523e-04 - mae: 0.0043 - mse: 7.2605e-05 - val_loss: 1.9631e-04 - val_mae: 0.0066 - val_mse: 7.9814e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 728us/step - loss: 1.6675e-04 - mae: 0.0040 - mse: 7.6042e-05 - val_loss: 2.9709e-05 - val_mae: 0.0047 - val_mse: 3.1805e-05\n",
      "Iteration 4: delta = 1.3000000000000003, val_mae = 0.004734203685075045, val_mse = 3.180505154887214e-05\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0143 - mae: 0.0229 - mse: 0.0078 - val_loss: 0.0051 - val_mae: 0.0192 - val_mse: 0.0012\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 2s 745us/step - loss: 9.4397e-04 - mae: 0.0098 - mse: 4.0530e-04 - val_loss: 5.6629e-04 - val_mae: 0.0076 - val_mse: 1.6035e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 742us/step - loss: 7.3812e-04 - mae: 0.0082 - mse: 3.7524e-04 - val_loss: 7.9188e-05 - val_mae: 0.0043 - val_mse: 3.5213e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 787us/step - loss: 4.3919e-04 - mae: 0.0071 - mse: 1.7022e-04 - val_loss: 3.6321e-05 - val_mae: 0.0039 - val_mse: 3.2168e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 3s 807us/step - loss: 2.8927e-04 - mae: 0.0066 - mse: 1.4890e-04 - val_loss: 2.6708e-05 - val_mae: 0.0036 - val_mse: 2.1533e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 765us/step - loss: 2.9225e-04 - mae: 0.0057 - mse: 1.4677e-04 - val_loss: 6.1800e-06 - val_mae: 0.0017 - val_mse: 4.8657e-06\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 783us/step - loss: 2.4053e-04 - mae: 0.0060 - mse: 1.2239e-04 - val_loss: 2.9752e-04 - val_mae: 0.0060 - val_mse: 7.3166e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 783us/step - loss: 1.6981e-04 - mae: 0.0048 - mse: 7.8080e-05 - val_loss: 1.8212e-05 - val_mae: 0.0020 - val_mse: 7.7566e-06\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 802us/step - loss: 1.8702e-04 - mae: 0.0042 - mse: 7.7386e-05 - val_loss: 6.6317e-05 - val_mae: 0.0046 - val_mse: 3.8446e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 771us/step - loss: 2.5621e-04 - mae: 0.0039 - mse: 1.2504e-04 - val_loss: 2.6680e-06 - val_mae: 0.0010 - val_mse: 2.3236e-06\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 795us/step - loss: 1.3260e-04 - mae: 0.0042 - mse: 7.2791e-05 - val_loss: 3.7183e-06 - val_mae: 0.0012 - val_mse: 2.7247e-06\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 765us/step - loss: 1.1981e-04 - mae: 0.0038 - mse: 5.6340e-05 - val_loss: 4.0421e-05 - val_mae: 0.0028 - val_mse: 1.4432e-05\n",
      "Iteration 5: delta = 1.4000000000000004, val_mae = 0.0027558112051337957, val_mse = 1.4432062926061917e-05\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0184 - mae: 0.0240 - mse: 0.0089 - val_loss: 6.7112e-04 - val_mae: 0.0083 - val_mse: 1.7434e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 872us/step - loss: 7.6071e-04 - mae: 0.0104 - mse: 3.1447e-04 - val_loss: 3.0087e-04 - val_mae: 0.0076 - val_mse: 1.3054e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 793us/step - loss: 4.9510e-04 - mae: 0.0079 - mse: 2.1604e-04 - val_loss: 2.0946e-04 - val_mae: 0.0070 - val_mse: 1.0391e-04\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 3s 816us/step - loss: 5.4612e-04 - mae: 0.0076 - mse: 2.3778e-04 - val_loss: 3.9605e-05 - val_mae: 0.0035 - val_mse: 2.3210e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 3s 877us/step - loss: 3.4931e-04 - mae: 0.0074 - mse: 1.6438e-04 - val_loss: 7.1281e-05 - val_mae: 0.0034 - val_mse: 2.5622e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 3s 853us/step - loss: 3.7342e-04 - mae: 0.0069 - mse: 1.7057e-04 - val_loss: 5.8535e-05 - val_mae: 0.0053 - val_mse: 4.7152e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 784us/step - loss: 1.6150e-04 - mae: 0.0054 - mse: 8.9586e-05 - val_loss: 2.3905e-05 - val_mae: 0.0022 - val_mse: 1.1140e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 3s 803us/step - loss: 1.6270e-04 - mae: 0.0050 - mse: 7.3959e-05 - val_loss: 6.0957e-05 - val_mae: 0.0037 - val_mse: 2.6452e-05\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 823us/step - loss: 1.6416e-04 - mae: 0.0052 - mse: 9.4049e-05 - val_loss: 6.3824e-05 - val_mae: 0.0042 - val_mse: 2.9303e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 841us/step - loss: 1.4142e-04 - mae: 0.0039 - mse: 7.3517e-05 - val_loss: 0.0021 - val_mae: 0.0148 - val_mse: 5.0029e-04\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 793us/step - loss: 1.3899e-04 - mae: 0.0041 - mse: 6.0633e-05 - val_loss: 1.3192e-04 - val_mae: 0.0069 - val_mse: 7.7740e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 3s 811us/step - loss: 1.6577e-04 - mae: 0.0043 - mse: 8.3676e-05 - val_loss: 6.3310e-06 - val_mae: 0.0016 - val_mse: 4.1904e-06\n",
      "Iteration 6: delta = 1.5000000000000004, val_mae = 0.0015621938509866595, val_mse = 4.190354502497939e-06\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0215 - mae: 0.0264 - mse: 0.0079 - val_loss: 7.2389e-04 - val_mae: 0.0081 - val_mse: 1.7681e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 802us/step - loss: 0.0013 - mae: 0.0121 - mse: 4.7698e-04 - val_loss: 4.7359e-05 - val_mae: 0.0039 - val_mse: 3.6028e-05\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 3s 816us/step - loss: 5.4380e-04 - mae: 0.0083 - mse: 2.1796e-04 - val_loss: 7.2604e-05 - val_mae: 0.0046 - val_mse: 3.8190e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 3s 842us/step - loss: 4.6828e-04 - mae: 0.0085 - mse: 2.0117e-04 - val_loss: 3.8682e-05 - val_mae: 0.0035 - val_mse: 2.4171e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 790us/step - loss: 4.9581e-04 - mae: 0.0073 - mse: 1.7605e-04 - val_loss: 4.4115e-04 - val_mae: 0.0096 - val_mse: 2.1586e-04\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 3s 810us/step - loss: 2.3099e-04 - mae: 0.0062 - mse: 1.3627e-04 - val_loss: 5.3732e-05 - val_mae: 0.0031 - val_mse: 2.0143e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 780us/step - loss: 2.2531e-04 - mae: 0.0056 - mse: 1.0975e-04 - val_loss: 4.5166e-04 - val_mae: 0.0101 - val_mse: 2.1361e-04\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 782us/step - loss: 2.7320e-04 - mae: 0.0056 - mse: 1.1516e-04 - val_loss: 5.8892e-06 - val_mae: 0.0016 - val_mse: 4.6273e-06\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 777us/step - loss: 1.1478e-04 - mae: 0.0042 - mse: 6.4566e-05 - val_loss: 2.2862e-05 - val_mae: 0.0026 - val_mse: 1.1541e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 751us/step - loss: 1.7308e-04 - mae: 0.0047 - mse: 9.1792e-05 - val_loss: 4.4658e-06 - val_mae: 0.0014 - val_mse: 3.8278e-06\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 753us/step - loss: 1.1917e-04 - mae: 0.0035 - mse: 5.8563e-05 - val_loss: 2.2314e-06 - val_mae: 9.9341e-04 - val_mse: 1.8954e-06\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 744us/step - loss: 1.5548e-04 - mae: 0.0046 - mse: 7.7529e-05 - val_loss: 8.3338e-05 - val_mae: 0.0048 - val_mse: 3.4672e-05\n",
      "Iteration 7: delta = 1.6000000000000005, val_mae = 0.004795899149030447, val_mse = 3.4672262700041756e-05\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0284 - mae: 0.0248 - mse: 0.0095 - val_loss: 2.7878e-04 - val_mae: 0.0084 - val_mse: 1.2638e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 803us/step - loss: 7.7063e-04 - mae: 0.0099 - mse: 2.9171e-04 - val_loss: 1.8817e-04 - val_mae: 0.0082 - val_mse: 1.4510e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 775us/step - loss: 5.8577e-04 - mae: 0.0091 - mse: 2.4479e-04 - val_loss: 5.0678e-04 - val_mae: 0.0086 - val_mse: 1.7535e-04\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 775us/step - loss: 3.2974e-04 - mae: 0.0072 - mse: 1.4915e-04 - val_loss: 2.3738e-04 - val_mae: 0.0068 - val_mse: 9.0237e-05\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 748us/step - loss: 4.5078e-04 - mae: 0.0075 - mse: 1.7758e-04 - val_loss: 2.1626e-05 - val_mae: 0.0029 - val_mse: 1.6609e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 744us/step - loss: 2.0491e-04 - mae: 0.0059 - mse: 1.0574e-04 - val_loss: 4.4855e-05 - val_mae: 0.0026 - val_mse: 1.4235e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 3s 812us/step - loss: 2.5345e-04 - mae: 0.0051 - mse: 9.5685e-05 - val_loss: 7.5672e-05 - val_mae: 0.0041 - val_mse: 3.6212e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 767us/step - loss: 2.1403e-04 - mae: 0.0053 - mse: 9.8138e-05 - val_loss: 4.2362e-04 - val_mae: 0.0066 - val_mse: 1.1699e-04\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 789us/step - loss: 2.2506e-04 - mae: 0.0040 - mse: 1.0030e-04 - val_loss: 1.8455e-05 - val_mae: 0.0018 - val_mse: 7.8734e-06\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 818us/step - loss: 1.5862e-04 - mae: 0.0041 - mse: 7.9121e-05 - val_loss: 1.1024e-05 - val_mae: 0.0014 - val_mse: 4.6549e-06\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 787us/step - loss: 1.8665e-04 - mae: 0.0046 - mse: 1.0116e-04 - val_loss: 1.9766e-05 - val_mae: 0.0024 - val_mse: 9.1517e-06\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 767us/step - loss: 1.0980e-04 - mae: 0.0034 - mse: 5.1465e-05 - val_loss: 3.7453e-05 - val_mae: 0.0029 - val_mse: 1.3474e-05\n",
      "Iteration 8: delta = 1.7000000000000006, val_mae = 0.002859613625332713, val_mse = 1.3473911167238839e-05\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0202 - mae: 0.0278 - mse: 0.0099 - val_loss: 4.6532e-04 - val_mae: 0.0072 - val_mse: 1.3147e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 810us/step - loss: 7.3054e-04 - mae: 0.0098 - mse: 2.5472e-04 - val_loss: 1.5490e-04 - val_mae: 0.0051 - val_mse: 5.3768e-05\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 784us/step - loss: 6.3957e-04 - mae: 0.0090 - mse: 2.2639e-04 - val_loss: 2.9764e-04 - val_mae: 0.0051 - val_mse: 6.7444e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 794us/step - loss: 4.2139e-04 - mae: 0.0078 - mse: 1.6765e-04 - val_loss: 1.9003e-04 - val_mae: 0.0075 - val_mse: 1.0858e-04\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 787us/step - loss: 4.6065e-04 - mae: 0.0070 - mse: 1.5710e-04 - val_loss: 3.5675e-05 - val_mae: 0.0036 - val_mse: 2.1620e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 777us/step - loss: 2.8112e-04 - mae: 0.0065 - mse: 1.2653e-04 - val_loss: 5.7843e-04 - val_mae: 0.0099 - val_mse: 2.3816e-04\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 3s 925us/step - loss: 2.2514e-04 - mae: 0.0058 - mse: 9.4194e-05 - val_loss: 3.5057e-04 - val_mae: 0.0058 - val_mse: 8.3197e-05\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 3s 927us/step - loss: 2.2173e-04 - mae: 0.0051 - mse: 9.6291e-05 - val_loss: 3.8102e-04 - val_mae: 0.0089 - val_mse: 2.2523e-04\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 1.2057e-04 - mae: 0.0043 - mse: 5.3559e-05 - val_loss: 0.0016 - val_mae: 0.0193 - val_mse: 6.9083e-04\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 994us/step - loss: 1.8086e-04 - mae: 0.0047 - mse: 7.2457e-05 - val_loss: 3.9937e-05 - val_mae: 0.0032 - val_mse: 1.8388e-05\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 3s 892us/step - loss: 1.2407e-04 - mae: 0.0037 - mse: 4.8093e-05 - val_loss: 3.0701e-04 - val_mae: 0.0052 - val_mse: 7.9276e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 3s 995us/step - loss: 1.2302e-04 - mae: 0.0041 - mse: 5.5105e-05 - val_loss: 6.0428e-06 - val_mae: 0.0014 - val_mse: 3.4008e-06\n",
      "Iteration 9: delta = 1.8000000000000007, val_mae = 0.001435689046047628, val_mse = 3.4007787235168507e-06\n",
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 5s 2ms/step - loss: 0.0275 - mae: 0.0241 - mse: 0.0082 - val_loss: 1.4769e-04 - val_mae: 0.0079 - val_mse: 1.1303e-04\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 3s 932us/step - loss: 8.0542e-04 - mae: 0.0100 - mse: 3.1851e-04 - val_loss: 2.1641e-04 - val_mae: 0.0098 - val_mse: 1.7475e-04\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 3s 961us/step - loss: 8.1998e-04 - mae: 0.0104 - mse: 3.5927e-04 - val_loss: 1.7142e-04 - val_mae: 0.0056 - val_mse: 5.5307e-05\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 3s 1ms/step - loss: 3.6975e-04 - mae: 0.0071 - mse: 1.5361e-04 - val_loss: 5.1176e-04 - val_mae: 0.0122 - val_mse: 3.2507e-04\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 3s 895us/step - loss: 4.5659e-04 - mae: 0.0077 - mse: 2.0923e-04 - val_loss: 4.3152e-05 - val_mae: 0.0039 - val_mse: 2.8650e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 3s 917us/step - loss: 2.7404e-04 - mae: 0.0067 - mse: 1.4935e-04 - val_loss: 4.2055e-05 - val_mae: 0.0034 - val_mse: 1.7580e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 3s 925us/step - loss: 2.7083e-04 - mae: 0.0059 - mse: 1.1705e-04 - val_loss: 3.4858e-04 - val_mae: 0.0119 - val_mse: 2.9068e-04\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 3s 987us/step - loss: 2.4899e-04 - mae: 0.0058 - mse: 1.2242e-04 - val_loss: 2.5552e-04 - val_mae: 0.0080 - val_mse: 1.6363e-04\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 3s 926us/step - loss: 2.4025e-04 - mae: 0.0052 - mse: 1.2556e-04 - val_loss: 2.1244e-05 - val_mae: 0.0027 - val_mse: 1.0635e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 3s 927us/step - loss: 1.7372e-04 - mae: 0.0046 - mse: 9.0098e-05 - val_loss: 3.2940e-05 - val_mae: 0.0026 - val_mse: 1.7367e-05\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 3s 901us/step - loss: 1.5417e-04 - mae: 0.0041 - mse: 8.1378e-05 - val_loss: 2.9088e-05 - val_mae: 0.0029 - val_mse: 1.3817e-05\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 3s 975us/step - loss: 1.4427e-04 - mae: 0.0042 - mse: 7.6117e-05 - val_loss: 5.0783e-06 - val_mae: 0.0017 - val_mse: 4.5484e-06\n",
      "Iteration 10: delta = 1.9000000000000008, val_mae = 0.001716128084808588, val_mse = 4.548403467197204e-06\n",
      "Best delta: 1.2000000000000002\n",
      "Best validation MAE: 0.0012556943111121655\n",
      "Best validation MSE: 2.5508288672426715e-06\n"
     ]
    }
   ],
   "source": [
    "# find best delta for loss function\n",
    "\n",
    "# function to train and evaluate model with a given delta\n",
    "def train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=delta), metrics=['mae', 'mse'])\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    # get validation MAE and MSE\n",
    "    val_mae = history.history['val_mae'][-1]\n",
    "    val_mse = history.history['val_mse'][-1]\n",
    "\n",
    "    return val_mae, val_mse\n",
    "\n",
    "# initialise delta value and step size\n",
    "delta = 1.0\n",
    "step_size = 0.1\n",
    "best_delta = delta\n",
    "best_val_mae = float('inf')\n",
    "best_val_mse = float('inf')\n",
    "\n",
    "# iterative search for optimal delta (10 iterations)\n",
    "for i in range(10):\n",
    "    val_mae, val_mse = train_and_evaluate(delta, Xtrain, ytrain, wtrain, Xval, yval, wval)\n",
    "\n",
    "    # print current results\n",
    "    print(f\"Iteration {i+1}: delta = {delta}, val_mae = {val_mae}, val_mse = {val_mse}\")\n",
    "\n",
    "    # update best delta if current results are better\n",
    "    if val_mae < best_val_mae and val_mse < best_val_mse: \n",
    "        best_delta = delta\n",
    "        best_val_mae = val_mae\n",
    "        best_val_mse = val_mse\n",
    "    \n",
    "    # adjust delta for next iteration \n",
    "    delta += step_size\n",
    "\n",
    "print(f\"Best delta: {best_delta}\")\n",
    "print(f\"Best validation MAE: {best_val_mae}\")\n",
    "print(f\"Best validation MSE: {best_val_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.0210 - val_loss: 0.0013\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 2s 762us/step - loss: 7.3117e-04 - val_loss: 0.0012\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 651us/step - loss: 5.9045e-04 - val_loss: 9.3531e-04\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 692us/step - loss: 4.5600e-04 - val_loss: 1.0606e-04\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 709us/step - loss: 3.9195e-04 - val_loss: 8.3013e-05\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 741us/step - loss: 3.0722e-04 - val_loss: 1.5809e-05\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 2s 735us/step - loss: 1.6639e-04 - val_loss: 5.8386e-06\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 725us/step - loss: 1.9166e-04 - val_loss: 7.2212e-06\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 695us/step - loss: 1.7678e-04 - val_loss: 1.8932e-05\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 719us/step - loss: 1.9328e-04 - val_loss: 9.1088e-05\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 720us/step - loss: 1.5966e-04 - val_loss: 2.1216e-06\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 718us/step - loss: 1.9109e-04 - val_loss: 5.4566e-05\n",
      "Training Huber Loss: 0.00019109311688225716\n",
      "Validation Huber Loss: 5.4566415201406926e-05\n"
     ]
    }
   ],
   "source": [
    "# evaluate base model \n",
    "base_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "base_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta))\n",
    "\n",
    "# initialise History object to save training history\n",
    "history = History()\n",
    "\n",
    "history = base_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])\n",
    "\n",
    "# create folder for models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save model in models\n",
    "base_model.save(\"models/base_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/base_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "base_train_hl = history.history[\"loss\"][-1]\n",
    "base_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {base_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {base_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularised Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "3125/3125 [==============================] - 4s 1ms/step - loss: 0.1095 - val_loss: 0.0252\n",
      "Epoch 2/12\n",
      "3125/3125 [==============================] - 2s 651us/step - loss: 0.0189 - val_loss: 0.0175\n",
      "Epoch 3/12\n",
      "3125/3125 [==============================] - 2s 767us/step - loss: 0.0128 - val_loss: 0.0133\n",
      "Epoch 4/12\n",
      "3125/3125 [==============================] - 2s 687us/step - loss: 0.0096 - val_loss: 0.0102\n",
      "Epoch 5/12\n",
      "3125/3125 [==============================] - 2s 672us/step - loss: 0.0087 - val_loss: 0.0064\n",
      "Epoch 6/12\n",
      "3125/3125 [==============================] - 2s 713us/step - loss: 0.0066 - val_loss: 0.0056\n",
      "Epoch 7/12\n",
      "3125/3125 [==============================] - 3s 833us/step - loss: 0.0065 - val_loss: 0.0053\n",
      "Epoch 8/12\n",
      "3125/3125 [==============================] - 2s 688us/step - loss: 0.0058 - val_loss: 0.0050\n",
      "Epoch 9/12\n",
      "3125/3125 [==============================] - 2s 708us/step - loss: 0.0062 - val_loss: 0.0047\n",
      "Epoch 10/12\n",
      "3125/3125 [==============================] - 2s 654us/step - loss: 0.0050 - val_loss: 0.0047\n",
      "Epoch 11/12\n",
      "3125/3125 [==============================] - 2s 672us/step - loss: 0.0056 - val_loss: 0.0044\n",
      "Epoch 12/12\n",
      "3125/3125 [==============================] - 2s 698us/step - loss: 0.0051 - val_loss: 0.0042\n"
     ]
    }
   ],
   "source": [
    "reg_model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(Xtrain.shape[1],), kernel_regularizer='l2'), \n",
    "        Dense(32, activation='relu', kernel_regularizer='l2'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "reg_model.compile(optimizer=Adam(learning_rate=0.001), loss=Huber(delta=best_delta))\n",
    "\n",
    "# initialise History object to save training history\n",
    "history = History()\n",
    "\n",
    "history = reg_model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval), callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Huber Loss: 0.00510440394282341\n",
      "Validation Huber Loss: 0.004232339560985565\n"
     ]
    }
   ],
   "source": [
    "# create folder for models\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# save model in models\n",
    "base_model.save(\"models/reg_dnn_model_raw_pd.h5\")\n",
    "\n",
    "# save model history in models\n",
    "with open('models/reg_dnn_model_raw_pd_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "\n",
    "# print results\n",
    "reg_train_hl = history.history[\"loss\"][-1]\n",
    "reg_val_hl = history.history[\"val_loss\"][-1]\n",
    "print(f\"Training Huber Loss: {reg_train_hl}\")\n",
    "print(f\"Validation Huber Loss: {reg_val_hl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  34.1s\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  32.9s\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  30.4s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=sgd; total time=  57.0s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=sgd; total time=  53.0s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=sgd; total time=  54.3s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=None, learning_rate=0.1, optimizer=sgd; total time=  53.0s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=None, learning_rate=0.1, optimizer=sgd; total time=  53.0s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=None, learning_rate=0.1, optimizer=sgd; total time=  54.0s\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  30.8s\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  30.6s\n",
      "[CV] END activation=tanh, batch_size=32, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  30.5s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  55.2s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  52.0s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l1, learning_rate=0.01, optimizer=sgd; total time=  54.2s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.001, optimizer=adam; total time=  57.6s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.001, optimizer=adam; total time=  59.0s\n",
      "[CV] END activation=tanh, batch_size=16, kernel_regularizer=l2, learning_rate=0.001, optimizer=adam; total time=  57.6s\n",
      "[CV] END activation=relu, batch_size=32, kernel_regularizer=l2, learning_rate=0.001, optimizer=sgd; total time=  27.3s\n",
      "[CV] END activation=relu, batch_size=32, kernel_regularizer=l2, learning_rate=0.001, optimizer=sgd; total time=  28.2s\n",
      "[CV] END activation=relu, batch_size=32, kernel_regularizer=l2, learning_rate=0.001, optimizer=sgd; total time=  26.5s\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=adam; total time=  15.1s\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=adam; total time=  15.8s\n",
      "[CV] END activation=relu, batch_size=64, kernel_regularizer=l1, learning_rate=0.01, optimizer=adam; total time=  15.6s\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  16.3s\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  16.7s\n",
      "[CV] END activation=tanh, batch_size=64, kernel_regularizer=None, learning_rate=0.1, optimizer=adam; total time=  17.8s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  59.7s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  57.3s\n",
      "[CV] END activation=relu, batch_size=16, kernel_regularizer=l1, learning_rate=0.1, optimizer=adam; total time=  57.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x000001E470232E88>,\n",
       "                   param_distributions={'activation': ['relu', 'tanh'],\n",
       "                                        'batch_size': [16, 32, 64],\n",
       "                                        'kernel_regularizer': [None, 'l1',\n",
       "                                                               'l2'],\n",
       "                                        'learning_rate': [0.001, 0.01, 0.1],\n",
       "                                        'optimizer': ['sgd', 'adam']},\n",
       "                   random_state=42, verbose=2)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define base model function for KerasRegressor\n",
    "def create_model(activation='relu', optimizer='adam', learning_rate=0.001, kernel_regularizer=None, batch_size=32):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation=activation, input_shape=(Xtrain.shape[1],)), \n",
    "        Dense(32, activation=activation, kernel_regularizer=kernel_regularizer),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    optimizer = Adam(learning_rate=learning_rate) if optimizer == 'adam' else optimizer\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=Huber(delta=delta))\n",
    "\n",
    "    history = model.fit(Xtrain, ytrain, sample_weight=wtrain, epochs=early_stop.stopped_epoch, batch_size=32, validation_data=(Xval, yval, wval))\n",
    "\n",
    "    return model\n",
    "\n",
    "# define hyperparameter search space\n",
    "param_dist = {\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'optimizer': ['sgd', 'adam'],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'kernel_regularizer': [None, 'l1', 'l2'],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# create a KerasRegressor wrapper for use in RandomizedSearchCV\n",
    "keras_reg = KerasRegressor(build_fn=create_model, epochs=20, verbose=0)\n",
    "\n",
    "# perform random search hyperparameter tuning\n",
    "random_search = RandomizedSearchCV(estimator=keras_reg, param_distributions=param_dist, n_iter=5, cv=3, verbose=2, random_state=42)\n",
    "\n",
    "random_search.fit(Xtrain, ytrain, sample_weight=wtrain, validation_data=(Xval, yval, wval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'optimizer': 'adam', 'learning_rate': 0.001, 'kernel_regularizer': 'l2', 'batch_size': 16, 'activation': 'tanh'}\n"
     ]
    }
   ],
   "source": [
    "# get the best model and its hyperparameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5860/5860 [==============================] - 5s 919us/step - loss: 0.0327 - val_loss: 0.0031\n",
      "Epoch 2/20\n",
      "5860/5860 [==============================] - 5s 827us/step - loss: 0.0015 - val_loss: 7.7626e-04\n",
      "Epoch 3/20\n",
      "5860/5860 [==============================] - 5s 825us/step - loss: 0.0014 - val_loss: 7.1127e-04\n",
      "Epoch 4/20\n",
      "5860/5860 [==============================] - 5s 801us/step - loss: 0.0012 - val_loss: 7.2515e-04\n",
      "Epoch 5/20\n",
      "5860/5860 [==============================] - 5s 796us/step - loss: 0.0011 - val_loss: 6.0863e-04\n",
      "Epoch 6/20\n",
      "5860/5860 [==============================] - 5s 808us/step - loss: 0.0011 - val_loss: 5.9080e-04\n",
      "Epoch 7/20\n",
      "5860/5860 [==============================] - 5s 782us/step - loss: 0.0012 - val_loss: 5.8227e-04\n",
      "Epoch 8/20\n",
      "5860/5860 [==============================] - 5s 799us/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 9/20\n",
      "5860/5860 [==============================] - 5s 778us/step - loss: 0.0010 - val_loss: 5.4847e-04\n",
      "Epoch 10/20\n",
      "5860/5860 [==============================] - 5s 784us/step - loss: 9.9469e-04 - val_loss: 5.8952e-04\n",
      "Epoch 11/20\n",
      "5860/5860 [==============================] - 5s 788us/step - loss: 9.2160e-04 - val_loss: 9.9104e-04\n",
      "Epoch 12/20\n",
      "5860/5860 [==============================] - 5s 792us/step - loss: 9.9418e-04 - val_loss: 4.8483e-04\n",
      "Epoch 13/20\n",
      "5860/5860 [==============================] - 5s 772us/step - loss: 9.9313e-04 - val_loss: 7.7730e-04\n",
      "Epoch 14/20\n",
      "5860/5860 [==============================] - 6s 956us/step - loss: 9.8770e-04 - val_loss: 6.8833e-04\n",
      "Epoch 15/20\n",
      "5860/5860 [==============================] - 5s 806us/step - loss: 9.0400e-04 - val_loss: 9.4123e-04\n",
      "Epoch 16/20\n",
      "5860/5860 [==============================] - 5s 807us/step - loss: 8.9543e-04 - val_loss: 5.5934e-04\n",
      "Epoch 17/20\n",
      "5860/5860 [==============================] - 5s 801us/step - loss: 9.8747e-04 - val_loss: 5.8068e-04\n",
      "Epoch 18/20\n",
      "5860/5860 [==============================] - 5s 830us/step - loss: 9.4331e-04 - val_loss: 7.2801e-04\n",
      "Epoch 19/20\n",
      "5860/5860 [==============================] - 5s 821us/step - loss: 9.2750e-04 - val_loss: 4.9672e-04\n",
      "Epoch 20/20\n",
      "5860/5860 [==============================] - 5s 824us/step - loss: 9.2681e-04 - val_loss: 4.3869e-04\n"
     ]
    }
   ],
   "source": [
    "# save the best model\n",
    "best_model = create_model(activation='tanh', optimizer='adam', learning_rate=0.001, batch_size=16, kernel_regularizer='l2')\n",
    "best_model.fit(Xtrain, ytrain, epochs=20, batch_size=16, validation_data=(Xval,yval), verbose=1)\n",
    "base_model.save(\"best_dnn_model_raw.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Training MSE: 0.0009268095018342137\n",
      "Best Validation MSE: 0.00043869437649846077\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_train_mse = best_model.history.history[\"loss\"][-1]\n",
    "best_val_mse = best_model.history.history[\"val_loss\"][-1]\n",
    "print(f\"Best Training MSE: {best_train_mse}\")\n",
    "print(f\"Best Validation MSE: {best_val_mse}\")\n",
    "\n",
    "plt.plot(best_model.history.history['loss'], label='Training MSE')\n",
    "plt.plot(best_model.history.history['val_loss'], label='Validation MSE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processed best model: 'optimizer': 'adam', 'learning_rate': 0.001, 'kernel_regularizer': 'l2', 'batch_size': 16, 'activation': 'tanh'\n",
    "\n",
    "Raw best model: 'optimizer': 'sgd', 'learning_rate': 0.1, 'kernel_regularizer': None, 'batch_size': 16, 'activation': 'relu'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4cfd526cd006b8dad7d00207e92bc75bf60bfe4f4bf09b3b9e04b28bb891841c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
